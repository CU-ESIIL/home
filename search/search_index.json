{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Open Analysis and Synthesis Infrastructure for Science \ud83d\udd0d Quickstart Containers Analytics Data Library Resources <p>Welcome to the OASIS, a hub for open analysis and synthesis in Environmental Data Science.</p> CLICK \u25b6 Open Quick Start        Click here to contribute to OASIS"},{"location":"#data-analytics-libraries","title":"\ud83d\udcda Data &amp; Analytics Libraries","text":"<p>Data Library</p> <p>Organizational hub for ESIIL datasets.</p> <p>Analytics Library</p> <p>Repository for data harmonization and analytics.</p> <p>Container Image Library</p> <p>Browse available container images for ESIIL computing.</p> <p>Advanced Textbook</p> <p>Comprehensive guide to environmental data science.</p>    Tags: environmental data science, synthesis, open science, analytics, ESIIL"},{"location":"#nsf-synthesis-working-groups","title":"\ud83d\ude80 NSF Synthesis Working Groups","text":"<p>BioViewPoint</p> <p>Visualization tools for biodiversity data.</p> <p>Extreme Wildfire</p> <p>Investigating extreme wildfire behavior.</p> <p>Fungal Dispersal</p> <p>ESIIL working group on fungal dispersal.</p> <p>Macrophenology</p> <p>Macroecological patterns in phenology.</p> <p>Maka-Sitomniya</p> <p>Research on traditional ecological knowledge.</p> <p>AI for Natural Methane</p> <p>Harmonizing natural methane datasets using AI.</p> <p>Zooplankton Indicator Dataset</p> <p>Dataset and tools for zooplankton as environmental indicators.</p>"},{"location":"#nsf-synthesis-postdoc-researcher-projects","title":"\ud83c\udf93 NSF Synthesis Postdoc Researcher Projects","text":"<p>These repositories represent postdoc-led research initiatives at ESIIL.</p> <p>Datacube Sandbox</p> <p>Training and practice space for data cubes.</p> <p>Biotic Niche Modeling</p> <p>Research on species niche dynamics.</p> <p>Nutrient Flows</p> <p>Analysis of seafood trade and sustainability.</p> <p>LTER Material Legacies</p> <p>Impact of dead tree legacies on forest resilience.</p> <p>Team Science</p> <p>Studying scientific collaboration networks.</p> <p>Opt Decision Making</p> <p>Optimization and decision science.</p> <p>Water Carbon Dynamics</p> <p>Investigating water-carbon interactions.</p> <p>Cultural ES WildfireRx</p> <p>Wildfire effects on cultural ecosystem services.</p> <p>SCE Wildfire</p> <p>Socioecological impacts of wildfire.</p> <p>Tundra Shrub Expansion</p> <p>Remote sensing tools for alpine ecosystems.</p>"},{"location":"#staff-and-affiliate-research-projects","title":"\ud83d\udd2c Staff and Affiliate Research Projects","text":"<p>These repositories represent broader research efforts contributing to environmental data science.</p> Name Description Link SpectralBEF Spectral analysis of biodiversity-ecosystem functioning. \ud83d\udd17 Website GEDI-ECOSTRESS Data Project Aligning GEDI and ECOSTRESS datasets for ML applications. \ud83d\udd17 GitHub"},{"location":"#events-summits","title":"\ud83c\udf9f\ufe0f Events &amp; Summits","text":"<p>Major ESIIL-hosted and affiliated events.</p> <p>Innovation Summit 2025</p> <p>Official repository for the ESIIL Innovation Summit 2025.</p> <p>FIRE Plan 2024</p> <p>Planning and strategy for fire management.</p> <p>Innovation Summit 2024</p> <p>Official repo for the ESIIL Innovation Summit 2024.</p> <p>Forest Carbon Codefest</p> <p>Hands-on coding event for forest carbon research.</p> <p>HYR-SENSE</p> <p>Remote sensing.</p> <p>AGU 2023 Innovation Session</p> <p>Maximizing Stakeholder Engagement in Open Environmental Data Science.</p>"},{"location":"#oasis-templates-reusable-frameworks","title":"\ud83d\udee0\ufe0f OASIS Templates &amp; Reusable Frameworks","text":"<p>Templates to streamline project development and research collaboration.</p> <p>Postdoc OASIS</p> <p>Template for postdoc research documentation.</p> <p>Working Group OASIS</p> <p>Central hub for information on ESIIL working groups.</p> <p>Base GitHub Pages</p> <p>Starter repository for GitHub Pages projects.</p> <p>Education OASIS</p> <p>Template for ESIIL education students to create their own OASIS.</p> <p>Slideshow OASIS</p> <p>Basic slideshow template for presentations.</p>"},{"location":"#tutorials","title":"\ud83d\udcd6 Tutorials","text":"<p>Guides and walkthroughs to help researchers and students learn new tools and techniques.</p> Name Description Link Haskell Data Cube Creating data cubes with multi-source remote sensing data. \ud83d\udd17 GitHub CI/CD Demo Demonstration repository for continuous integration and deployment. \ud83d\udd17 GitHub Haskell API Demo Sensing the Earth workshop demonstration. \ud83d\udd17 GitHub"},{"location":"#ci-infrastructure-tools","title":"\ud83c\udfd7\ufe0f CI Infrastructure &amp; Tools","text":"<p>Repositories focused on software, data infrastructure, and computational tools.</p> <p>Docker</p> <p>Private repository for containerized workflows and CI/CD.</p> <p>CyVerse Utils</p> <p>Utilities for working with CyVerse infrastructure.</p> <p>OASIS Docker Demo</p> <p>Public demonstration of Dockerized workflows.</p> <p>Min Docker Demo</p> <p>Minimal demonstration for Docker containers.</p> <p>CyVerse Issues</p> <p>Documenting known issues in CyVerse workflows.</p> <p>Development Schedule</p> <p>Living task list and timeline for OASIS development.</p>"},{"location":"#teaching-resources","title":"\ud83c\udf93 Teaching Resources","text":"<p>Repositories containing materials for courses, workshops, and tutorials.</p> Name Description Link Pre-Innovation Summit Training Repository for all things pre-innovation-summit-training. \ud83d\udd17 Website EDS Demo Example repository for environmental data science education. \ud83d\udd17 Website Git &amp; GitHub Fundamentals GitHub Classroom-created fundamentals course. \ud83d\udd17 GitHub WG PI Orientation Orientation materials for ESIIL working group PIs. \ud83d\udd17 Website"},{"location":"#to-be-deleted","title":"\u274c To Be Deleted","text":"<p>These repositories are marked for removal.</p> Name Description Link Shannon-Boldt Private repository, marked for deletion. \ud83d\udd17 GitHub Ty Ed Demo One-time use education repository. \ud83d\udd17 Website Ty Test Single-use education test repository. \ud83d\udd17 Website"},{"location":"#group-sub-repositories","title":"\ud83c\udfdb\ufe0f Group Sub-Repositories","text":"<p>This section contains sub-repositories from various events, including the Innovation Summit, HYR-SENSE, FCC24, and Hackathon 2023. These repos represent collaborative breakout group projects and associated work.</p>"},{"location":"#innovation-summit-2025-breakout-groups","title":"\ud83c\udf1f Innovation Summit 2025 \u2014 Breakout Groups","text":"Name Description Link 1. Defining Tipping Points And Transformation Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 2. Stressors Order Duration Frequency Intensity Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 3. Abrupt Vs Gradual Shifts Rate Factors Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 4. Stressors Food Web Connectivity Stability Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 5. Historic Biodiversity Human Infrastructure Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 6. Unsustainable Land Use Agriculture Behavior Change Capacity Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 7. Policy Law Indigenous Sacred Sites Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 8. System Dynamics Early Warning Signals Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 9. Anomaly Detection Coral Reef Remote Sensing Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 10. Ecoregional Models PNW Regime Shifts Hazards Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 11. Transformations Ecosystem Services Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 12. Transformations Species Interactions Restoration Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 13. Sacred Spaces Mining Tourism Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 14. Social Impacts Measurement Evaluation Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 15. Resilience Rare Hydrologic Events Management Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 16. Indigenous Approaches Co Management Hegemonic Responses Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 17. Management Practices Prevent Thresholds Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 18. Cross Cutting Data Interoperability Harmonization Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 19. Wildcard Topic (19) Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website 20. Wildcard Topic (20) Breakout group from the Innovation Summit 2025. \ud83d\udd17 Website"},{"location":"#innovation-summit-2024-breakout-groups","title":"\ud83c\udf1f Innovation Summit 2024 Breakout Groups","text":"Name Description Link Innovation-Summit-2024__3 Breakout group 3 from the Innovation Summit 2024. \ud83d\udd17 GitHub Innovation-Summit-2024__4 Breakout group 4 from the Innovation Summit 2024. \ud83d\udd17 GitHub Innovation-Summit-2024__5 Breakout group 5 from the Innovation Summit 2024. \ud83d\udd17 GitHub Innovation-Summit-2024__6 Breakout group 6 from the Innovation Summit 2024. \ud83d\udd17 GitHub"},{"location":"#hyr-sense-workshop-group-repositories","title":"\ud83c\udf0d HYR-SENSE Workshop Group Repositories","text":"Name Description Link HYR-SENSE-Alaska HYR-SENSE Alaska breakout group. \ud83d\udd17 GitHub HYR-SENSE-Tyler HYR-SENSE Tyler breakout project. \ud83d\udd17 GitHub HYR-SENSE-VTaho HYR-SENSE Vermont-Tahoe research group. \ud83d\udd17 GitHub HYR-SENSE-MaryB HYR-SENSE research led by Mary B. \ud83d\udd17 GitHub"},{"location":"#fcc24-group-repositories","title":"\ud83d\udd2c FCC24 Group Repositories","text":"Name Description Link FCC24_Group_1 Group 1 from FCC24. \ud83d\udd17 GitHub FCC24_Group_2 Group 2 from FCC24. \ud83d\udd17 GitHub FCC24_Group_3 Group 3 from FCC24. \ud83d\udd17 GitHub FCC24_Group_4 Group 4 from FCC24. \ud83d\udd17 GitHub FCC24_Group_5 Group 5 from FCC24. \ud83d\udd17 GitHub FCC24_Group_6 Group 6 from FCC24. \ud83d\udd17 GitHub"},{"location":"#hackathon-2023-group-repositories","title":"\ud83d\ude80 Hackathon 2023 Group Repositories","text":"Name Description Link hackathon2023_A Breakout group A from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_B Breakout group B from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_C Breakout group C from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_D Breakout group D from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_E Breakout group E from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_F Breakout group F from Hackathon 2023. \ud83d\udd17 GitHub hackathon2023_G Breakout group G from Hackathon 2023. \ud83d\udd17 GitHub <p>\ud83d\udce7 Contact: esiil-support@cu.edu</p>"},{"location":"agenda/","title":"ESIIL Innovation Summit - Agenda","text":"<p>Big Data for Environmental Resilience and Adaptation Date: May 13-16, 2024 Location: SEEC Auditorium, University of Colorado Boulder Summit Website </p>"},{"location":"agenda/#goals-of-the-2024-esiil-innovation-summit","title":"Goals of the 2024 ESIIL Innovation Summit","text":"<ul> <li>Explore big data for environmental resilience and adaptation by identifying data synthesis opportunities and utilizing ESIIL cloud-compute capabilities.</li> <li>Promote best practices in ethical, open science, by supporting accessibility and usability of environmental data by all stakeholders.</li> <li>Champion ethical and equitable practices in environmental science, honoring data sovereignty and encouraging the responsible use of AI.</li> <li>Support diverse and inclusive teams by establishing collaborations around data-inspired themes across different disciplines, sectors, career stages, and backgrounds.</li> <li>Encourage the co-production of environmental knowledge with communities that are experiencing significant environmental challenges.</li> </ul>"},{"location":"agenda/#day-zero-may-13th","title":"Day Zero - May 13<sup>th</sup>","text":"Time Event Location 9:00 AM - 12:00 PM MDT Leadership Program S372 (Viz Studio) 9:00 AM - 12:00 PM MDT Auditorium Set Up: Tables, Questions, Handouts, etc. SEEC Auditorium 12:00 - 1:00 PM MDT Facilitators Lunch 1:00 or 1:30 PM MDT Concurrent Optional Activities NEON Tour, HIKE 3:00 - 5:00 PM MDT Early Registration opens SEEC Atrium 3:00 - 4:00 PM MDT Technical Help Desk SEEC Auditorium 4:00 - 6:00 PM MDT Social Mixer SEEC Cafe"},{"location":"agenda/#day-one-may-14th","title":"Day One - May 14<sup>th</sup>","text":"Time Event Location 8:30 AM MDT Registration SEEC Atrium 9:00 AM MDT Welcome &amp; Opening Ceremony SEEC Auditorium 9:35 AM MDT Logistics and Planning Team Introductions SEEC Auditorium 9:45 AM MDT Positive Polarities SEEC Auditorium 10:00 AM MDT Navigating Miscommunications SEEC Auditorium 10:15 AM MDT Creating a shared language SEEC Auditorium 10:30 AM MDT Break SEEC Atrium 10:45 AM MDT Science of Team Science SEEC Auditorium 11:05 AM MDT Big Data for Resilience SEEC Auditorium 11:45 AM MDT Q&amp;A SEEC Auditorium 12:15 PM MDT Group Photo SEEC Atrium 12:30 PM MDT Lunch SEEC Atrium 1:30 PM MDT Leveraging NEON to Understand Ecosystem Resilience Across Scales SEEC Auditorium 1:45 PM MDT Explore Topics in Resilience and Adaptation SEEC Auditorium 3:15 PM MDT Break SEEC Atrium 3:30 PM MDT Team Breakouts: Innovation Time Rooms available: S124, S127, S221, etc. 4:20 PM MDT Report Back SEEC Auditorium 4:50 PM MDT Whole Group Reflection SEEC Auditorium 4:55 PM MDT Day 1 Evaluation SEEC Auditorium 5:00 PM MDT Day 1 Close SEEC Auditorium"},{"location":"agenda/#day-two-may-15th","title":"Day Two - May 15<sup>th</sup>","text":"Time Event Location 8:30 AM MDT Coffee &amp; Tea SEEC Atrium 9:00 AM MDT Welcome Back SEEC Auditorium 9:20 AM MDT AI Research for Climate Change and Environmental Sustainability SEEC Auditorium 9:35 PM MDT Prepare for the day SEEC Auditorium 9:50 AM MDT Team Breakouts: Innovation Time Breakout Spaces with your Team 12:30 PM MDT Lunch SEEC Atrium 1:30 PM MDT Working Through the Groan Zone SEEC Auditorium 1:50 PM MDT Team Breakouts: Innovation Time Breakout Spaces with your Team 4:10 PM MDT Report Back SEEC Auditorium 4:50 PM MDT Whole Group Reflection SEEC Auditorium 5:00 PM MDT Day 2 Close"},{"location":"agenda/#day-three-may-16th","title":"Day Three - May 16<sup>th</sup>","text":"Time Event Location 8:30 AM MDT Coffee &amp; Tea SEEC Atrium 9:00 AM MDT Welcome Back SEEC Auditorium 9:15 AM MDT Final Team Breakout: Prepare for the Final Report Back Breakout Spaces with your Team 9:45 AM MDT Final Break SEEC Atrium 10:00 AM MDT Final Report back SEEC Auditorium 11:20 AM MDT What\u2019s Next? SEEC Auditorium 11:35 AM MDT Final Reflection SEEC Auditorium 11:50 PM MDT Closing SEEC Auditorium"},{"location":"breakout/","title":"Breakout prompts &amp; dedicated working space","text":""},{"location":"breakout/#dedicated-working-space","title":"Dedicated working space","text":"<p>Each team will have a room that has been reserved for their use at all team times. Those rooms will shift for each breakout time, shown below. However, your team is also welcome to explore the building and find other spaces that make you more comfortable or creative. For example, you may want to check out:</p> <ul> <li>The SEEC Cafe dining area (north end of the building, lots of windows in the eating room!)</li> <li>The SEEC lobby (both north and south)</li> <li>The Earth Lab conference room (S340). Note that this room may sometimes be reserved by Earth Lab staff.</li> <li>The southern end of the first floor, S148</li> <li>Outside if it is sunny! The grass or SEEC courtyard! You are also welcome to use other rooms if they are available, but please be aware that other classes, study groups, or workshops may have reserved them and kick you out.</li> </ul> Day 1 Team Dedicated Space Morning team time Team 1 S221 Team 2 S149 Team 3 C325 Team 4 S240 Team 5 Viz Studio (S372A) Team 6 Viz Studio (S372B) Afternoon team time Team 1 Viz Studio (S372B) Team 2 S221 Team 3 S149 Team 4 C325 Team 5 S240 Team 6 Viz Studio (S372A) Day 2 Team Dedicated Space Morning team time Team 1 Viz Studio (S372A) Team 2 Viz Studio (S372B) Team 3 S221 Team 4 S149 Team 5 C325 Team 6 S240 Afternoon team time Team 1 S240 Team 2 Viz Studio (S372A) Team 3 Viz Studio (S372B) Team 4 S221 Team 5 S149 Team 6 C325 Day 3 Team Dedicated Space Morning team time Team 1 C325 Team 2 S240 Team 3 Viz Studio (S372A) Team 4 Viz Studio (S372B) Team 5 S221 Team 6 S149"},{"location":"breakout/#breakout-prompts","title":"Breakout Prompts","text":"<p>For ease of access to breakout group prompts throughout the codefest.</p>"},{"location":"breakout/#breakout-0-virtual-meeting-3","title":"Breakout #0: Virtual meeting #3","text":"<p>Introduce yourselves! Please briefly share:</p> <ul> <li>Your preferred name and where you are currently based</li> <li>A skill or area of expertise that you feel you are bringing to the table</li> <li>Something you are worried about regarding the codefest</li> <li>Something that brings you joy</li> <li>What is a topic that you are excited to investigate for two days related to forest carbon in the Southern Rocky mountains?</li> <li>What datasets are you excited to potentially use?</li> </ul> <p>(Tuesday morning you will have ~2.5 hours to continue brainstorming, with a draft question ready by noon! So don't stress, this is just a first opportunity to get a sense of what your team is generally excited about.)</p>"},{"location":"breakout/#breakout-1-day-1-morning-team-time","title":"Breakout #1: Day 1 morning team time","text":"<p>In-person introductions</p> <ul> <li>Who are you and why are you excited to be here?</li> </ul> <p>Establish team norms</p> <ul> <li>Note-taking and documenting the flow of ideas</li> <li>Expectations for work outside of official event hours</li> </ul> <p>Brainstorm</p> <ul> <li>What will your team project be for the next 2.5 days?! This should be a specific scientific question related to forest carbon in the Southern Rocky Mountains that you think is potentially answerable (at least in a very rough form) by the end of the event.</li> <li>Think about\u2026<ul> <li>What are you each excited about and what skills do you have around the table that can be leveraged?</li> <li>What datasets are you familiar with and/or excited to work with? Spin up some instances and get familiar with the data!</li> <li>Evaluation criteria (linked on the website!)</li> </ul> </li> </ul> <p>Bring back</p> <ul> <li>One spokesperson to talk for 1 minute</li> <li>Your specific, answerable scientific question</li> <li>One \u2018need\u2019 that you see, whether that is help accessing an additional dataset, guidance on a dataset already available, or just your first step to get cracking</li> </ul>"},{"location":"breakout/#breakout-2-day-1-afternoon-team-time","title":"Breakout #2: Day 1 afternoon team time","text":"<p>Establish:</p> <ul> <li>How are you going to divide work, responsibilities, and code workflows?</li> <li>How are you going to manage people working in different coding languages?</li> </ul> <p>Take time to explore the datasets you intend to use and make sure you know how to work with and visualize them.</p> <p>Map out an initial workflow. What are the steps you will need to take to get from start to 'finish'?</p> <p>Begin work!</p>"},{"location":"breakout/#breakout-3-day-2-morning-team-time","title":"Breakout #3: Day 2 morning team time","text":"<p>Code, code, code!</p> <p>Focus on concrete, tractable problems, and don't get sucked into unneccessary coding or debugging. Is there an easier or faster way to answer your question?</p> <p>Graphics and deliverables are your friend! Demonstrate the progress you're making and remember to document what you are doing and WHY you're making the decisions you are.</p> <p>Keep your repo up to date!</p>"},{"location":"breakout/#breakout-4-day-2-afternoon-team-time","title":"Breakout #4: Day 2 afternoon team time","text":"<p>Prepare your deliverables. What figures are necessary to tell the story of your project? What do you want in your presentation and on your team website?</p>"},{"location":"breakout/#breakout-5-day-3-morning-team-time","title":"Breakout #5: Day 3 morning team time","text":"<p>Finalize all deliverables, push them to your website &amp; GitHub, and finalize your presentation!</p>"},{"location":"container-library/","title":"Container Image Library","text":"<p>This page will catalog available container images.</p>"},{"location":"container-library/#available-images","title":"Available Images","text":"<ul> <li>TODO: Add container descriptions and links.</li> </ul>"},{"location":"dev-requests/","title":"Development Requests","text":"<p>A running list of development requests from collaborators and partner teams.</p> <ul> <li> Data team: integrate repository with centralized dataset service</li> <li> Learning team: add interactive tutorials for onboarding</li> <li> Operations: improve deployment scripts and automation</li> </ul>","tags":["development","requests"]},{"location":"dev-schedule/","title":"Development Schedule","text":"<p>A collaborative timeline for OASIS development. Update this page through pull requests and link to contributions as tasks are completed.</p> <p>Use <code>python scripts/add_task.py \"Task title\" ISSUE --duration DAYS</code> to append work items. The Gantt chart refreshes automatically on pushes and pull requests.</p> <p>See Development Requests for external requests made of the team.</p> <p>Since February 2025, OASIS has grown from an initial scaffold into a tagged documentation site. Late August 2025 added sidebar tag pages, fixed tag links, and launched the Cloud Triangle lesson with examples. Early September 2025 expanded the ecosystem with the How to Contribute guide, new quickstarts pointing to the Data Library and Analytics Library, and a refreshed Project Group OASIS hub. Our next steps are to link tasks directly to GitHub issues and automate Gantt chart updates so the roadmap stays current.</p>","tags":["development","schedule"]},{"location":"dev-schedule/#historical-task-list","title":"Historical Task List","text":"<ul> <li> Initial repository setup \u2014 2025-02-05</li> <li> Tag-based navigation \u2014 2025-08-13</li> <li> Static tag system \u2014 2025-08-14</li> <li> Repository structure docs \u2014 2025-08-14</li> <li> <p> Development schedule page \u2014 2025-08-14</p> </li> <li> <p> Sidebar tag pages \u2014 2025-08-19</p> </li> <li> Fix tag links in sidebar \u2014 2025-08-19</li> <li> Cloud Triangle scaffold \u2014 2025-08-22</li> <li> Cloud Triangle overview \u2014 2025-08-22</li> <li> Cloud Triangle examples &amp; figure \u2014 2025-08-22</li> <li> How to Contribute guide launch \u2014 2025-09-12</li> <li> Data Library quickstart integration \u2014 2025-09-12</li> <li> Analytics Library quickstart integration \u2014 2025-09-12</li> <li> Project Group OASIS hub refresh \u2014 2025-09-16</li> </ul>","tags":["development","schedule"]},{"location":"dev-schedule/#upcoming-task-list","title":"Upcoming Task List","text":"<ul> <li> Add GitHub linking to tasks</li> <li> Automate Gantt chart updates</li> <li> Interactive Cloud Triangle lesson</li> </ul>","tags":["development","schedule"]},{"location":"dev-schedule/#timeline-overview","title":"Timeline Overview","text":"Task Start End Contributors Initial repository setup 2025-02-05 2025-02-05 Ty Tuff Tag-based navigation 2025-08-13 2025-08-13 Ty Tuff Static tag system 2025-08-14 2025-08-14 Ty Tuff Repository structure docs 2025-08-14 2025-08-14 Ty Tuff Development schedule page 2025-08-14 2025-08-14 Ty Tuff Sidebar tag pages 2025-08-19 2025-08-19 Ty Tuff Fix tag links in sidebar 2025-08-19 2025-08-19 Ty Tuff Cloud Triangle scaffold 2025-08-22 2025-08-22 Ty Tuff Cloud Triangle overview 2025-08-22 2025-08-22 Ty Tuff Cloud Triangle examples &amp; figure 2025-08-22 2025-08-22 Ty Tuff Add GitHub linking to tasks 2025-08-15 2025-08-21 TBD Automate Gantt chart updates 2025-08-22 2025-08-26 TBD How to Contribute guide launch 2025-09-10 2025-09-12 OASIS Team Data Library quickstart integration 2025-09-12 2025-09-12 Ty Tuff Analytics Library quickstart integration 2025-09-12 2025-09-12 Ty Tuff Project Group OASIS hub refresh 2025-09-15 2025-09-16 OASIS Team","tags":["development","schedule"]},{"location":"dev-schedule/#gantt-chart","title":"Gantt Chart","text":"<pre><code>gantt\n    dateFormat  YYYY-MM-DD\n    title OASIS Development Timeline\n%% gantt-start\n    section Completed\n    How to Contribute guide launch  :done, plan1, 2025-09-10, 3d\n    Data Library quickstart integration  :done, plan2, 2025-09-12, 1d\n    Analytics Library quickstart integration  :done, plan3, 2025-09-12, 1d\n    Project Group OASIS hub refresh  :done, plan4, 2025-09-15, 2d\n    section Upcoming\n    Add GitHub linking to tasks  :active, plan5, 2025-08-15, 7d\n    Automate Gantt chart updates  : plan6, 2025-08-22, 5d\n    Interactive Cloud Triangle lesson  : plan7, 2025-08-29, 5d\n%% gantt-end</code></pre>","tags":["development","schedule"]},{"location":"file_structure/","title":"Repository File Structure","text":"<p>This document summarizes the repository's key directories following recent cleanup.</p>"},{"location":"file_structure/#top-level-directories","title":"Top-level directories","text":"<ul> <li><code>code/</code> \u2013 source code and modules</li> <li><code>data_skills_example/</code> \u2013 example datasets and notebooks</li> <li><code>docs/</code> \u2013 project documentation and assets</li> <li><code>overrides/</code> \u2013 MkDocs theme overrides</li> <li><code>scripts/</code> \u2013 helper scripts for building docs and utilities</li> </ul>"},{"location":"file_structure/#documentation-substructure","title":"Documentation substructure","text":"<ul> <li><code>docs/assets/</code> \u2013 shared static assets used in the documentation</li> <li><code>art_gallery/</code> \u2013 images for the art gallery page</li> <li><code>cyverse_basics/</code> \u2013 screenshots for CyVerse basics tutorial</li> <li><code>esiil_content/</code>, <code>thumbnails/</code>, and others for images and media</li> <li><code>docs/worksheets/</code> \u2013 worksheet materials and generated figures</li> <li><code>redlining_figures/</code> \u2013 figures produced for the redlining worksheet</li> <li><code>redlining_student_figures/</code> \u2013 figures for the student edition worksheet</li> </ul> <p>Additional files and directories exist within these folders as needed.</p>"},{"location":"pending-tasks/","title":"Pending Development Tasks","text":"<p>Some items from the development schedule remain outstanding. The notes below outline recommended steps for completing them.</p>","tags":["development"]},{"location":"pending-tasks/#interactive-cloud-triangle-lesson","title":"Interactive Cloud Triangle lesson","text":"<ol> <li>Design interactive plots using libraries such as Plotly or Altair to illustrate the Cloud Triangle concept.</li> <li>Host the interactive components as standalone HTML or within Jupyter notebooks and integrate them into the documentation with <code>mkdocs-jupyter</code> or embedded iframes.</li> <li>Provide explanatory text and data sources alongside the interactive elements.</li> <li>Link the lesson to the existing Cloud Triangle overview and examples pages.</li> </ol>","tags":["development"]},{"location":"pending-tasks/#further-automation","title":"Further automation","text":"<ul> <li>Enhance <code>scripts/update_dev_schedule.py</code> to fetch issue titles and status directly from the GitHub API.</li> <li>Extend the script to update the timeline table in <code>docs/dev-schedule.md</code> for a fully automated roadmap.</li> </ul> <p>Running <code>python scripts/update_dev_schedule.py</code> will rebuild the upcoming task list and planned Gantt section from <code>docs/upcoming_tasks.yaml</code>.</p>","tags":["development"]},{"location":"teams/","title":"Event Logistics","text":""},{"location":"teams/#venue-information","title":"Venue Information","text":"<p>The ESIIL Innovation Summit will be held at the University of Colorado Boulder East Campus SEEC Building (4001 Discovery Dr, Boulder, CO 80303). Directions to the SEEC Building here.</p> <p>(Building Maps to follow)</p>"},{"location":"virtual-meetings/","title":"Pre-Summit Virtual Meetings","text":"<p>There are three virtual meetings associated with the 2024 ESIIL Summit.</p>"},{"location":"virtual-meetings/#virtual-meeting-1","title":"Virtual Meeting 1","text":"<p>Head in the Clouds: Navigating the Basics of Cloud Computing</p> <p>Date: April 24, 2024</p> <p>Time: 12:00-2:00 PM MST</p> <p>Virtual Meeting 1 Recording: https://www.youtube.com/watch?v=JxVPjDtIBmU</p> <p>Important Note: Please set up a GitHub account and a Cyverse account prior to this training.</p>"},{"location":"virtual-meetings/#virtual-meeting-2","title":"Virtual Meeting 2","text":"<p>Feet on the ground: Collaborating with Other People Using Cloud Computing</p> <p>Date:  May 1, 2024</p> <p>Time: 12:00-2:00 PM MST</p> <p>Virtual Meeting 2 Recording: https://www.youtube.com/watch?v=213C7faZVFQ</p>"},{"location":"virtual-meetings/#virtual-meeting-3","title":"Virtual Meeting 3","text":"<p>Voices in Concert: Cultural Intelligence, the Art of Team Science, and Community Skills</p> <p>Date: May 6, 2024</p> <p>Time: 9-11 AM MST</p> <p>Virtual Meeting 3 Recording: https://youtu.be/Ea21i3do9sA</p> <p>Science of Team Science Slides</p> <p>Community Skills Slides</p>"},{"location":"_includes/badges/","title":"Badges","text":"<p>{% include \"callouts.md\" %}</p> <p>{% macro stable() -%} Stable</p> <p>{% macro preview() -%} Preview</p> <p>{% macro archived() -%} Archived</p>"},{"location":"_includes/callouts/","title":"Callouts","text":"<p>{% macro note() -%}</p> <p>Note</p> <p>{{ caller() }}</p> <p>{%- endmacro %}</p> <p>{% macro tip() -%}</p> <p>Tip</p> <p>{{ caller() }}</p> <p>{%- endmacro %}</p> <p>{% macro caution() -%}</p> <p>Caution</p> <p>{{ caller() }}</p> <p>{%- endmacro %}</p>"},{"location":"_includes/next-steps/","title":"Next steps","text":"<p>{% include \"callouts.md\" %}</p> <p>{% macro next_steps(links) -%}</p>"},{"location":"_includes/next-steps/#next-steps","title":"Next steps","text":"<p>{% for text, url in links %} - {{ text }} {% endfor %}</p>"},{"location":"additional-resources/bilingualism_md/","title":"R and Python bilingualism","text":"<p>Welcome to the R and Python bilingualism reference guide! If you\u2019re fluent in one of these languages but hesitant to learn the other, you\u2019re in the right place. The good news is that there are many similarities between R and Python that make it easy to switch between the two.</p> <p>Both R and Python are widely used in data science and are open-source, meaning that they are free to use and constantly being improved by the community. They both have extensive libraries for data analysis, visualization, and machine learning. In fact, many of the libraries in both languages have similar names and functions, such as Pandas in Python and data.table in R.</p> <p>While there are differences between the two languages, they can complement each other well. Python is versatile and scalable, making it ideal for large and complex projects such as web development and artificial intelligence. R, on the other hand, is known for its exceptional statistical capabilities and is often used in data analysis and modeling. Visualization is also easier in R, making it a popular choice for creating graphs and charts.</p> <p>By learning both R and Python, you\u2019ll be able to take advantage of the strengths of each language and create more efficient and robust data analysis workflows. Don\u2019t let the differences between the two languages intimidate you - once you become familiar with one, learning the other will be much easier.</p> <p>So, whether you\u2019re a Python enthusiast looking to expand your statistical analysis capabilities, or an R user interested in exploring the world of web development and artificial intelligence, this guide will help you become bilingual in R and Python.</p>"},{"location":"additional-resources/bilingualism_md/#install-packages","title":"Install packages","text":"<p>In R, packages can be installed from CRAN repository by using the install.packages() function:</p> <p>R code:</p> <pre><code># Install the dplyr package from CRAN\ninstall.packages(\"dplyr\")\n</code></pre> <p>In Python, packages can be installed from the Anaconda repository by using the conda install command:</p> <p>Python code:</p> <pre><code># Install the pandas package from Anaconda\n!conda install pandas\n</code></pre> <p>Loading libraries in R and Python</p> <p>In R, libraries can be loaded in the same way as before, using the library() function:</p> <p>R code:</p> <pre><code># Load the dplyr library\nlibrary(dplyr)\n</code></pre> <p>In Python, libraries can be loaded in the same way as before, using the import statement. Here\u2019s an example:</p> <p>Python code:</p> <pre><code># Load the pandas library\nimport pandas as pd\n</code></pre> <p>Note that the package or library must be installed from the respective repository before it can be loaded. Also, make sure you have the correct repository specified in your system before installing packages. By default, R uses CRAN as its primary repository, whereas Anaconda uses its own repository by default.</p>"},{"location":"additional-resources/bilingualism_md/#reticulate","title":"reticulate","text":"<p>The reticulate package lets you run both R and Python together in the R environment.</p> <p>R libraries are stored and managed in a repository called CRAN. You can download R packages with the install.packages() function</p> <pre><code>install.packages(\"reticulate\")\n</code></pre> <p>You only need to install packages once, but you need to mount those packages with the library() function each time you open R.</p> <pre><code>library(reticulate)\n</code></pre> <p>Python libraries are stored and managed in a few different libraries and their dependencies are not regulated as strictly as R libraries are in CRAN. It\u2019s easier to publish a python package but it can also be more cumbersome for users because you need to manage dependencies yourself. You can download python packages using both R and Python code</p> <pre><code>py_install(\"laspy\")\n</code></pre> <pre><code>## + '/Users/ty/opt/miniconda3/bin/conda' 'install' '--yes' '--prefix' '/Users/ty/opt/miniconda3/envs/earth-analytics-python' '-c' 'conda-forge' 'laspy'\n</code></pre> <p>Now, let\u2019s create a Python list and assign it to a variable py_list:</p> <p>R code:</p> <pre><code>py_list &lt;- r_to_py(list(1, 2, 3))\n</code></pre> <p>We can now print out the py_list variable in Python using the py_run_string() function:</p> <p>R code:</p> <pre><code>py_run_string(\"print(r.py_list)\")\n</code></pre> <p>This will output [1, 2, 3] in the Python console.</p> <p>Now, let\u2019s create an R vector and assign it to a variable r_vec:</p> <p>R code:</p> <pre><code>r_vec &lt;- c(4, 5, 6)\n</code></pre> <p>We can now print out the r_vec variable in R using the py$ syntax to access Python variables:</p> <p>R code:</p> <pre><code>print(py$py_list)\n</code></pre> <p>This will output [1, 2, 3] in the R console.</p> <p>We can also call Python functions from R using the py_call() function. For example, let\u2019s call the Python sum() function on the py_list variable and assign the result to an R variable r_sum:</p> <p>R code:</p> <pre><code>r_sum &lt;- py_call(\"sum\", args = list(py_list))\n</code></pre> <p>We can now print out the r_sum variable in R:</p> <p>R code:</p> <pre><code>print(r_sum)\n</code></pre> <p>This will output 6 in the R console.</p>"},{"location":"additional-resources/bilingualism_md/#load-packages-and-change-settings","title":"Load packages and change settings","text":"<pre><code>options(java.parameters = \"-Xmx5G\")\n\nlibrary(r5r)\nlibrary(sf)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(interp)\nlibrary(dplyr)\nlibrary(osmdata)\nlibrary(ggthemes)\nlibrary(sf)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(akima)\nlibrary(dplyr)\nlibrary(raster)\nlibrary(osmdata)\nlibrary(mapview)\nlibrary(cowplot)\nlibrary(here)\nlibrary(testthat)\n</code></pre> <pre><code>import sys\nsys.argv.append([\"--max-memory\", \"5G\"])\n\nimport pandas as pd\nimport geopandas\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotnine\nimport contextily as cx\nimport r5py\nimport seaborn as sns\n</code></pre> <p>R and Python are two popular programming languages used for data analysis, statistics, and machine learning. Although they share some similarities, there are some fundamental differences between them. Here\u2019s an example code snippet in R and Python to illustrate some of the differences:</p> <p>R Code:</p> <pre><code># Create a vector of numbers from 1 to 10\nx &lt;- 1:10\n\n# Compute the mean of the vector\nmean_x &lt;- mean(x)\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>## [1] 5.5\n</code></pre> <p>Python Code:</p> <pre><code># Import the numpy library for numerical operations\nimport numpy as np\n\n# Create a numpy array of numbers from 1 to 10\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Compute the mean of the array\nmean_x = np.mean(x)\n\n# Print the result\nprint(mean_x)\n</code></pre> <pre><code>## 5.5\n</code></pre> <p>In this example, we can see that there are several differences between R and Python:</p> <p>Syntax: R uses the assignment operator \\&lt;- while Python uses the equals sign = for variable assignment.</p> <p>Libraries: Python relies heavily on external libraries such as numpy, pandas, and matplotlib for data analysis, while R has built-in functions for many data analysis tasks.</p> <p>Data types: R is designed to work with vectors and matrices, while Python uses lists and arrays. In the example above, we used the numpy library to create a numerical array in Python.</p> <p>Function names: Function names in R and Python can differ significantly. In the example above, we used the mean() function in R and the np.mean() function in Python to calculate the mean of the vector/array.</p> <p>These are just a few of the many differences between R and Python. Ultimately, the choice between the two languages will depend on your specific needs and preferences.</p>"},{"location":"additional-resources/bilingualism_md/#load-saved-data","title":"Load saved data","text":"<p>R Code:</p> <pre><code>data(\"iris\")\nhere()\nload(file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\"))\nobjects()\n</code></pre> <p>Python code:</p>"},{"location":"additional-resources/bilingualism_md/#save-data","title":"Save data","text":"<p>R Code:</p> <pre><code>save(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\"))\n\nwrite.csv(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.csv\"))\n</code></pre> <p>Python code:</p>"},{"location":"additional-resources/bilingualism_md/#functions","title":"functions","text":"<p>Both R and Python are powerful languages for writing functions that can take input, perform a specific task, and return output. R Code:</p> <pre><code># Define a function that takes two arguments and returns their sum\nsum_r &lt;- function(a, b) {\n  return(a + b)\n}\n\n# Call the function with two arguments and print the result\nresult_r &lt;- sum_r(3, 5)\nprint(result_r)\n</code></pre> <pre><code>## [1] 8\n</code></pre> <p>Python code:</p> <pre><code># Define a function that takes two arguments and returns their sum\ndef sum_py(a, b):\n    return a + b\n\n# Call the function with two arguments and print the result\nresult_py = sum_py(3, 5)\nprint(result_py)\n</code></pre> <pre><code>## 8\n</code></pre> <p>In both cases, we define a function that takes two arguments and returns their sum. In R, we use the function keyword to define a function, while in Python, we use the def keyword. The function body in R is enclosed in curly braces, while in Python it is indented.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Function arguments: In R, function arguments are separated by commas, while in Python they are enclosed in parentheses. The syntax for specifying default arguments and variable-length argument lists can also differ between the two languages. Return statement: In R, we use the return keyword to specify the return value of a function, while in Python, we simply use the return statement. Function names: Function names in R and Python can differ significantly. In the example above, we used the sum_r() function in R and the sum_py() function in Python to calculate the sum of two numbers.</p>"},{"location":"additional-resources/bilingualism_md/#data-plots","title":"Data Plots","text":"<p>R Code:</p> <pre><code># Load the \"ggplot2\" package for plotting\nlibrary(ggplot2)\n\n# Generate some sample data\nx &lt;- seq(1, 10, 1)\ny &lt;- x + rnorm(10)\n\n# Create a scatter plot\nggplot(data.frame(x, y), aes(x = x, y = y)) +\n  geom_point()\n</code></pre> <p> Python code:</p> <pre><code># Load the \"matplotlib\" library\nimport matplotlib.pyplot as plt\n\n# Generate some sample data\nimport numpy as np\nx = np.arange(1, 11)\ny = x + np.random.normal(0, 1, 10)\n\n#clear last plot\nplt.clf()\n\n# Create a scatter plot\nplt.scatter(x, y)\nplt.show()\n</code></pre> <p></p> <p>In both cases, we generate some sample data and create a scatter plot to visualize the relationship between the variables.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the ggplot2 package for plotting, while in Python, we use the matplotlib library. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays. Plotting functions: In R, we use the ggplot() function to create a new plot object, and then use the geom_point() function to create a scatter plot layer. In Python, we use the scatter() function from the matplotlib.pyplot module to create a scatter plot directly.</p>"},{"location":"additional-resources/bilingualism_md/#linear-regression","title":"Linear regression","text":"<p>R Code:</p> <pre><code># Load the \"ggplot2\" package for plotting\nlibrary(ggplot2)\n\n# Generate some sample data\nx &lt;- seq(1, 10, 1)\ny &lt;- x + rnorm(10)\n\n# Perform linear regression\nmodel_r &lt;- lm(y ~ x)\n\n# Print the model summary\nsummary(model_r)\n</code></pre> <pre><code>## \n## Call:\n## lm(formula = y ~ x)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.69344 -0.42336  0.08961  0.34778  1.56728 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -0.1676     0.6781  -0.247    0.811    \n## x             0.9750     0.1093   8.921 1.98e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9926 on 8 degrees of freedom\n## Multiple R-squared:  0.9087, Adjusted R-squared:  0.8972 \n## F-statistic: 79.59 on 1 and 8 DF,  p-value: 1.976e-05\n</code></pre> <pre><code># Plot the data and regression line\nggplot(data.frame(x, y), aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n</code></pre> <pre><code>## `geom_smooth()` using formula = 'y ~ x'\n</code></pre> <p></p> <p>Python code:</p> <pre><code># Load the \"matplotlib\" and \"scikit-learn\" libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate some sample data\nimport numpy as np\nx = np.arange(1, 11)\ny = x + np.random.normal(0, 1, 10)\n\n# Perform linear regression\nmodel_py = LinearRegression().fit(x.reshape(-1, 1), y)\n\n# Print the model coefficients\nprint(\"Coefficients: \", model_py.coef_)\n</code></pre> <pre><code>## Coefficients:  [1.15539692]\n</code></pre> <pre><code>print(\"Intercept: \", model_py.intercept_)\n\n#clear last plot\n</code></pre> <pre><code>## Intercept:  -1.1291396173221218\n</code></pre> <pre><code>plt.clf()\n\n# Plot the data and regression line\nplt.scatter(x, y)\nplt.plot(x, model_py.predict(x.reshape(-1, 1)), color='red')\nplt.show()\n</code></pre> <p></p> <p>In both cases, we generate some sample data with a linear relationship between x and y, and then perform a simple linear regression to estimate the slope and intercept of the line. We then plot the data and regression line to visualize the fit.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the lm() function from the base package to perform linear regression, while in Python, we use the LinearRegression() class from the scikit-learn library. Additionally, we use the ggplot2 package in R for plotting, while we use the matplotlib library in Python. Data format: In R, we can specify the dependent and independent variables in the formula used for regression. In Python, we need to reshape the input data to a two-dimensional array before fitting the model. Model summary: In R, we can use the summary() function to print a summary of the model, including the estimated coefficients, standard errors, and p-values. In Python, we need to print the coefficients and intercept separately.</p>"},{"location":"additional-resources/bilingualism_md/#random-forest","title":"Random Forest","text":"<p>R Code:</p> <pre><code># Load the \"randomForest\" package\nlibrary(randomForest)\n\n# Load the \"iris\" dataset\ndata(iris)\n\n# Split the data into training and testing sets\nset.seed(123)\ntrain_idx &lt;- sample(1:nrow(iris), nrow(iris) * 0.7, replace = FALSE)\ntrain_data &lt;- iris[train_idx, ]\ntest_data &lt;- iris[-train_idx, ]\n\n# Build a random forest model\nrf_model &lt;- randomForest(Species ~ ., data = train_data, ntree = 500)\n\n# Make predictions on the testing set\npredictions &lt;- predict(rf_model, test_data)\n\n# Calculate accuracy of the model\naccuracy &lt;- sum(predictions == test_data$Species) / nrow(test_data)\nprint(paste(\"Accuracy:\", accuracy))\n</code></pre> <pre><code>## [1] \"Accuracy: 0.977777777777778\"\n</code></pre> <p>Python code:</p> <pre><code># Load the \"pandas\", \"numpy\", and \"sklearn\" libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the \"iris\" dataset\niris = load_iris()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=123)\n\n# Build a random forest model\nrf_model = RandomForestClassifier(n_estimators=500, random_state=123)\nrf_model.fit(X_train, y_train)\n\n# Make predictions on the testing set\n</code></pre> <pre><code>## RandomForestClassifier(n_estimators=500, random_state=123)\n</code></pre> <pre><code>predictions = rf_model.predict(X_test)\n\n# Calculate accuracy of the model\naccuracy = sum(predictions == y_test) / len(y_test)\nprint(\"Accuracy:\", accuracy)\n</code></pre> <pre><code>## Accuracy: 0.9555555555555556\n</code></pre> <p>In both cases, we load the iris dataset and split it into training and testing sets. We then build a random forest model using the training data and evaluate its accuracy on the testing data.</p> <p>There are a few differences in the syntax and functionality between the two approaches:</p> <p>Library and package names: In R, we use the randomForest package to build random forest models, while in Python, we use the RandomForestClassifier class from the sklearn.ensemble module. We also use different libraries for loading and manipulating data (pandas and numpy in Python, and built-in datasets in R). Model parameters: The syntax for setting model parameters is slightly different in R and Python. For example, in R, we specify the number of trees using the ntree parameter, while in Python, we use the n_estimators parameter. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays.</p>"},{"location":"additional-resources/bilingualism_md/#basic-streetmap-from-open-street-map","title":"Basic streetmap from Open Street Map","text":"<p>R Code:</p> <pre><code># Load the \"osmdata\" package for mapping\nlibrary(osmdata)\nlibrary(tmap)\n\n# Define the map location and zoom level\nbbox &lt;- c(left = -0.16, bottom = 51.49, right = -0.13, top = 51.51)\n\n# Get the OpenStreetMap data\nosm_data &lt;- opq(bbox) %&gt;% \n  add_osm_feature(key = \"highway\") %&gt;% \n  osmdata_sf()\n\n# Plot the map using tmap\ntm_shape(osm_data$osm_lines) + \n  tm_lines()\n</code></pre> <p> Python code:</p> <pre><code># Load the \"osmnx\" package for mapping\nimport osmnx as ox\n\n# Define the map location and zoom level\nbbox = (51.49, -0.16, 51.51, -0.13)\n\n# Get the OpenStreetMap data\nosm_data = ox.graph_from_bbox(north=bbox[2], south=bbox[0], east=bbox[3], west=bbox[1], network_type='all')\n\n# Plot the map using osmnx\nox.plot_graph(osm_data)\n</code></pre> <pre><code>## (&lt;Figure size 1600x1600 with 0 Axes&gt;, &lt;AxesSubplot:&gt;)\n</code></pre> <p></p> <p>In both cases, we define the map location and zoom level, retrieve the OpenStreetMap data using the specified bounding box, and plot the map.</p> <p>The main differences between the two approaches are:</p> <p>Package names and syntax: In R, we use the osmdata package and its syntax to download and process the OpenStreetMap data, while in Python, we use the osmnx package and its syntax. Mapping libraries: In R, we use the tmap package to create a static map of the OpenStreetMap data, while in Python, we use the built-in ox.plot_graph function from the osmnx package to plot the map.</p>"},{"location":"additional-resources/bilingualism_md/#cnn-on-raster-data","title":"CNN on Raster data","text":"<p>R Code:</p> <pre><code># Load the \"keras\" package for building the CNN\nlibrary(tensorflow)\nlibrary(keras)\n\n# Load the \"raster\" package for working with raster data\nlibrary(raster)\n\n# Load the \"magrittr\" package for pipe operator\nlibrary(magrittr)\n\n# Load the data as a raster brick\nraster_data &lt;- brick(\"raster_data.tif\")\n\n# Split the data into training and testing sets\nsplit_data &lt;- sample(1:nlayers(raster_data), size = nlayers(raster_data)*0.8, replace = FALSE)\ntrain_data &lt;- raster_data[[split_data]]\ntest_data &lt;- raster_data[[setdiff(1:nlayers(raster_data), split_data)]]\n\n# Define the CNN model\nmodel &lt;- keras_model_sequential() %&gt;% \n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\", input_shape = c(ncol(train_data), nrow(train_data), ncell(train_data))) %&gt;% \n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% \n  layer_dropout(rate = 0.25) %&gt;% \n  layer_flatten() %&gt;% \n  layer_dense(units = 128, activation = \"relu\") %&gt;% \n  layer_dropout(rate = 0.5) %&gt;% \n  layer_dense(units = nlayers(train_data), activation = \"softmax\")\n\n# Compile the model\nmodel %&gt;% compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\n# Train the model\nhistory &lt;- model %&gt;% fit(x = array(train_data), y = to_categorical(1:nlayers(train_data)), epochs = 10, validation_split = 0.2)\n\n# Evaluate the model\nmodel %&gt;% evaluate(x = array(test_data), y = to_categorical(1:nlayers(test_data)))\n\n# Plot the model accuracy over time\nplot(history)\n</code></pre>"},{"location":"additional-resources/bilingualism_md/#piping","title":"Piping","text":"<p>Piping is a powerful feature in both R and Python that allows for a more streamlined and readable code. However, the syntax for piping is slightly different between the two languages.</p> <p>In R, piping is done using the %&gt;% operator from the magrittr package, while in Python, it is done using the | operator from the pandas package.</p> <p>Let\u2019s compare and contrast piping in R and Python with some examples:</p> <p>Piping in R In R, we can use the %&gt;% operator to pipe output from one function to another, which can make our code more readable and easier to follow. Here\u2019s an example:</p> <p>R code:</p> <pre><code>library(dplyr)\n\n# create a data frame\ndf &lt;- data.frame(x = c(1,2,3), y = c(4,5,6))\n\n# calculate the sum of column x and y\ndf %&gt;%\n  mutate(z = x + y) %&gt;%\n  summarize(sum_z = sum(z))\n</code></pre> <pre><code>##   sum_z\n## 1    21\n</code></pre> <p>In this example, we first create a data frame df with two columns x and y. We then pipe the output of df to mutate, which adds a new column z to the data frame that is the sum of x and y. Finally, we pipe the output to summarize, which calculates the sum of z and returns the result.</p> <p>Piping in Python In Python, we can use the | operator to pipe output from one function to another. However, instead of piping output from one function to another, we pipe a DataFrame to a method of the DataFrame. Here\u2019s an example:</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# create a DataFrame\ndf = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]})\n\n# calculate the sum of column x and y\n(df.assign(z = df['x'] + df['y'])\n   .agg(sum_z = ('z', 'sum')))\n</code></pre> <pre><code>##         z\n## sum_z  21\n</code></pre> <p>In this example, we first create a DataFrame df with two columns x and y. We then use the assign() method to add a new column z to the DataFrame that is the sum of x and y. Finally, we use the agg() method to calculate the sum of z and return the result.</p> <p>As we can see, the syntax for piping is slightly different between R and Python, but the concept remains the same. Piping can make our code more readable and easier to follow, which is an important aspect of creating efficient and effective code.</p> <p>R code:</p> <pre><code>library(dplyr)\nlibrary(ggplot2)\n\niris %&gt;%\n  filter(Species == \"setosa\") %&gt;%\n  group_by(Sepal.Width) %&gt;%\n  summarise(mean.Petal.Length = mean(Petal.Length)) %&gt;%\n  mutate(Sepal.Width = as.factor(Sepal.Width)) %&gt;%\n  ggplot(aes(x = Sepal.Width, y = mean.Petal.Length)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  labs(title = \"Mean Petal Length of Setosa by Sepal Width\",\n       x = \"Sepal Width\",\n       y = \"Mean Petal Length\")\n</code></pre> <p></p> <p>In this example, we start with the iris dataset and filter it to only include rows where the Species column is \u201csetosa\u201d. We then group the remaining rows by the Sepal.Width column and calculate the mean Petal.Length for each group. Next, we convert Sepal.Width to a factor variable to ensure that it is treated as a categorical variable in the visualization. Finally, we create a bar plot using ggplot2, with Sepal.Width on the x-axis and mean.Petal.Length on the y-axis. The resulting plot shows the mean petal length of setosa flowers for each sepal width category.</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Load the iris dataset and pipe it into the next function\n( pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n\n  # Select columns and pivot the dataset\n  .loc[:, ['sepal_length', 'sepal_width', 'petal_length']]\n  .melt(var_name='variable', value_name='value')\n\n  # Group by variable and calculate mean\n  .groupby('variable', as_index=False)\n  .mean()\n\n  # Filter for mean greater than 3.5 and sort by descending mean\n  .query('value &gt; 3.5')\n  .sort_values('value', ascending=False)\n)\n</code></pre> <pre><code>##        variable     value\n## 1  sepal_length  5.843333\n## 0  petal_length  3.758667\n</code></pre>"},{"location":"additional-resources/bilingualism_md/#for-loops","title":"for loops","text":"<p>Here is an example of a for loop in R:</p> <p>R code</p> <pre><code># Create a vector of numbers\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Use a for loop to print out each number in the vector\nfor (i in numbers) {\n  print(i)\n}\n</code></pre> <pre><code>## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n</code></pre> <p>In this example, the for loop iterates over each element in the numbers vector, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i.</p> <p>Here is the equivalent example in Python:</p> <p>Python code</p> <pre><code># Create a list of numbers\nnumbers = [1, 2, 3, 4, 5]\n\n# Use a for loop to print out each number in the list\nfor i in numbers:\n  print(i)\n</code></pre> <pre><code>## 1\n## 2\n## 3\n## 4\n## 5\n</code></pre> <p>In Python, the for loop iterates over each element in the numbers list, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i.</p> <p>Both languages also support nested for loops, which can be used to perform iterations over multiple dimensions, such as looping through a 2D array.</p>"},{"location":"additional-resources/bilingualism_md/#parallel","title":"Parallel","text":"<p>Parallel computing is a technique used to execute multiple computational tasks simultaneously, which can significantly reduce the time required to complete a task. Both R and Python have built-in support for parallel computing, although the approaches are slightly different. In this answer, we will compare and contrast the parallel computing capabilities of R and Python, and provide working examples in code.</p> <p>Parallel computing in R In R, there are several packages that support parallel computing, such as parallel, foreach, and doParallel. The parallel package provides basic functionality for parallel computing, while foreach and doParallel provide higher-level abstractions that make it easier to write parallel code.</p> <p>Here is an example of using the foreach package to execute a loop in parallel:</p> <p>R code:</p> <pre><code>library(foreach)\nlibrary(doParallel)\n\n# Set up a parallel backend with 4 workers\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\n# Define a function to apply in parallel\nmyfunc &lt;- function(x) {\n  # some computation here\n  return(x^2)\n}\n\n# Generate some data\nmydata &lt;- 1:1000\n\n# Apply the function to the data in parallel\nresult &lt;- foreach(i = mydata) %dopar% {\n  myfunc(i)\n}\n\n# Stop the cluster\nstopCluster(cl)\n</code></pre> <p>In this example, we use the makeCluster() function to set up a cluster with 4 workers, and the registerDoParallel() function to register the cluster as the parallel backend for foreach. We then define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use foreach to apply myfunc() to each element of mydata in parallel, using the %dopar% operator.</p> <p>R Tidyverse parallel</p> <p>In R Tidyverse, we can use the furrr package for parallel computing. Here\u2019s an example of using furrr to parallelize a map function:</p> <p>R Tidy code:</p> <pre><code>library(tidyverse)\nlibrary(furrr)\n\n# Generate a list of numbers\nnumbers &lt;- 1:10\n\n# Use the future_map function from furrr to parallelize the map function\nplan(multisession)\nsquares &lt;- future_map(numbers, function(x) x^2)\n</code></pre> <p>In this example, we first load the Tidyverse and furrr libraries. We then generate a list of numbers from 1 to 10. We then use the plan function to set the parallelization strategy to \u201cmultisession\u201d, which will use multiple CPU cores to execute the code. Finally, we use the future_map function from furrr to apply the function x^2 to each number in the list in parallel.</p> <p>Parallel computing in Python In Python, the standard library includes the multiprocessing module, which provides basic support for parallel computing. Additionally, there are several third-party packages that provide higher-level abstractions, such as joblib and dask.</p> <p>Here is an example of using the multiprocessing module to execute a loop in parallel:</p> <p>Python code:</p> <pre><code>def square(x):\n    return x**2\n\nfrom multiprocessing import Pool\n\n# Generate a list of numbers\nnumbers = list(range(1, 11))\n\n# Use the map function and a pool of workers to parallelize the square function\nwith Pool() as pool:\n    squares = pool.map(square, numbers)\n\nprint(squares)\n</code></pre> <p>In this example, we define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use the Pool class from the multiprocessing module to set up a pool of 4 workers. We then use the map() method of the Pool class to apply myfunc() to each element of mydata in parallel.</p> <p>Comparison and contrast Both R and Python have built-in support for parallel computing, with similar basic functionality for creating and managing parallel processes. However, the higher-level abstractions differ between the two languages. In R, the foreach package provides a high-level interface that makes it easy to write parallel code, while in Python, the multiprocessing module provides a basic interface that can be extended using third-party packages like joblib and dask.</p> <p>Additionally, Python has better support for distributed computing using frameworks like Apache Spark, while R has better support for shared-memory parallelism using tools like data.table and ff.</p>"},{"location":"additional-resources/bilingualism_md/#data-wrangling","title":"Data wrangling","text":"<p>Data wrangling is an important part of any data analysis project, and both R and Python provide tools and libraries for performing this task. In this answer, we will compare and contrast data wrangling in R\u2019s tidyverse and Python\u2019s pandas library, with working examples in code.</p> <p>Data Wrangling in R Tidyverse</p> <p>The tidyverse is a collection of R packages designed for data science, and it includes several packages that are useful for data wrangling. One of the most popular packages is dplyr, which provides a grammar of data manipulation for data frames.</p> <p>Here is an example of using dplyr to filter, mutate, and summarize a data frame:</p> <p>R code</p> <pre><code>library(dplyr)\n\n# Load data\ndata(mtcars)\n\n# Filter for cars with more than 100 horsepower\nmtcars %&gt;%\n  filter(hp &gt; 100) %&gt;%\n  # Add a new column with fuel efficiency in km per liter\n  mutate(kmpl = 0.425 * mpg) %&gt;%\n  # Group by number of cylinders and summarize\n  group_by(cyl) %&gt;%\n  summarize(mean_hp = mean(hp),\n            mean_kmpl = mean(kmpl))\n</code></pre> <pre><code>## # A tibble: 3 \u00d7 3\n##     cyl mean_hp mean_kmpl\n##   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n## 1     4    111      11.0 \n## 2     6    122.      8.39\n## 3     8    209.      6.42\n</code></pre> <p>In this example, we first filter the mtcars data frame to only include cars with more than 100 horsepower. We then use mutate to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency.</p> <p>Data Wrangling in Python Pandas</p> <p>Pandas is a popular library for data manipulation in Python. It provides a data frame object similar to R\u2019s data frames, along with a wide range of functions for data wrangling.</p> <p>Here is an example of using pandas to filter, transform, and group a data frame:</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Load data\nmtcars = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mtcars.csv')\n\n# Filter for cars with more than 100 horsepower\nfiltered_mtcars = mtcars[mtcars['hp'] &gt; 100]\n\n# Add a new column with fuel efficiency in km per liter\nfiltered_mtcars['kmpl'] = 0.425 * filtered_mtcars['mpg']\n\n# Group by number of cylinders and calculate mean horsepower and fuel efficiency\ngrouped_mtcars = filtered_mtcars.groupby('cyl').agg({'hp': 'mean',\n                                                     'kmpl': 'mean'})\n</code></pre> <p>In this example, we first load the mtcars data from a CSV file. We then filter the data to only include cars with more than 100 horsepower, using boolean indexing. We use the assign function to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency.</p> <p>Comparison</p> <p>Overall, both R\u2019s tidyverse and Python\u2019s pandas provide similar functionality for data wrangling. Both allow for filtering, transforming, and aggregating data frames. The syntax for performing these operations is slightly different between the two languages, with R using the %&gt;% operator for chaining operations and Python using method chaining or the apply family of functions.</p> <p>One key difference between the two languages is that R\u2019s tidyverse provides a consistent grammar for data manipulation across its various packages, making it easier to learn and use. However, Python\u2019s pandas library has a larger developer community and is more versatile for use in other applications, such as web development or machine learning.</p> <p>In conclusion, both R and Python provide powerful tools for data wrangling, and the choice between the two ultimately depends on the specific needs of the user and their familiarity</p>"},{"location":"additional-resources/bilingualism_md/#data-from-api","title":"Data from API","text":"<p>Retrieving data from an API is a common task in both R and Python. Here are examples of how to retrieve data from an API in both languages:</p> <p>Python</p> <p>To retrieve data from an API in Python, we can use the requests library. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API:</p> <p>Python code:</p> <pre><code>import requests\n\nurl = 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse = requests.get(url)\n\ndata = response.json()\n\nprint(data)\n</code></pre> <p>This code retrieves the current weather data for London from the OpenWeatherMap API. We first construct the API URL with the location and API key, then use the requests.get() function to make a request to the API. We then extract the JSON data from the response using the .json() method and print the resulting data.</p> <p>R</p> <p>In R, we can use the httr package to retrieve data from an API. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API in R:</p> <p>R code:</p> <pre><code>library(httr)\n\nurl &lt;- 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&amp;appid=API_KEY'\n\nresponse &lt;- GET(url)\n\ndata &lt;- content(response, 'text')\n\nprint(data)\n</code></pre> <p>This code is similar to the Python code above. We first load the httr library, then construct the API URL and use the GET() function to make a request to the API. We then extract the data from the response using the content() function and print the resulting data.</p> <p>Retrieving Data from an API in R Tidyverse In R Tidyverse, we can use the httr and jsonlite packages to retrieve and process data from an API.</p> <p>R code:</p> <pre><code># Load required packages\nlibrary(httr)\nlibrary(jsonlite)\n\n# Define API endpoint\nendpoint &lt;- \"https://jsonplaceholder.typicode.com/posts\"\n\n# Retrieve data from API\nresponse &lt;- GET(endpoint)\n\n# Extract content from response\ncontent &lt;- content(response, \"text\")\n\n# Convert content to JSON\njson &lt;- fromJSON(content)\n\n# Convert JSON to a data frame\ndf &lt;- as.data.frame(json)\n</code></pre> <p>In the above example, we use the GET() function from the httr package to retrieve data from an API endpoint, and the content() function to extract the content of the response. We then use the fromJSON() function from the jsonlite package to convert the JSON content to a list, and the as.data.frame() function to convert the list to a data frame.</p> <p>Retrieving Data from an API in Python In Python, we can use the requests library to retrieve data from an API, and the json library to process the JSON data.</p> <p>Python code:</p> <pre><code># Load required libraries\nimport requests\nimport json\n\n# Define API endpoint\nendpoint = \"https://jsonplaceholder.typicode.com/posts\"\n\n# Retrieve data from API\nresponse = requests.get(endpoint)\n\n# Extract content from response\ncontent = response.content\n\n# Convert content to JSON\njson_data = json.loads(content)\n\n# Convert JSON to a list of dictionaries\ndata = [dict(row) for row in json_data]\n</code></pre> <p>In the above example, we use the get() function from the requests library to retrieve data from an API endpoint, and the content attribute to extract the content of the response. We then use the loads() function from the json library to convert the JSON content to a list of dictionaries.</p> <p>Comparison Both R Tidyverse and Python provide powerful tools for retrieving and processing data from an API. In terms of syntax, the two languages are somewhat similar. In both cases, we use a library to retrieve data from the API, extract the content of the response, and then process the JSON data. However, there are some differences in the specific functions and methods used. For example, in R Tidyverse, we use the content() function to extract the content of the response, whereas in Python, we use the content attribute. Additionally, in R Tidyverse, we use the fromJSON() function to convert the JSON data to a list, whereas in Python, we use the loads() function.</p>"},{"location":"additional-resources/bilingualism_md/#census-data","title":"Census data","text":"<p>Retrieving USA census data in R, R Tidy, and Python can be done using different packages and libraries. Here are some working examples in code for each language:</p> <p>R:</p> <p>To retrieve census data in R, we can use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>R code:</p> <pre><code>library(tidycensus)\nlibrary(tidyverse)\n\n# Set your Census API key\ncensus_api_key(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",\n  state = \"CA\"\n) %&gt;% \n  rename(total_population = estimate) %&gt;% \n  select(total_population)\n\n# View the result\nca_pop\n</code></pre> <p>R Tidy:</p> <p>To retrieve census data in R Tidy, we can also use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California using pipes and dplyr functions:</p> <p>R tidy code:</p> <pre><code>library(tidycensus)\nlibrary(tidyverse)\n\n# Set your Census API key\ncensus_api_key(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",\n  state = \"CA\"\n) %&gt;% \n  rename(total_population = estimate) %&gt;% \n  select(total_population)\n\n# View the result\nca_pop\n</code></pre> <p>Python:</p> <p>To retrieve census data in Python, we can use the census library. Here\u2019s an example of how to retrieve the total population for the state of California:</p> <p>Python code:</p> <pre><code>from census import Census\nfrom us import states\nimport pandas as pd\n\n# Set your Census API key\nc = Census(\"your_api_key\")\n\n# Get the total population for the state of California\nca_pop = c.acs5.state((\"B01003_001\"), states.CA.fips, year=2019)\n\n# Convert the result to a Pandas DataFrame\nca_pop_df = pd.DataFrame(ca_pop)\n\n# Rename the column\nca_pop_df = ca_pop_df.rename(columns={\"B01003_001E\": \"total_population\"})\n\n# Select only the total population column\nca_pop_df = ca_pop_df[[\"total_population\"]]\n\n# View the result\nca_pop_df\n</code></pre>"},{"location":"additional-resources/bilingualism_md/#lidar-data","title":"Lidar data","text":"<p>To find Lidar data in R and Python, you typically need to start by identifying sources of Lidar data and then accessing them using appropriate packages and functions. Here are some examples of how to find Lidar data in R and Python:</p> <p>R:</p> <p>Identify sources of Lidar data: The USGS National Map Viewer provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the lidR package in R to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>R code:</p> <pre><code>library(lidR)\n\n# Download Lidar data\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlidar &lt;- readLAS(LASfile)\n\n# Visualize the data\nplot(lidar)\n</code></pre> <p>Python:</p> <p>Identify sources of Lidar data: The USGS 3DEP program provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the pylastools package in Python to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area:</p> <p>Python code:</p> <pre><code>py_install(\"requests\")\npy_install(\"pylas\")\npy_install(\"laspy\")\n</code></pre> <pre><code>import requests\nfrom pylas import read\nimport laspy\nimport numpy as np\n\n# Download Lidar data\nurl = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_SanFrancisco_2016_LAS_2018.zip\"\nlasfile = \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.las\"\nr = requests.get(url, allow_redirects=True)\nopen(lasfile, 'wb').write(r.content)\n\n# Read the data\nlidar = read(lasfile)\n\n# Visualize the data\nlaspy.plot.plot(lidar)\n</code></pre>"},{"location":"additional-resources/bilingualism_md/#data-for-black-lives","title":"Data for black lives","text":"<p>Data for Black Lives (https://d4bl.org/) is a movement that uses data science to create measurable change in the lives of Black people. While the Data for Black Lives website provides resources, reports, articles, and datasets related to racial equity, it doesn\u2019t provide a direct API for downloading data.</p> <p>Instead, you can access the Data for Black Lives GitHub repository (https://github.com/Data4BlackLives) to find datasets and resources to work with. In this example, we\u2019ll use a sample dataset available at https://github.com/Data4BlackLives/covid-19/tree/master/data. The dataset \u201cCOVID19_race_data.csv\u201d contains COVID-19 race-related data.</p> <p>R: In R, we\u2019ll use the \u2018readr\u2019 and \u2018dplyr\u2019 packages to read, process, and analyze the dataset.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\n\nlibrary(readr)\nlibrary(dplyr)\n\n# Read the CSV file\nurl &lt;- \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata &lt;- read_csv(url)\n\n# Basic information about the dataset\nprint(dim(data))\nprint(head(data))\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\ndata %&gt;%\n  group_by(state) %&gt;%\n  summarize(mean_cases_total = mean(cases_total, na.rm = TRUE)) %&gt;%\n  arrange(desc(mean_cases_total))\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018pandas\u2019 library to read, process, and analyze the dataset.</p> <p>Python code:</p> <pre><code>import pandas as pd\n\n# Read the CSV file\nurl = \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\"\ndata = pd.read_csv(url)\n\n# Basic information about the dataset\nprint(data.shape)\nprint(data.head())\n\n# Example analysis: calculate the mean of 'cases_total' by 'state'\nmean_cases_total = data.groupby(\"state\")[\"cases_total\"].mean().sort_values(ascending=False)\nprint(mean_cases_total)\n</code></pre> <p>In conclusion, both R and Python provide powerful libraries and tools for downloading, processing, and analyzing datasets, such as those found in the Data for Black Lives repository. The \u2018readr\u2019 and \u2018dplyr\u2019 libraries in R offer a simple and intuitive way to read and manipulate data, while the \u2018pandas\u2019 library in Python offers similar functionality with a different syntax. Depending on your preferred programming language and environment, both options can be effective in working with social justice datasets.</p>"},{"location":"additional-resources/bilingualism_md/#propublica-congress-api","title":"Propublica Congress API","text":"<p>The ProPublica Congress API provides information about the U.S. Congress members and their voting records. In this example, we\u2019ll fetch data about the current Senate members and calculate the number of members in each party.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the ProPublica Congress API.</p> <p>R code:</p> <pre><code># load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Replace 'your_api_key' with your ProPublica API key\n\n#\n\n# Fetch data about the current Senate members\nurl &lt;- \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse &lt;- GET(url, add_headers(`X-API-Key` = api_key))\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  members &lt;- data$results[[1]]$members\n\n  # Calculate the number of members in each party\n  party_counts &lt;- table(sapply(members, function(x) x$party))\n  print(party_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## \n##  D  I ID  R \n## 49  1  2 51\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the ProPublica Congress API and \u2018pandas\u2019 library to process the data.</p> <p>python code:</p> <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Replace 'your_api_key' with your ProPublica API key\napi_key = \"your_api_key\"\nheaders = {\"X-API-Key\": api_key}\n\n# Fetch data about the current Senate members\nurl = \"https://api.propublica.org/congress/v1/117/senate/members.json\"\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    members = data[\"results\"][0][\"members\"]\n\n    # Calculate the number of members in each party\n    party_counts = pd.DataFrame(members)[\"party\"].value_counts()\n    print(party_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the ProPublica Congress API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the ProPublica Congress API.</p>"},{"location":"additional-resources/bilingualism_md/#nonprofit-explorer-api-by-propublica","title":"Nonprofit Explorer API by ProPublica","text":"<p>The Nonprofit Explorer API by ProPublica provides data on tax-exempt organizations in the United States. In this example, we\u2019ll search for organizations with the keyword \u201ceducation\u201d and analyze the results.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Nonprofit Explorer API.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data for organizations with the keyword \"education\"\nurl &lt;- \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  organizations &lt;- data$organizations\n\n  # Count the number of organizations per state\n  state_counts &lt;- table(sapply(organizations, function(x) x$state))\n  print(state_counts)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## \n##      AZ      CA      CO      DC      FL      GA      HI      IL Indiana      LA \n##       3      22       6       5       3       2       1       2       1       1 \n##      MD      MI      MN      MO      MP      MS      NC      NE      NJ      NM \n##       1       2       5       3       1       1       2       2       2       1 \n##      NY      OH      OK  Oregon      PA      TX      UT      VA      WA      WV \n##       1       5       1       2       2      12       1       4       3       1 \n##      ZZ \n##       2\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Nonprofit Explorer API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\nimport requests\nimport pandas as pd\n\n# Fetch data for organizations with the keyword \"education\"\nurl = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    organizations = data[\"organizations\"]\n\n    # Count the number of organizations per state\n    state_counts = pd.DataFrame(organizations)[\"state\"].value_counts()\n    print(state_counts)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>## CA         22\n## TX         12\n## CO          6\n## MN          5\n## OH          5\n## DC          5\n## VA          4\n## AZ          3\n## WA          3\n## MO          3\n## FL          3\n## IL          2\n## GA          2\n## NC          2\n## MI          2\n## Oregon      2\n## NE          2\n## ZZ          2\n## PA          2\n## NJ          2\n## HI          1\n## MS          1\n## NY          1\n## Indiana     1\n## NM          1\n## LA          1\n## UT          1\n## MD          1\n## MP          1\n## WV          1\n## OK          1\n## Name: state, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Nonprofit Explorer API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Nonprofit Explorer API.</p>"},{"location":"additional-resources/bilingualism_md/#campaign-finance-api-by-propublica","title":"Campaign Finance API by ProPublica","text":"<p>The Campaign Finance API by the Federal Election Commission (FEC) provides data on campaign finance in U.S. federal elections. In this example, we\u2019ll fetch data about individual contributions for the 2020 election cycle and analyze the results.</p> <p>R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Campaign Finance API.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl &lt;- \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key='OGwpkX7tH5Jihs1qQcisKfVAMddJzmzouWKtKoby'&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse &lt;- GET(url)\n\n# Check if the request was successful\nif (http_status(response)$category == \"Success\") {\n  data &lt;- content(response, \"parsed\")\n  contributions &lt;- data$results\n\n  # Calculate the total contributions per state\n  state_totals &lt;- aggregate(contributions$contributor_state, by = list(contributions$contributor_state), FUN = sum)\n  colnames(state_totals) &lt;- c(\"State\", \"Total_Contributions\")\n  print(state_totals)\n} else {\n  print(http_status(response)$message)\n}\n</code></pre> <pre><code>## [1] \"Client error: (403) Forbidden\"\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Campaign Finance API and \u2018pandas\u2019 library to process the data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\n\nimport requests\nimport pandas as pd\n\n# Fetch data about individual contributions for the 2020 election cycle\nurl = \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key=your_api_key&amp;two_year_transaction_period=2020&amp;sort_hide_null=false&amp;sort_null_only=false&amp;per_page=20&amp;page=1\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    data = response.json()\n    contributions = data[\"results\"]\n\n    # Calculate the total contributions per state\n    df = pd.DataFrame(contributions)\n    state_totals = df.groupby(\"contributor_state\")[\"contribution_receipt_amount\"].sum()\n    print(state_totals)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <pre><code>## Error: 403\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Campaign Finance API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like aggregate() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Campaign Finance API.</p> <p>Note: Remember to replace your_api_key with your actual FEC API key in the code examples above.</p>"},{"location":"additional-resources/bilingualism_md/#historic-redlining","title":"Historic Redlining","text":"<p>Historic redlining data refers to data from the Home Owners\u2019 Loan Corporation (HOLC) that created residential security maps in the 1930s, which contributed to racial segregation and disinvestment in minority neighborhoods. One popular source for this data is the Mapping Inequality project (https://dsl.richmond.edu/panorama/redlining/).</p> <p>In this example, we\u2019ll download historic redlining data for Philadelphia in the form of a GeoJSON file and analyze the data in R and Python.</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the GeoJSON data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Download historic redlining data for Philadelphia\nurl &lt;- \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson &lt;- read_sf(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts &lt;- philly_geojson %&gt;%\n  group_by(holc_grade) %&gt;%\n  summarize(count = n())\n\nplot(grade_counts)\n</code></pre> <p></p> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the GeoJSON data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\n\n\nimport geopandas as gpd\n\n# Download historic redlining data for Philadelphia\nurl = \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\"\nphilly_geojson = gpd.read_file(url)\n\n# Count the number of areas per HOLC grade\ngrade_counts = philly_geojson[\"holc_grade\"].value_counts()\nprint(grade_counts)\n</code></pre> <pre><code>## B    28\n## D    26\n## C    18\n## A    10\n## Name: holc_grade, dtype: int64\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process historic redlining data in the form of GeoJSON files. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with historic redlining data.</p>"},{"location":"additional-resources/bilingualism_md/#american-indian-and-alaska-native-areas-aiannh","title":"American Indian and Alaska Native Areas (AIANNH)","text":"<p>In this example, we\u2019ll download and analyze the American Indian and Alaska Native Areas (AIANNH) TIGER/Line Shapefile from the U.S. Census Bureau. We\u2019ll download the data for the year 2020, and analyze the number of AIANNH per congressional district</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the Shapefile data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(sf)\nlibrary(dplyr)\n\n# Download historic redlining data for Philadelphia\nurl &lt;- \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\"\ntemp_file &lt;- tempfile(fileext = \".zip\")\ndownload.file(url, temp_file, mode = \"wb\")\nunzip(temp_file, exdir = tempdir())\n\n# Read the Shapefile\nshapefile_path &lt;- file.path(tempdir(), \"tl_2020_us_aiannh.shp\")\naiannh &lt;- read_sf(shapefile_path)\n\n# Count the number of AIANNH per congressional district\nstate_counts &lt;- aiannh %&gt;%\n  group_by(LSAD) %&gt;%\n  summarize(count = n())\n\nprint(state_counts[order(-state_counts$count),])\n</code></pre> <pre><code>## Simple feature collection with 26 features and 2 fields\n## Geometry type: GEOMETRY\n## Dimension:     XY\n## Bounding box:  xmin: -174.236 ymin: 18.91069 xmax: -67.03552 ymax: 71.34019\n## Geodetic CRS:  NAD83\n## # A tibble: 26 \u00d7 3\n##    LSAD  count                                                          geometry\n##    &lt;chr&gt; &lt;int&gt;                                                &lt;MULTIPOLYGON [\u00b0]&gt;\n##  1 79      221 (((-166.5331 65.33918, -166.5331 65.33906, -166.533 65.33699, -1\u2026\n##  2 86      206 (((-83.38811 35.46645, -83.38342 35.46596, -83.38316 35.46593, -\u2026\n##  3 OT      155 (((-92.32972 47.81374, -92.3297 47.81305, -92.32967 47.81196, -9\u2026\n##  4 78       75 (((-155.729 20.02457, -155.7288 20.02428, -155.7288 20.02427, -1\u2026\n##  5 85       46 (((-122.3355 37.95215, -122.3354 37.95206, -122.3352 37.95199, -\u2026\n##  6 92       35 (((-93.01356 31.56287, -93.01354 31.56251, -93.01316 31.56019, -\u2026\n##  7 88       25 (((-97.35299 36.908, -97.35291 36.90801, -97.35287 36.908, -97.3\u2026\n##  8 96       19 (((-116.48 32.63814, -116.48 32.63718, -116.4794 32.63716, -116.\u2026\n##  9 84       16 (((-105.5937 36.40379, -105.5937 36.40324, -105.5937 36.40251, -\u2026\n## 10 89       11 (((-95.91705 41.28037, -95.91653 41.28036, -95.91653 41.28125, -\u2026\n## # \u2139 16 more rows\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the Shapefile data.</p> <p>Python code:</p> <pre><code>import geopandas as gpd\nimport pandas as pd\nimport requests\nimport zipfile\nimport os\nfrom io import BytesIO\n\n# Download historic redlining data for Philadelphia\nurl = \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\"\nresponse = requests.get(url)\nzip_file = zipfile.ZipFile(BytesIO(response.content))\n\n# Extract Shapefile\ntemp_dir = \"temp\"\nif not os.path.exists(temp_dir):\n    os.makedirs(temp_dir)\n\nzip_file.extractall(path=temp_dir)\nshapefile_path = os.path.join(temp_dir, \"tl_2020_us_aiannh.shp\")\n\n# Read the Shapefile\naiannh = gpd.read_file(shapefile_path)\n\n# Count the number of AIANNH per congressional district\nstate_counts = aiannh.groupby(\"LSAD\").size().reset_index(name=\"count\")\n\n# Sort by descending count\nstate_counts_sorted = state_counts.sort_values(by=\"count\", ascending=False)\n\nprint(state_counts_sorted)\n</code></pre> <pre><code>##    LSAD  count\n## 2    79    221\n## 9    86    206\n## 25   OT    155\n## 1    78     75\n## 8    85     46\n## 15   92     35\n## 11   88     25\n## 19   96     19\n## 7    84     16\n## 12   89     11\n## 5    82      8\n## 3    80      7\n## 4    81      6\n## 21   98      5\n## 20   97      5\n## 13   90      4\n## 18   95      3\n## 6    83      3\n## 17   94      2\n## 16   93      1\n## 14   91      1\n## 10   87      1\n## 22   99      1\n## 23   9C      1\n## 24   9D      1\n## 0    00      1\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process AIANNH TIGER/Line Shapefile data from the U.S. Census Bureau. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with AIANNH data.</p>"},{"location":"additional-resources/bilingualism_md/#indian-entities-recognized-and-eligible-to-receive-services-by-bia","title":"Indian Entities Recognized and Eligible To Receive Services by BIA","text":"<p>The Bureau of Indian Affairs (BIA) provides a PDF document containing a list of Indian Entities Recognized and Eligible To Receive Services. To analyze the data, we\u2019ll first need to extract the information from the PDF. In this example, we\u2019ll extract the names of the recognized tribes and count the number of tribes per state.</p> <p>R: In R, we\u2019ll use the \u2018pdftools\u2019 package to extract text from the PDF and the \u2018stringr\u2019 package to process the text data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(dplyr)\n\n# Download the BIA PDF\nurl &lt;- \"https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01789.pdf\"\ntemp_file &lt;- tempfile(fileext = \".pdf\")\ndownload.file(url, temp_file, mode = \"wb\")\n\n# Extract text from the PDF\npdf_text &lt;- pdf_text(temp_file)\ntribe_text &lt;- pdf_text[4:length(pdf_text)]\n\n# Define helper functions\ntribe_state_extractor &lt;- function(text_line) {\n  regex_pattern &lt;- \"(.*),\\\\s+([A-Z]{2})$\"\n  tribe_state &lt;- str_match(text_line, regex_pattern)\n  return(tribe_state)\n}\n\nis_valid_tribe_line &lt;- function(text_line) {\n  regex_pattern &lt;- \"^\\\\d+\\\\s+\"\n  return(!is.na(str_match(text_line, regex_pattern)))\n}\n\n# Process text data to extract tribes and states\ntribe_states &lt;- sapply(tribe_text, tribe_state_extractor)\nvalid_lines &lt;- sapply(tribe_text, is_valid_tribe_line)\ntribe_states &lt;- tribe_states[valid_lines, 2:3]\n\n# Count the number of tribes per state\ntribe_data &lt;- as.data.frame(tribe_states)\ncolnames(tribe_data) &lt;- c(\"Tribe\", \"State\")\nstate_counts &lt;- tribe_data %&gt;%\n  group_by(State) %&gt;%\n  summarise(Count = n())\n\nprint(state_counts)\n</code></pre> <pre><code>## # A tibble: 0 \u00d7 2\n## # \u2139 2 variables: State &lt;chr&gt;, Count &lt;int&gt;\n</code></pre> <p>Python: In Python, we\u2019ll use the \u2018PyPDF2\u2019 library to extract text from the PDF and the \u2018re\u2019 module to process the text data.</p> <p>Python code:</p> <pre><code># Install necessary libraries\nimport requests\nimport PyPDF2\nimport io\nimport re\nfrom collections import Counter\n\n# Download the BIA PDF\nurl = \"https://www.bia.gov/sites/bia.gov/files/assets/public/raca/online-tribal-leaders-directory/tribal_leaders_2021-12-27.pdf\"\nresponse = requests.get(url)\n\n# Extract text from the PDF\npdf_reader = PyPDF2.PdfFileReader(io.BytesIO(response.content))\ntribe_text = [pdf_reader.getPage(i).extractText() for i in range(3, pdf_reader.numPages)]\n\n# Process text data to extract tribes and states\ntribes = [re.findall(r'^\\d+\\s+(.+),\\s+([A-Z]{2})', line) for text in tribe_text for line in text.split('\\n') if line]\ntribe_states = [state for tribe, state in tribes]\n\n# Count the number of tribes per state\nstate_counts = Counter(tribe_states)\nprint(state_counts)\n</code></pre> <p>In conclusion, both R and Python offer efficient ways to download and process the list of Indian Entities Recognized and Eligible To Receive Services from the BIA. The \u2018pdftools\u2019 package in R provides a simple way to extract text from PDF files, while the \u2018PyPDF2\u2019 library in Python offers similar functionality. The \u2018stringr\u2019 package in R and the \u2018re\u2019 module in Python can be used to process and analyze text data. Depending on your preferred programming language and environment, both options can be effective for working with BIA data.</p>"},{"location":"additional-resources/bilingualism_md/#national-atlas-indian-lands-of-the-united-states-dataset","title":"National Atlas - Indian Lands of the United States dataset","text":"<p>In this example, we will download and analyze the National Atlas - Indian Lands of the United States dataset in both R and Python. We will read the dataset and count the number of Indian lands per state.</p> <p>R: In R, we\u2019ll use the \u2018sf\u2019 package to read the Shapefile and the \u2018dplyr\u2019 package to process the data.</p> <p>R code:</p> <pre><code># Install and load necessary libraries\n\nlibrary(sf)\nlibrary(dplyr)\n\n# Download the Indian Lands dataset\nurl &lt;- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00968.tar.gz\"\ntemp_file &lt;- tempfile(fileext = \".tar.gz\")\ndownload.file(url, temp_file, mode = \"wb\")\nuntar(temp_file, exdir = tempdir())\n\n# Read the Shapefile\nshapefile_path &lt;- file.path(tempdir(), \"indlanp010g.shp\")\nindian_lands &lt;- read_sf(shapefile_path)\n\n# Count the number of Indian lands per state\n# state_counts &lt;- indian_lands %&gt;%\n#   group_by(STATE) %&gt;%\n#   summarize(count = n())\n\nplot(indian_lands)\n</code></pre> <pre><code>## Warning: plotting the first 9 out of 23 attributes; use max.plot = 23 to plot\n## all\n</code></pre> <p></p> <p>Python: In Python, we\u2019ll use the \u2018geopandas\u2019 and \u2018pandas\u2019 libraries to read the Shapefile and process the data.</p> <p>Python code:</p> <pre><code>import geopandas as gpd\nimport pandas as pd\nimport requests\nimport tarfile\nimport os\nfrom io import BytesIO\n\n# Download the Indian Lands dataset\nurl = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00966.tar.gz\"\nresponse = requests.get(url)\ntar_file = tarfile.open(fileobj=BytesIO(response.content), mode='r:gz')\n\n# Extract Shapefile\ntemp_dir = \"temp\"\nif not os.path.exists(temp_dir):\n    os.makedirs(temp_dir)\n\ntar_file.extractall(path=temp_dir)\nshapefile_path = os.path.join(temp_dir, \"indlanp010g.shp\")\n\n# Read the Shapefile\nindian_lands = gpd.read_file(shapefile_path)\n\n# Count the number of Indian lands per state\nstate_counts = indian_lands.groupby(\"STATE\").size().reset_index(name=\"count\")\n\nprint(state_counts)\n</code></pre> <p>Both R and Python codes download the dataset and read the Shapefile using the respective packages. They then group the data by the \u2018STATE\u2019 attribute and calculate the count of Indian lands per state.</p>"},{"location":"additional-resources/code-of-conduct/","title":"Code of Conduct and Respectful Inclusive Collaboration Guidelines","text":"<p>Environmental Data Science Innovation &amp; Inclusion Lab (ESIIL) is committed to building, maintaining, and fostering an inclusive, kind, collaborative, and diverse transdisciplinary environmental data science community, whose members feel welcome, supported, and safe to contribute ideas and knowledge.</p> <p>The 2024 ESIIL Innovation Summit will follow all aspects of the ESIIL Code of Conduct (below).</p> <p>All community members are responsible for creating this culture, embodying our values, welcoming diverse perspectives and ways of knowing, creating safe inclusive spaces, and conducting ethical science as guided by FAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Authority to Control, Responsibility, and Ethics) principles for scientific and Indigenous data management, governance, and stewardship.</p>"},{"location":"additional-resources/code-of-conduct/#our-values","title":"Our values","text":"<p>ESIIL\u2019s vision is grounded in the conviction that innovation and breakthroughs in environmental data science will be precipitated by a diverse, collaborative, curious, and inclusive research community empowered by open data and infrastructure, cross-sector and community partnerships, team science, and engaged learning.</p> <p>As such, our core values center people through inclusion, kindness, respect, collaboration, and genuine relationships. They also center innovation, driven by collaborative, cross-sector science and synthesis, open, accessible data and tools, and fun, diverse teams. Finally, they center learning, propelled by curiosity and accessible, inclusive training, and education opportunities.</p>"},{"location":"additional-resources/code-of-conduct/#when-and-how-to-use-these-guidelines","title":"When and how to use these guidelines","text":"<p>These guidelines outline behavior expectations for ESIIL community members. Your participation in the ESIIL network is contingent upon following these guidelines in all ESIIL activities, including, but not limited to, participating in meetings, webinars, hackathons, working groups, hosted or funded by ESIIL, as well as email lists and online forums such as GutHub, Slack, and Twitter. These guidelines have been adapted from those of the International Arctic Research Policy Committee, the Geological Society of America, the American Geophysical Union, the University Corporation for Atmospheric Research, The Carpentries, and others. We encourage other organizations to adapt these guidelines for use in their own meetings.</p> <p>Note: Working groups and hackathon/codefest teams are encouraged to discuss these guidelines and what they mean to them, and will have the opportunity to add to them to specifically support and empower their team. Collaborative and behavior commitments complement data use, management, authorship, and access plans that commit to CARE and FAIR principles.</p>"},{"location":"additional-resources/code-of-conduct/#behavior-agreements","title":"Behavior Agreements","text":"<p>ESIIL community members are expected to act professionally and respectfully in all activities, such that each person, regardless of gender, gender identity or expression, sexual orientation, disability, physical appearance, age, body size, race, religion, national origin, ethnicity, level of experience, language fluency, political affiliation, veteran status, pregnancy, country of origin, and any other characteristic protected under state or federal law, feels safe and welcome in our activities and community. We gain strength from diversity and actively seek participation from those who enhance it.</p> <p>In order to garner the benefits of a diverse community and to reach the full potential of our mission and charge, ESIIL participants must be allowed to develop a sense of belonging and trust within a respectful, inclusive, and collaborative culture. Guiding behaviors that contribute to this culture include, but are not limited to:</p>"},{"location":"additional-resources/code-of-conduct/#showing-respect","title":"Showing Respect","text":"<ul> <li> <p>Listen carefully \u2013 we each bring our own styles of communication, language, and ideas, and we must do our best to accept and accommodate differences. Do not interrupt when someone is speaking and maintain an open mind when others have different ideas than yours.</p> </li> <li> <p>Be present \u2013 when engaging with others, give them your full attention. If you need to respond to outside needs, please step away from the group quietly.</p> </li> <li> <p>Be kind \u2013 offer positive, supportive comments and constructive feedback. Critique ideas, not people. Harassment, discrimination, bullying, aggression, including offensive comments, jokes, and imagery, are unacceptable, regardless of intent, and will not be tolerated.</p> </li> <li> <p>Be punctual - adhere to the schedule provided by the organizers and avoid disruptive behavior during presentations, trainings, or working sessions.</p> </li> <li> <p>Respect privacy - be mindful of the confidentiality of others. Always obtain explicit consent before recording, sharing, or using someone else\u2019s personal information, photos, or recordings.</p> </li> <li> <p>Practice good digital etiquette (netiquette) when communicating online, whether in emails, messages, or social media - think before posting online and consider the potential impact on others. Do not share or distribute content generated by or involving others without their explicit consent.</p> </li> </ul>"},{"location":"additional-resources/code-of-conduct/#being-inclusive","title":"Being Inclusive","text":"<ul> <li> <p>Create space for everyone to participate \u2013 be thoughtful about who is at the table; openly address accessibility needs, and provide multiple ways to contribute.</p> </li> <li> <p>Be welcoming \u2013 ESIIL participants come from a wide range of skill levels and career stages, backgrounds, and cultures. Demonstrate that you value these different perspectives and identities through your words and actions, including through correct use of names, titles, and pronouns.</p> </li> <li> <p>Be self-aware \u2013 recognize that positionality, identity, unconscious biases, and upbringing can all affect how words and behaviors are perceived. Ensure that your words and behavior make others feel welcome.</p> </li> <li> <p>Commit to ongoing learning \u2013 the move toward inclusive, equitable, and just environmental data science is a collective journey. Continue to learn about and apply practices of inclusion, anti-racism, bystander intervention, and cultural sensitivity. None of us is perfect; all of us will, from time to time, fail to live up to our own high standards. Being perfect is not what matters; owning our mistakes and committing to clear and persistent efforts to grow and improve is.</p> </li> </ul>"},{"location":"additional-resources/code-of-conduct/#being-curious","title":"Being Curious","text":"<ul> <li> <p>Check your presumptions \u2013 we each bring our own ideas and assumptions about how the world should and does work \u2013 what are yours, and how do they affect how you interact with others? How do they shape your perception of new ideas?</p> </li> <li> <p>Ask questions \u2013 one of the strengths of interdisciplinary and diverse teams is that we all bring different knowledge and viewpoints; no one person is expected to know everything. So don\u2019t be afraid to ask, to learn, and to share.</p> </li> <li> <p>Be bold \u2013 significant innovations don\u2019t come from incremental efforts. Be brave in proposing and testing new ideas. When things don\u2019t work, learn from the experience.</p> </li> <li> <p>Invite feedback \u2013 new ideas and improvements can emerge from many places when we\u2019re open to hearing them. Check your defensiveness and listen; accept feedback as a gift toward improving our work and ourselves.</p> </li> </ul>"},{"location":"additional-resources/code-of-conduct/#being-collaborative","title":"Being Collaborative","text":"<ul> <li> <p>Recognize that everyone is bringing something different to the table \u2013 take the time to get to know each other. Keep an open mind, encourage ideas that are different from yours, and learn from each other\u2019s expertise and experience.</p> </li> <li> <p>Be accountable - great team science depends on trust, communication, respect, and delivering on your commitments. Be clear about your needs, as both a requester and a responder, realistic about your time and capacity commitments, and communicate timelines and standards in advance.</p> </li> <li> <p>Make assumptions explicit and provide context wherever possible - misunderstandings are common on transdisciplinary and cross-cultural teams and can best be managed with intentionality. Check in about assumptions, and be willing to share and correct misunderstandings or mistakes when they happen. Make use of collaboration agreements, communicate clearly and avoid jargon wherever possible.</p> </li> <li> <p>Respect intellectual property and Indigenous data sovereignty \u2013 ESIIL recognizes the extractive and abusive history of scientific engagement with Native peoples, and is committed to doing better. Indigenous knowledge holders are under no obligation to share their data, stories or knowledge. Their work should always be credited, and only shared with permission. Follow guidelines for authorship, Indigenous data sovereignty, and CARE principles. Acknowledge and credit the ideas and work of others.</p> </li> <li> <p>Use the resources that we provide - take advantage of the cyberinfrastructure and data cube at your disposal, but do not use them for unrelated tasks, as it could disrupt the event, introduce security risks, undermine the spirit of collaboration and fair play, and erode trust within the event community.</p> </li> <li> <p>Be safe - never share sensitive personal information; use strong passwords for your Cyverse and GitHub accounts and do not share them with other participants; be cautious of unsolicited emails, messages, or links; and verify online contacts. If you encounter any illegal or harmful activities online related to this event, report them to Tyler McIntosh or Susan Sullivan.</p> </li> </ul> <p>Finally, speak up if you experience or notice a dangerous situation, or someone in distress!</p>"},{"location":"additional-resources/code-of-conduct/#code-of-conduct-unacceptable-behaviors","title":"Code of Conduct: Unacceptable behaviors","text":"<p>We adopt the full Code of Conduct of our home institution, the University of Colorado, details of which are found here. To summarize, examples of unacceptable and reportable behaviors include, but are not limited to:</p> <ul> <li>Harassment, intimidation, or discrimination in any form</li> <li>Physical or verbal abuse by anyone to anyone, including but not limited to a participant, member of the public, guest, member of any institution or sponsor</li> <li>Unwelcome sexual attention or advances</li> <li>Personal attacks directed at other guests, members, participants, etc.</li> <li>Alarming, intimidating, threatening, or hostile comments or conduct</li> <li>Inappropriate use of nudity and/or sexual images in public spaces or in presentations</li> <li>Threatening or stalking anyone</li> <li>Unauthorized use or sharing of personal or confidential information or private communication</li> <li>Continuing interactions, including but not limited to conversations, photographies, recordings, instant messages, and emails, after being asked to stop</li> <li>Ethical and scientific misconduct, including failing to credit contributions or respect intellectual property</li> <li>Engaging in any illegal activities, including hacking, cheating, or unauthorized access to systems or data</li> <li>Using the cyberinfrastructure provided by the organizers for activities unrelated to this event.</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting. </li> </ul> <p>The University of Colorado recognizes all Federal and State protected classes, which include the following: race, color, national origin, sex, pregnancy, age, marital status, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Mistreatment or harassment not related to protected class also has a negative impact and will be addressed by the ESIIL team.</p> <p>Anyone requested to stop unacceptable behavior is expected to comply immediately.</p> <p>If there is a clear violation of the code of conduct during an ESIIL event\u2014for example, a meeting is Zoom bombed or a team member is verbally abusing another participant during a workshop\u2014 ESIIL leaders, facilitators (or their designee) or campus/local police may take any action deemed necessary and appropriate, including expelling the violator, or immediate removal of the violator from any online or in-person event or platform without warning or refund. If such actions are necessary, there will be follow up with the ESIIL Diversity Equity and Inclusion (DEI) team to determine what further action is needed (see Reporting Process and Consequences below).</p>"},{"location":"additional-resources/code-of-conduct/#addressing-behavior-directly","title":"Addressing Behavior Directly","text":"<p>For smaller incidents that might be settled with a brief conversation, you may choose to contact the person in question or set up a (video) conversation to discuss how the behavior affected you. Please use this approach only if you feel comfortable; you do not have to carry the weight of addressing these issues yourself. If you are interested in this option but unsure how to go about it, please contact the ESIIL DEI lead, Susan Sullivan, first\u2014she will have advice on how to make the conversation happen and is available to join you in a conversation as requested.</p>"},{"location":"additional-resources/code-of-conduct/#reporting-process-and-consequences","title":"Reporting Process and Consequences","text":"<p>We take any reports of Code of Conduct violations seriously, and aim to support those who are impacted and ensure that problematic behavior doesn\u2019t happen again.</p>"},{"location":"additional-resources/code-of-conduct/#making-a-report","title":"Making a Report","text":"<p>If you believe you\u2019re experiencing or have experienced unacceptable behavior that is counter to this code of conduct, or you are witness to this behavior happening to someone else, we encourage you to contact our DEI lead:</p> <ul> <li>Susan Sullivan, CIRES</li> <li>Email: susan.sullivan@colorado.edu</li> </ul> <p>You may also choose to anonymously report behavior to ESIIL using this form.</p> <p>The DEI team will keep reports as confidential as possible. However, as mandatory reporters, we have an obligation to report alleged protected class violations to our home institution or to law enforcement.</p>"},{"location":"additional-resources/code-of-conduct/#specifically","title":"Specifically:","text":"<ul> <li>Cases of potential protected-class harassment will be reported to the CU Office of Institutional Equity and Compliance.</li> <li>If the violation is made by a member of another institution, that information may also be shared with that member\u2019s home institution by the CU Office of Institutional Equity and Compliance under Title IX.</li> <li>In some instances, harassment information may be shared with the National Science Foundation, who are the funding organization of ESIIL.</li> </ul> <p>When we discuss incidents with people who are accused of misconduct (the respondent), we will anonymize details as much as possible to protect the privacy of the reporter and the person who was impacted (the complainant). In some cases, even when the details are anonymized, the respondent may guess at the identities of the reporter and complainants. If you have concerns about retaliation or your personal safety, please let us know (or note that in your report). We encourage you to report in any case, so that we can support you while keeping ESIIL members safe. In some cases, we are able to compile several anonymized reports into a pattern of behavior, and take action based on that pattern.</p> <p>If you prefer to speak with someone who is not on the ESIIL leadership team, or who can maintain confidentiality, you may contact:</p> <ul> <li>CU Ombuds</li> <li>Phone: 303-492-5077 (for guidance and support navigating difficult conversations)</li> <li>CU Office of Victim Assistance</li> <li>Phone: 303-492-8855 </li> </ul> <p>If you want more information about when to report, or how to help someone who needs to report, please review the resources at Don\u2019t Ignore It.</p> <p>Note: The reporting party does not need to be directly involved in a code of conduct violation incident. Please make a bystander report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines, even if the situation is not happening to you.</p>"},{"location":"additional-resources/code-of-conduct/#what-happens-after-a-report-is-filed","title":"What Happens After a Report Is Filed","text":"<p>After a member of the ESIIL DEI team takes your report, they will (if necessary) consult with the appropriate support people at CU. The ESIIL DEI team will respond with a status update within 5 business days.</p> <p>During this time, they, or members of the CU Office of Institutional Equity and Compliance, will:</p> <ul> <li>Meet with you or review report documentation to determine what happened</li> <li>Consult documentation of past incidents for patterns of behavior</li> <li>Discuss appropriate response(s) to the incident</li> <li>Connect with the appropriate offices and/or make those response(s)</li> <li>Determine the follow-up actions for any impacted people and/or the reporter</li> <li>Follow up with the impacted people, including connecting them with support and resources.</li> </ul>"},{"location":"additional-resources/code-of-conduct/#as-a-result-of-this-process-in-minor-cases-esiil-dei-may-communicate-with-the-respondent-to","title":"As a result of this process, in minor cases ESIIL DEI may communicate with the respondent to:","text":"<ul> <li>Explain what happened and the impact of their behavior</li> <li>Offer concrete examples of how to improve their behavior</li> <li>Explain consequences of their behavior, or future consequences if the behavior is repeated.</li> </ul> <p>For significant infractions, follow up to the report may be turned over to the CU Office of Institutional Equity and Compliance and/or campus police.</p>"},{"location":"additional-resources/code-of-conduct/#possible-consequences-to-code-of-conduct-violations","title":"Possible Consequences to Code of Conduct Violations","text":"<p>What follows are examples of possible responses to an incident report. This list is not inclusive, and ESIIL reserves the right to take any action it deems necessary. Generally speaking, the strongest response ESIIL may take is to completely ban a user from further engagement with ESIIL activities and, as is required, report a person to the CU Office of Institutional Equity and Compliance and/or their home institution and NSF. If law enforcement should be involved, they will recommend that the complainant make that contact. Employees of CU Boulder may also be subject to consequences as determined by the institution.</p> <p>In addition to the responses above, ESIIL responses may include but are not limited to the following:</p> <ul> <li>A verbal discussion in person or via phone/Zoom followed by documentation of the conversation via email</li> <li>Not publishing the video or slides of a talk that violated the code of conduct</li> <li>Not allowing a speaker who violated the code of conduct to give (further) talks</li> <li>Immediately ending any team leadership, membership, or other responsibilities and privileges that a person holds</li> <li>Temporarily banning a person from ESIIL activities</li> <li>Permanently banning a person from ESIIL activities</li> <li>Nothing, if the behavior is determined to not be a code of conduct violation</li> </ul> <p>Do you need more resources?</p> <p>Please don\u2019t hesitate to contact the ESIIL DEI lead, Susan Sullivan, if you have questions or concerns.</p> <p>The CU Office of Institutional Equity and Compliance is a resource for all of us in navigating this space. They also offer resource materials that can assist you in exploring various topics and skills here.</p> <p>If you have questions about what, when or how to report, or how to help someone else with concerns, Don\u2019t Ignore It.</p> <p>CU Ombud\u2019s Office: Confidential support to navigate university situations. (Most universities have these resources)</p> <p>The CU Office of Victims Assistance (counseling limited to CU students/staff/faculty, though advocacy is open to everyone engaged with a CU-sponsored activity. Please look for a similar resource on your campus if you are from another institution).</p> <p>National Crisis Hotlines</p> <p>How are we doing?</p> <p>Despite our best intentions, in some cases we may not be living up to our ideals of a positive, supportive, inclusive, respectful and collaborative community. If you feel we could do better, we welcome your feedback. Comments, suggestions and praise are also very welcome! Acknowledgment By participating in this event, you agree to abide by this code of conduct and understand the consequences of violating it. We believe that a respectful and inclusive environment benefits all participants and leads to more creative and successful outcomes. Thank you for your cooperation in making the this event a welcoming event for all. Have fun!</p>"},{"location":"additional-resources/cyverse_hacks/","title":"Transitioning Workflows to CyVerse: Tips &amp; Tricks","text":""},{"location":"additional-resources/cyverse_hacks/#forest-carbon-codefest-data-storage","title":"Forest Carbon Codefest Data Storage","text":"<ul> <li>Path: <code>~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/</code></li> <li>Your team has a subdirectory within the Team_outputs directory.</li> </ul>"},{"location":"additional-resources/cyverse_hacks/#setup","title":"Setup","text":"<ol> <li>CyVerse Account:</li> <li>Create an account if not already owned.</li> <li>Contact Tyson for account upgrades after maximizing current limits.</li> </ol>"},{"location":"additional-resources/cyverse_hacks/#github-connection","title":"GitHub Connection","text":"<ul> <li>Follow the guide for connecting GitHub to CyVerse</li> <li>Select \u201cJupyterLab ESIIL\u201d and choose \u201cmacrosystems\u201d in the version dropdown.</li> <li>Clone into <code>/home/jovyan/data-store</code>.</li> <li>Clone <code>innovation-summit-utils</code> for SSH connection to GitHub.</li> <li>Run <code>conda install -c conda-forge openssh</code> in the terminal if encountering errors.</li> <li>GitHub authentication is session-specific.</li> </ul>"},{"location":"additional-resources/cyverse_hacks/#rstudio-in-discovery-environment","title":"RStudio in Discovery Environment","text":"<ol> <li>Copy your instance ID. It can be found in your analyis URL in form https://.cyverse.run/lab. <li>Use your ID in these links and run them each, in sequence, in the same browser window:  </li> <li><code>https://&lt;id&gt;.cyverse.run/rstudio/auth-sign-in</code> </li> <li><code>https://&lt;id&gt;.cyverse.run/rstudio/</code> </li>"},{"location":"additional-resources/cyverse_hacks/#data-transfer-to-cyverse","title":"Data Transfer to CyVerse","text":"<ul> <li>Use GoCommands for HPC/CyVerse transfers.</li> <li>Installation:</li> <li>Linux: GOCMD_VER=\\((curl -L -s https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt); \\ curl -L -s https://github.com/cyverse/gocommands/releases/download/\\)-linux-amd64.tar.gz | tar zxvf -}/gocmd-${GOCMD_VER</li> <li>Windows Powershell: curl -o gocmdv.txt https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt ; \\(env:GOCMD_VER = (Get-Content gocmdv.txt) curl -o gocmd.zip https://github.com/cyverse/gocommands/releases/download/\\)env:GOCMD_VER/gocmd-$env:GOCMD_VER-windows-amd64.zip ; tar zxvf gocmd.zip ; del gocmd.zip ; del gocmdv.txt</li> <li>Usage: </li> <li>./gocmd init</li> <li>Hit enter until you are asked for your iRODS Username (which is your cyverse username)</li> <li>Use <code>put</code> for upload and <code>get</code> for download.</li> <li>Ensure correct CyVerse directory path. Note that the CyVerse directory path should start from \u201c/iplant/home/\u2026\u201d (i.e. if you start from \u2018/home/jovyan/\u2026\u2019 GoCommands will not find the directory and throw an error)</li> </ul>"},{"location":"additional-resources/participant_agreement/","title":"Participant Agreement","text":"<p>This Participant Agreement (\u201cAgreement\u201d) is a contract between you (\u201cYou/Your\u201d or \u201cParticipant\u201d) and THE REGENTS OF THE UNIVERSITY OF COLORADO, a body corporate, acting on behalf of the University of Colorado Boulder, a public institution of higher education created under the Constitution and the Law of the State of Colorado (the \u201cUniversity\u201d), having offices located at 3100 Marine Street, Boulder, CO 80309.</p> <p>In consideration of Your participation in the 2024 ESIIL Innovation Summit, the sufficiency of which is hereby acknowledged, You agree as follows:</p> <p>Environmental Data Science Innovation &amp; Inclusion Lab (\u201cESIIL\u201d) is a National Science Foundation (\u201cNSF\u201d) funded data synthesis center led by the University. Earth Lab is part of the Cooperative Institute for Research in Environmental Sciences (CIRES) specializing in data-intensive open, reproducible environmental science. ESIIL will host the Summit in person from May 13 through May 16, 2024.</p>"},{"location":"additional-resources/participant_agreement/#innovation-summit-description","title":"Innovation Summit Description","text":"<p>ESIIL's 2024 Innovation Summit will offer an opportunity to use big data to understand resilience across genes, species, ecosystems and societies, advance ecological forecasting with solutions in mind, and inform adaptive management and natural climate solutions. The Summit will support attendees to advance data-informed courses of action for resilience and adaptation in the face of our changing environment. It will be an in-person \u2018unconference\u2019, enabling participants to dynamically work on themes that most inspire them, with inclusive physical and intellectual spaces for working together. Over two and a half days participants will work in teams to explore research questions using open science approaches, including: data infrastructure, artificial intelligence (AI) and novel analytics, and cloud computing. Participants will be encouraged to work across and respect different perspectives, with the aim of co-developing resilience solutions. ESIIL will provide participants with opportunities to learn more about cultural intelligence, ethical and open science practices, and leadership in the rapidly evolving field of environmental data science. Overall, the Summit will capitalize on the combination of open data and analytics opportunities to develop innovative or impactful approaches that improve environmental resilience and adaptation.</p>"},{"location":"additional-resources/participant_agreement/#how-to-participate","title":"How to Participate","text":"<p>You will join a team of environmental scientists, data experts, and coders to explore curated data, consider the objectivity of the data, propose a scientific question that can be addressed with all or some of the data sets, and analyze the data in an attempt to answer your scientific question. You will present your Work to the event community. ESIIL will provide environmental data, cyberinfrastructure, cyberinfrastructure and data analytics training, and technical support. </p>"},{"location":"additional-resources/participant_agreement/#representations-and-warranties","title":"Representations and Warranties","text":"<p>By and through Your participation in the Summit, You represent and warrant the following:</p> <ul> <li>You have read, understand, and agree to abide by the Code of Conduct and Respectful Inclusive Collaboration Guidelines for the 2024 ESIIL Innovation Summit (\u201cCode of Conduct\u201d).</li> <li>Any decisions concerning the Code of Conduct, Official Rules, or any other matter relating to this Summit by the University is final and binding on all Participants.</li> </ul>"},{"location":"additional-resources/participant_agreement/#summit-assets","title":"Summit Assets","text":""},{"location":"additional-resources/participant_agreement/#51-access-and-use","title":"5.1 Access and Use","text":"<p>By participating in the Innovation Summit, You may receive access to certain datasets, webinars, and/or other copyrighted materials (collectively, the \u201cSummit Assets\u201d). You agree to follow all licenses, restrictions, and other instructions provided to You with the Summit Assets.</p>"},{"location":"additional-resources/participant_agreement/#52-disclaimer","title":"5.2 Disclaimer","text":"<p>The Summit Assets are provided \u201cas is\u201d without warranty of any kind, either express or implied, including, without limitation, any implied warranties of merchantability and fitness for a particular purpose. Without limiting the foregoing, the University does not warrant that the Materials will be suitable for Your Solution or that the operation or supply of the Summit Assets will be uninterrupted or error free.</p>"},{"location":"additional-resources/participant_agreement/#53-restrictions","title":"5.3 Restrictions","text":"<p>You agree not to access or use the Summit Assets in a manner that may interfere with any other participants\u2019 or users\u2019 use of such assets, unless provided with express written consent by the University. Your access to and use of the Summit Assets may be limited, throttled, or terminated at any time at the sole discretion of the University.</p>"},{"location":"additional-resources/participant_agreement/#54-originality-and-third-party-materials","title":"5.4 Originality and Third-Party Materials","text":"<p>You represent that Your Work is Your original creation. If you obtain permission to include third-party materials, You represent that Your Work includes complete details of any third-party license or other restriction (including, but not limited to, related patents and trademarks) of which You are aware and which are associated with any part of Your Work. You represent and warrant that You will not submit any materials to the University that You know or believe to have components that are malicious or harmful. You represent that You will perform a reasonable amount of due diligence in order to be properly informed of third-party licenses, infringing materials, or harmful content associated with any part of Your Work.</p>"},{"location":"additional-resources/participant_agreement/#55-work-publication","title":"5.5 Work Publication","text":"<p>You agree to make Your Work publicly available in GitHub under the MIT open-source license within five (5) months from the end of the Summit.</p>"},{"location":"additional-resources/participant_agreement/#limitation-of-liability","title":"Limitation of Liability","text":"<p>TO THE EXTENT ALLOWED BY LAW, IN NO EVENT SHALL THE UNIVERSITY, ITS PARTNERS, LICENSORS, SERVICE PROVIDERS, OR ANY OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES OR REPRESENTATIVES, BE LIABLE FOR DIRECT, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY OR PUNITIVE DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE SUMMIT OR THIS AGREEMENT (HOWEVER ARISING, INCLUDING NEGLIGENCE). IF YOU HAVE A DISPUTE WITH ANY PARTICIPANT OR ANY OTHER THIRD PARTY, YOU RELEASE THE UNIVERSITY, ITS, PARTNERS, LICENSORS, AND SERVICE PROVIDERS, AND EACH OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES AND REPRESENTATIVES FROM ANY AND ALL CLAIMS, DEMANDS AND DAMAGES (ACTUAL AND CONSEQUENTIAL) OF EVERY KIND AND NATURE ARISING OUT OF OR IN ANY WAY CONNECTED WITH SUCH DISPUTES. YOU AGREE THAT ANY CLAIMS AGAINST UNIVERSITY ARISING OUT OF THE SUMMIT OR THIS AGREEMENT MUST BE FILED WITHIN ONE YEAR AFTER SUCH CLAIM AROSE; OTHERWISE, YOUR CLAIM IS PERMANENTLY BARRED.</p>"},{"location":"additional-resources/participant_agreement/#not-an-offer-or-contract-of-employment","title":"Not an Offer or Contract of Employment","text":"<p>Under no circumstances will Your participation in the Summit or anything in this Agreement be construed as an offer or contract of employment with the University.</p>"},{"location":"additional-resources/participant_agreement/#additional-terms","title":"Additional Terms","text":"<ul> <li>You must be at least eighteen (18) years of age to participate in the Summit. The Summit is subject to applicable federal, state, and local laws.</li> <li>The University reserves the right to permanently disqualify any person from the Summit that it reasonably believes has violated this Agreement, the Code of Conduct, and/or the Official Rules.</li> <li>Any attempt to deliberately damage the Summit or the operation thereof is unlawful and subject to legal action by the University, which may seek damages to the fullest extent permitted by law.</li> <li>The University assumes no responsibility for any injury or damage to Your or any other person\u2019s computer relating to or resulting from entering or downloading materials or software in connection with the Summit. </li> <li>The University is not responsible for telecommunications, network, electronic, technical, or computer failures of any kind; for inaccurate transcription of entry information; for any human or electronic error; or for Solutions that are stolen, misdirected, garbled, delayed, lost, late, damaged, or returned.</li> <li>The University reserves the right to cancel, modify, or suspend the Summit or any element thereof (including, without limitation, this Agreement) without notice in any manner and for any reason (including, without limitation, in the event of any unanticipated occurrence that is not fully addressed in this Agreement).</li> <li>The University may prohibit any person from participating in the Summit, if such person shows a disregard for this Agreement; acts with an intent to annoy, abuse, threaten, or harass any other entrant or any agents or representatives of the University (or any associated, partners, licensors, or service providers for the University); or behaves in any other disruptive manner (as determined by the University in its sole discretion).</li> <li>Nothing contained in this Agreement shall be construed as an express or implied waiver by University of its governmental immunity or of the governmental immunity of the State of Colorado.</li> <li>Your Work shall not contain any item(s) that are either export-controlled under the International Traffic in Arms Regulations, or that appear on the Commerce Control List (except as EAR99) of the Export Administration Regulations.</li> </ul>"},{"location":"additional-resources/participant_agreement/#dispute-resolution","title":"Dispute Resolution","text":"<p>This Agreement and the Summit shall be governed and construed in accordance with and governed by the laws of the state of Colorado without giving effect to conflict of law provisions.</p>"},{"location":"additional-resources/participant_agreement/#entire-agreement","title":"Entire Agreement","text":"<p>This Agreement and the Event Code of Conduct, constitutes the entire agreement between the University and You with respect to the Summit and supersedes all previous or contemporaneous oral or written agreements concerning the Summit. In the event of a conflict between this Agreement and/or the Event Code of Conduct, the conflict shall be resolved with the following order of precedence:</p> <ol> <li>This Agreement</li> <li>The Event Code of Conduct</li> </ol>"},{"location":"additional-resources/participant_agreement/#severability","title":"Severability","text":"<p>The invalidity, illegality, or unenforceability of any one or more phrases, sentences, clauses, or sections in this Agreement does not affect the remaining portions of this Agreement.</p> <p>If you have questions about the Summit, please contact ESIIL at esiil@colorado.edu.</p>"},{"location":"additional-resources/participant_agreement/#guidelines-for-intellectual-contributions-and-credit","title":"Guidelines for Intellectual Contributions and Credit","text":"<p>ESIIL Guidelines for Intellectual Contributions and Credit</p>"},{"location":"additional-resources/useful_links/","title":"Useful links","text":"<p>CyVerse User Portal</p> <p>GitHub</p> <p>ESIIL Website</p> <p>2024 Summit Slack</p>"},{"location":"collaborating-on-the-cloud/cyverse-instructions/","title":"Connecting to Cyverse and GitHub","text":""},{"location":"collaborating-on-the-cloud/cyverse-instructions/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account). When you make this account, please use the email that you have been using to communicate with our team regarding the event. That email is attached to our CyVerse workshop.</li> </ol> <ol> <li>Log in to Cyverse https://user.cyverse.org/ with your new account.</li> </ol> <ol> <li>From your account, go to the navigation bar at left and select 'Workshops'</li> </ol> <ol> <li>From the workshop page, find the workshop titled \"Forest Carbon Codefest\". It should look like this:</li> </ol> <ol> <li>Click on the tile, and then on the page for the workshop, click, \"Enroll\" at upper right. You should be enrolled automatically if you are using the email you have given our team.</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment by clicking on 'Services' at the upper right and then 'Discovery Environment' under 'My Services'.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p>"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu</li> </ol> <ol> <li>Select <code>JupyterLab ESIIL</code></li> </ol> <ol> <li>Configure and launch your analysis - the defaults are fine for now:</li> </ol> <ol> <li>Click <code>Go to analysis</code>:</li> </ol> <ol> <li>Now you should see Jupyter Lab!    </li> </ol>"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"collaborating-on-the-cloud/cyverse-instructions/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li>From Jupyter Lab, click on the GitHub icon on the left menu:</li> </ol> <ol> <li>Click <code>Clone a Repository</code>:</li> </ol> <ol> <li>Paste the link to the innovation-summit-utils https://github.com/CU-ESIIL/innovation-summit-utils.git and click <code>Clone</code>:</li> </ol> <ol> <li>You should now see the <code>innovation-summit-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code></li> </ol> <ol> <li>Go into the <code>innovation-summit-utils</code> folder:</li> </ol> <ol> <li>open up the <code>create_github_keypair.ipynb</code> notebook by double-clicking:</li> </ol> <ol> <li>Select the default kernel</li> </ol> <ol> <li>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:</li> </ol> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):</p> <p></p> </li> <li> <p>Select <code>SSH and GPG keys</code></p> <p></p> </li> <li> <p>Select <code>New SSH key</code></p> <p></p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:</p> <p></p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!</p> <p></p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"collaborating-on-the-cloud/cyverse_data_management/","title":"Cyverse data management","text":""},{"location":"collaborating-on-the-cloud/cyverse_data_management/#cloud-to-instance-data-access","title":"Cloud-to-instance data access","text":"<p>The best and most efficient way to access most data from within your Cyverse instance is via APIs, VSI, or STAC. Examples of such data access can be found throughout the data library. This is the preferred method of data access since it keeps data on the cloud, puts it directly on your instance, and then the data is removed upon instance termination. Note that any data you want to keep must be moved off the instance and to the Cyverse data store prior to instance termination (see below, \"Saving data from your instance to the data store\").</p>"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#pre-downloaded-data-on-cyverse-data-store","title":"Pre-downloaded data on Cyverse data store","text":"<p>Some data can be time consuming or frustrating to access. Or, you or one of your teammates may just be much more comfortable working with data that has effectively been 'downloaded locally'. In an attempt to streamline your projects, the ESIIL and Earth Lab teams have loaded a set of data onto the Cyverse data store, which can be read from your Cyverse instance.</p> <p>Pre-downloaded data for the Forest Carbon Codefest can be found in the Cyverse data store at this link.</p> <p>The path directory to this location from within a Cyverse instance is: <pre><code>~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest\n</code></pre> Note that, while data CAN be read on your instance directly from the data store, it is usually best to move the data to your instance prior to reading and processing the data. Having the data directly on your instance will dramatically improve processing time and performance. (see below, \"Moving data from the data store to your instance\")</p>"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#moving-data-from-the-data-store-to-your-instance","title":"Moving data from the data store to your instance","text":"<p>Use the terminal command line interface on your instance to move data from the data store to your instance (whether that is pre-downloaded data or data that you have saved to your team folder). The home directory of your instance is: <pre><code>/home/jovyan\n</code></pre> To do so, open the Terminal from your launcher</p> <p></p> <p>Then, use the 'cp' command to copy data from the data store to your instance. Use the flag -r if you are moving an entire directory or directory structure.</p> <p>The command is in the form: <pre><code>cp -r data-store-location new-location-on-instance\n</code></pre> For example, the below command will move the entire LCMAP_SR_1985-2021 directory to a new data directory on your instance: <pre><code>cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/LCMAP_SR_1985_2021 /home/jovyan/data/\n</code></pre></p>"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#saving-data-from-your-instance-to-the-data-store","title":"Saving data from your instance to the data store","text":"<p>Any data or outputs that you want to keep, such as newly derived datasets or figures, must be moved off the instance and to the Cyverse data store prior to instance termination. To do so, you will follow the same steps as in \"Moving data from the data store to your instance\" (see above), but with the directories in the command reversed.</p> <p>All team outputs should be stored in the subdirectories named TeamX in this directory: <pre><code>~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs\n</code></pre> Each team has their own directory; make sure you are saving to the correct one!</p> <p>For example, if you were on Team1 and wanted to save a figures directory, you could use the below command: <pre><code>cp -r /home/jovyan/figures ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs/Team1/\n</code></pre></p>"},{"location":"collaborating-on-the-cloud/github-basics/","title":"Github essentials","text":""},{"location":"collaborating-on-the-cloud/github-basics/#i-introduction-2-minutes","title":"I. Introduction (2 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-brief-overview-of-github","title":"A. Brief overview of GitHub:","text":"<p>GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-introduce-github-desktop-and-jupyterhub-github-widget","title":"B. Introduce GitHub Desktop and JupyterHub GitHub widget:","text":"<p>GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#1-download-github-desktop","title":"1. Download GitHub Desktop","text":""},{"location":"collaborating-on-the-cloud/github-basics/#step-1-download-github-desktop","title":"Step 1: Download GitHub Desktop","text":"<p>Go to the GitHub Desktop download page: https://desktop.github.com/</p> <p>Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-install-github-desktop","title":"Step 2: Install GitHub Desktop","text":"<p>For Windows:</p> <p>Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer.</p> <p>Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired.</p> <p>Once the installation is complete, GitHub Desktop will launch automatically. For macOS:</p> <p>Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application.</p> <p>Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder.</p> <p>Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-set-up-github-desktop","title":"Step 3: Set up GitHub Desktop","text":"<p>When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join.</p> <p>Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d</p> <p>You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup.</p> <p>Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#1-download-github-for-jupyterhub-cloud-service","title":"1. Download GitHub for JupyterHub cloud service","text":""},{"location":"collaborating-on-the-cloud/github-basics/#step-1-accessing-jupyterhub-on-the-cloud","title":"Step 1: Accessing JupyterHub on the cloud","text":"<p>Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization).</p> <p>Sign in with your credentials or authenticate using a third-party service if required.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-launch-a-new-jupyter-notebook-or-open-an-existing-one","title":"Step 2: Launch a new Jupyter Notebook or open an existing one","text":"<p>Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser.</p> <p>Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-install-and-enable-the-jupyterlab-git-extension","title":"Step 3: Install and enable the JupyterLab Git extension","text":"<p>In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension:</p> <p>!pip install jupyterlab-git</p> <p>Restart the Jupyter Notebook server for the changes to take effect.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-4-using-the-jupyterhub-github-widget","title":"Step 4: Using the JupyterHub GitHub widget","text":"<p>In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget.</p> <p>To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. You can now navigate through the cloned repository, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository.</p> <p>To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface.</p> <p>To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget.</p> <p>Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#c-github-in-rstudio","title":"C. GitHub in Rstudio:","text":"<p>Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-1-install-git","title":"Step 1: Install Git","text":"<p>Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-configure-rstudio-to-work-with-git","title":"Step 2: Configure RStudio to work with Git","text":"<p>Open RStudio.</p> <p>Go to \u201cTools\u201d &gt; \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab.</p> <p>Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory).</p> <p>Click \u201cOK\u201d to save the changes.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-create-or-open-an-rstudio-project","title":"Step 3: Create or open an RStudio project","text":"<p>To create a new RStudio project, go to \u201cFile\u201d &gt; \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project.</p> <p>To open an existing RStudio project, go to \u201cFile\u201d &gt; \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#step-4-link-your-rstudio-project-to-a-github-repository","title":"Step 4: Link your RStudio project to a GitHub repository","text":"<p>In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d &gt; \u201cProject Setup.\u201d</p> <p>In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d</p> <p>A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio.</p> <p>To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository.</p> <p>Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d).</p> <p>In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d &gt; \u201cShell.\u201d</p> <p>In the shell, run the following command to add the remote repository:</p> <p>git remote add origin https://github.com/username/repository.git</p> <p>Replace the URL with the one you copied from your GitHub repository.</p> <p>Push your changes to the remote repository by running the following command in the shell:</p> <p>git push -u origin master</p> <p>Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one.</p> <p>By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#ii-github-basics-4-minutes","title":"II. GitHub Basics (4 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-repository","title":"A. Repository:","text":"<p>A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-fork-and-clone","title":"B. Fork and Clone:","text":"<p>Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#c-branches","title":"C. Branches:","text":"<p>Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#d-replace-master-with-main","title":"D. Replace \u2018master\u2019 with \u2018main\u2019:","text":"<p>In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#iii-collaboration-and-version-control-5-minutes","title":"III. Collaboration and Version Control (5 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-commits","title":"A. Commits:","text":"<p>Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-push","title":"B. Push:","text":"<p>In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress.</p> <p>In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#c-pull-requests","title":"C. Pull Requests:","text":"<p>Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#d-merging-and-resolving-conflicts","title":"D. Merging and Resolving Conflicts:","text":"<p>Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#iv-additional-features-2-minutes","title":"IV. Additional Features (2 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-issues-and-project-management","title":"A. Issues and Project Management:","text":"<p>Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-github-pages","title":"B. GitHub Pages:","text":"<p>GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#v-conclusion-2-minutes","title":"V. Conclusion (2 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-recap-of-the-essentials-of-github","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-encourage-further-exploration-and-learning","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#c-share-resources-for-learning-more-about-github","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#v-conclusion-2-minutes_1","title":"V. Conclusion (2 minutes)","text":""},{"location":"collaborating-on-the-cloud/github-basics/#a-recap-of-the-essentials-of-github_1","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#b-encourage-further-exploration-and-learning_1","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"collaborating-on-the-cloud/github-basics/#c-share-resources-for-learning-more-about-github_1","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p> <p>By Ty Tuff, ESIIL</p>"},{"location":"collaborating-on-the-cloud/markdown_basics/","title":"Markdown for the Modern Researcher at ESIIL","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#introduction","title":"Introduction","text":"<ul> <li>Overview of Markdown's relevance and utility in modern research.</li> <li>How Markdown streamlines documentation in diverse scientific and coding environments.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-1-mastering-markdown-syntax","title":"Section 1: Mastering Markdown Syntax","text":"<ul> <li>Objective: Equip researchers with a thorough understanding of Markdown syntax and its diverse applications.</li> <li>Topics Covered:</li> <li>Fundamentals of Text Formatting (headings, lists, bold, italics)</li> <li>Advanced Structures (tables, blockquotes)</li> <li>Integrating Multimedia (image and video links)</li> <li>Diagrams with Mermaid (creating flowcharts, mind maps, timelines)</li> <li>Interactive Elements (hyperlinks, embedding interactive content)</li> <li>Activities:</li> <li>Crafting a Markdown document with various formatting elements.</li> <li>Developing diagrams using Mermaid for research presentations.</li> <li>Embedding multimedia elements in a Markdown document for enhanced communication.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-2-markdown-in-research-tools","title":"Section 2: Markdown in Research Tools","text":"<ul> <li>Objective: Showcase the integration of Markdown in RStudio and Jupyter Notebooks for scientific documentation.</li> <li>Topics Covered:</li> <li>Implementing Markdown in RStudio (R Markdown, knitting to HTML/PDF)</li> <li>Utilizing Markdown in Jupyter Notebooks (code and Markdown cells)</li> <li>Best practices for documenting research code</li> <li>Including code outputs and visualizations in documentation</li> <li>Activities:</li> <li>Creating and sharing an R Markdown document with annotated research data.</li> <li>Building a comprehensive Jupyter Notebook with integrated Markdown annotations.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-3-disseminating-research-with-markdown-and-github-pages","title":"Section 3: Disseminating Research with Markdown and GitHub Pages","text":"<ul> <li>Objective: Teach researchers how to publish and manage Markdown-based documentation as web pages.</li> <li>Topics Covered:</li> <li>Setting up a GitHub repository for hosting documentation</li> <li>Transforming Markdown files into web-friendly formats</li> <li>Customizing web page layouts and themes</li> <li>Advanced features using Jekyll</li> <li>Version control and content management for documentation</li> <li>Activities:</li> <li>Publishing a research project documentation on GitHub Pages.</li> <li>Applying custom themes and layouts to enhance online documentation.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#conclusion","title":"Conclusion","text":"<ul> <li>Review of Markdown's role in enhancing research efficiency and clarity.</li> <li>Encouraging the integration of Markdown into daily research activities for improved documentation and dissemination.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#additional-resources","title":"Additional Resources","text":"<ul> <li>Curated list of advanced Markdown tutorials, guides for GitHub Pages, and Jekyll resources for researchers.</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-1-mastering-markdown-syntax_1","title":"Section 1: Mastering Markdown Syntax","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#1-fundamentals-of-text-formatting","title":"1. Fundamentals of Text Formatting","text":"<ul> <li>Headings: Use <code>#</code> for different levels of headings.</li> <li> </li> <li> </li> <li> </li> <li> <p>Lists: Bulleted lists use asterisks, numbers for ordered lists.</p> </li> <li>Item 1</li> <li>Item 2<ul> <li>Subitem 2.1</li> <li>Subitem 2.2</li> </ul> </li> <li> <ol> <li>First item</li> </ol> </li> <li> <ol> <li>Second item</li> </ol> </li> <li> <p>Bold and Italics: Use asterisks or underscores.</p> </li> <li>Bold Text</li> <li>Italic Text</li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-1","title":"Heading Level 1","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-2","title":"Heading Level 2","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-3","title":"Heading Level 3","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#2-advanced-structures","title":"2. Advanced Structures","text":"<ul> <li>Tables: Create tables using dashes and pipes.</li> <li> Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data </li> <li> <p>Add a \":\"\" to change text justification. Here the : is added on the left for left justification.     | Header 1 | Header 2 | Header 3 |     |---------:|--------- |----------|     | Row 1    | Data     | Data     |     | Row 2    | Data     | Data     |</p> </li> <li> A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C </li> <li> <p>If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. </p> </li> </ul> <p>Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition.</p> 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 534678912 672195348 198342567 859761423 426853791 713924856 961537284 287419635 345286179 <ul> <li>Blockquotes: Use <code>&gt;</code> for blockquotes.</li> <li> <p>This is a blockquote.</p> </li> <li> <p>It can span multiple lines.</p> </li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#3-integrating-multimedia","title":"3. Integrating Multimedia","text":"<ul> <li>Images: Add images using the format <code>![alt text](image_url)</code>.</li> <li> </li> <li> <p>Videos: Embed videos using HTML in Markdown.</p> </li> <li><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#4-diagrams-with-mermaid","title":"4. Diagrams with Mermaid","text":"<ul> <li>Flowcharts:</li> </ul> <pre><code>    graph TD\n    A[Start] --&gt; B[Analyze Data]\n    B --&gt; C{Is Data Large?}\n    C --&gt;|Yes| D[Apply Big Data Solutions]\n    C --&gt;|No| E[Use Traditional Methods]\n    D --&gt; F[Machine Learning]\n    E --&gt; G[Statistical Analysis]\n    F --&gt; H{Model Accurate?}\n    G --&gt; I[Report Results]\n    H --&gt;|Yes| J[Deploy Model]\n    H --&gt;|No| K[Refine Model]\n    J --&gt; L[Monitor Performance]\n    K --&gt; F\n    L --&gt; M[End: Success]\n    I --&gt; N[End: Report Generated]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style M fill:#9f9,stroke:#333,stroke-width:2px\n    style N fill:#9f9,stroke:#333,stroke-width:2px</code></pre> <ul> <li> <p>Mind Maps: <pre><code>    mindmap\n  root((ESIIL))\n    section Data Sources\n      Satellite Imagery\n        ::icon(fa fa-satellite)\n      Remote Sensing Data\n        Drones\n        Aircraft\n      On-ground Sensors\n        Weather Stations\n        IoT Devices\n      Open Environmental Data\n        Public Datasets\n        ::icon(fa fa-database)\n    section Research Focus\n      Climate Change Analysis\n        Ice Melt Patterns\n        Sea Level Rise\n      Biodiversity Monitoring\n        Species Distribution\n        Habitat Fragmentation\n      Geospatial Analysis Techniques\n        Machine Learning Models\n        Predictive Analytics\n    section Applications\n      Conservation Strategies\n        ::icon(fa fa-leaf)\n      Urban Planning\n        Green Spaces\n      Disaster Response\n        Flood Mapping\n        Wildfire Tracking\n    section Tools and Technologies\n      GIS Software\n        QGIS\n        ArcGIS\n      Programming Languages\n        Python\n        R\n      Cloud Computing Platforms\n        AWS\n        Google Earth Engine\n      Data Visualization\n        D3.js\n        Tableau</code></pre></p> </li> <li> <p>Timelines:</p> </li> </ul> <pre><code>gantt\n    title ESIIL Year 2 Project Schedule\n    dateFormat  YYYY-MM-DD\n    section CI\n    Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30\n    OASIS documentation                    :2024-09-15, 70d\n    Data cube OASIS via cyverse account    :2024-09-15, 100d\n    Integrate with ESIIL User Management system :2024-08-01, 2024-11-30\n    Build badges to deploy DE from mkdoc   :2024-09-01, 2024-12-15\n    Streamline Github ssh key management   :2024-10-01, 2024-12-31\n    Cyverse support (R proxy link)         :2024-11-01, 2024-12-31\n    Cyverse use summary and statistics     :2024-08-01, 2024-12-15\n\n    section CI Consultation and Education\n    Conferences/Invited talks              :2024-08-01, 2024-12-31\n    Office hours                           :2024-08-15, 2024-12-15\n    Proposals                              :2024-09-01, 2024-11-15\n    Private lessons                        :2024-09-15, 2024-11-30\n    Pre-event trainings                    :2024-10-01, 2024-12-15\n    Textbook development w/ education team :2024-08-01, 2024-12-15\n    Train the trainers / group lessons     :2024-08-15, 2024-11-30\n    Tribal engagement                      :2024-09-01, 2024-12-15\n    Ethical Space training                 :2024-09-15, 2024-12-31\n\n    section CI Design and Build\n    Data library (repository)              :2024-08-01, 2024-10-30\n    Analytics library (repository)         :2024-08-15, 2024-11-15\n    Containers (repository)                :2024-09-01, 2024-11-30\n    Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15\n    Tribal resilience Data Cube            :2024-10-01, 2024-12-31</code></pre> <pre><code>\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%%\ngitGraph\n  commit id: \"Start from template\"\n  branch c1\n  commit id: \"Set up SSH key pair\"\n  commit id: \"Modify _config.yml for GitHub Pages\"\n  commit id: \"Initial website structure\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\n  commit id: \"Edit existing pages\"\n  commit id: \"Delete old markdown pages\"\n  commit id: \"Finalize website updates\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\ncheckout c1\n\n  branch b1\n\n  commit\n  commit\n  checkout c1\n  merge b1</code></pre> <pre><code>%%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%%\nquadrantChart\n  x-axis Urgent --&gt; Not Urgent\n  y-axis Not Important --&gt; \"Important \u2764\"\n  quadrant-1 Plan\n  quadrant-2 Do\n  quadrant-3 Delegate\n  quadrant-4 Delete</code></pre> <pre><code>timeline\n    title Major Events in Environmental Science and Data Science\n    section Environmental Science\n        19th century : Foundations in Ecology and Conservation\n        1962 : Publication of 'Silent Spring' by Rachel Carson\n        1970 : First Earth Day\n        1987 : Brundtland Report introduces Sustainable Development\n        1992 : Rio Earth Summit\n        2015 : Paris Agreement on Climate Change\n    section Data Science\n        1960s-1970s : Development of Database Management Systems\n        1980s : Emergence of Data Warehousing\n        1990s : Growth of the World Wide Web and Data Mining\n        2000s : Big Data and Predictive Analytics\n        2010s : AI and Machine Learning Revolution\n        2020s : Integration of AI in Environmental Research</code></pre> <pre><code>erDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }</code></pre> <pre><code>---\nconfig:\n  sankey:\n    showValues: false\n---\nsankey-beta\n\nNASA Data,Big Data Harmonization,100\n    Satellite Imagery,Big Data Harmonization,80\n    Open Environmental Data,Big Data Harmonization,70\n    Remote Sensing Data,Big Data Harmonization,90\n    Big Data Harmonization, Data Analysis and Integration,340\n    Data Analysis and Integration,Climate Change Research,100\n    Data Analysis and Integration,Biodiversity Monitoring,80\n    Data Analysis and Integration,Geospatial Mapping,60\n    Data Analysis and Integration,Urban Planning,50\n    Data Analysis and Integration,Disaster Response,50</code></pre>"},{"location":"collaborating-on-the-cloud/markdown_basics/#5-interactive-elements","title":"5. Interactive Elements","text":"<ul> <li>Hyperlinks: Use the format <code>[link text](URL)</code>.</li> <li>Google</li> <li> <p>Play Tetris</p> </li> <li> <p>Embedding Interactive Content: Use HTML tags or specific platform embed codes.</p> </li> <li><code>&lt;iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#6-math-notation","title":"6. Math Notation","text":"<p>Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX).</p> <ul> <li>Inline Math: Use single dollar signs for inline math expressions. Representing the normal distribution.</li> </ul> <p>Example: The probability density function of the normal distribution is given by \\(f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\).`</p> <ul> <li>Display Math: Use double dollar signs for standalone equations.</li> </ul> <p>Example:   $$   f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e<sup>{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)</sup>2}   $$</p> <ul> <li>Common LaTeX Elements for Environmental Data Science:</li> <li>Statistical Distributions:<ul> <li>Normal Distribution: <code>\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}</code> for \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)</li> <li>Poisson Distribution: <code>P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</code> for \\(P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)</li> </ul> </li> <li>Coordinate Systems:<ul> <li>Spherical Coordinates: <code>(r, \\theta, \\phi)</code> for \\((r, \\theta, \\phi)\\)</li> <li>Cartesian Coordinates: <code>(x, y, z)</code> for \\((x, y, z)\\)</li> </ul> </li> <li>Geospatial Equations:<ul> <li>Haversine Formula for Distance: <code>a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)</code> for \\(a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)\\)</li> </ul> </li> </ul> <p>Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities.</p>"},{"location":"collaborating-on-the-cloud/markdown_basics/#7-effective-citations-in-markdown","title":"7. Effective Citations in Markdown","text":""},{"location":"collaborating-on-the-cloud/markdown_basics/#inline-citations","title":"Inline Citations","text":"<ul> <li>Objective: Learn how to use inline citations in Markdown.</li> <li>Example Usage:</li> <li>Inline citation of a single work: <ul> <li>Some text with an inline citation. [@jones:envstudy:2020]</li> </ul> </li> <li>Inline citation with specific page or section: <ul> <li>More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2]</li> </ul> </li> <li>Contrasting views: <ul> <li>Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78]</li> </ul> </li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#footnote-citations","title":"Footnote Citations","text":"<ul> <li>Objective: Understand how to use footnote citations in Markdown.</li> <li>Example Usage:</li> <li>Citing with a footnote: <ul> <li>Some statement in the text.<sup>1</sup></li> </ul> </li> <li>Multiple references to the same footnote: <ul> <li>Another statement referring to the same source.<sup>1</sup></li> </ul> </li> <li>A different citation: <ul> <li>Additional comment with a new citation.<sup>2</sup></li> </ul> </li> </ul>"},{"location":"collaborating-on-the-cloud/markdown_basics/#creating-footnotes","title":"Creating Footnotes","text":"<ul> <li>Example Syntax:</li> <li></li> <li></li> </ul> <ol> <li> <p>First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"container-library/","title":"Container Image Library","text":"<p>Browse available container images for ESIIL computing. Use these images to run analyses in consistent environments.</p>"},{"location":"container-library/esiil-container-library/","title":"Container Image Library Index","text":"<p>The Container Image Library catalogs ready-to-use Docker images. Each image page is tagged to support search and discovery.</p>"},{"location":"container-library/example-container/","title":"Example Container","text":"<p>This placeholder documents a sample container image and demonstrates tagging for search.</p>","tags":["container","example","raster"]},{"location":"project-documentation/methods/","title":"Project methods overview","text":""},{"location":"project-documentation/methods/#data-sources","title":"Data Sources","text":"<p>List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines.</p>"},{"location":"project-documentation/methods/#data-processing-steps","title":"Data Processing Steps","text":"<p>Describe data processing steps taken, the order of scripts, etc.</p>"},{"location":"project-documentation/methods/#data-analysis","title":"Data Analysis","text":"<p>Describe steps taken to analyze data and resulting files in team data store file structure.</p>"},{"location":"project-documentation/methods/#visualizations","title":"Visualizations","text":"<p>Describe visualizations created and any specialized techniques or libraries that users should be aware of.</p>"},{"location":"project-documentation/methods/#conclusions","title":"Conclusions","text":"<p>Summary of the full workflow and its outcomes. Reflect on the methods used.</p>"},{"location":"project-documentation/methods/#references","title":"References","text":"<p>Citations of tools, data sources, and other references used.</p>"},{"location":"project-documentation/project-notes/","title":"Project discussion notes","text":""},{"location":"project-documentation/project-notes/#virtual-meeting-3","title":"Virtual meeting #3","text":""},{"location":"project-documentation/project-notes/#team-theme-tentative-area-of-interest-or-question","title":"Team theme, tentative area of interest, or question:","text":""},{"location":"project-documentation/project-notes/#day-1-march-12-2024-cu-boulder","title":"Day 1: March 12, 2024 - CU Boulder","text":""},{"location":"project-documentation/project-notes/#selected-scientific-question","title":"Selected scientific question:","text":""},{"location":"project-documentation/project-notes/#day-2-march-13-2024-cu-boulder","title":"Day 2: March 13, 2024 - CU Boulder","text":""},{"location":"project-documentation/project-notes/#day-3-march-14-2024-cu-boulder","title":"Day 3: March 14, 2024 - CU Boulder","text":""},{"location":"project-documentation/project-presentation/","title":"Project presentation overview","text":"<p>All project presentation materials should be made available on this page.</p> <p>Your team may present directly from this page if you would like to; alternatively, if you would prefer to use slides to present, please make sure to export your team's slides as a PDF, add them to your GitHub, and add the link to that PDF here below.</p>"},{"location":"project-documentation/project-presentation/#presentation","title":"Presentation","text":""},{"location":"quickstart/","title":"Quick Start","text":"Quick Start <p>Jump into OASIS with these starter resources</p> <p>New to ESIIL's tools? Start with one of the guides below. Each quickstart     page walks you through the basics with step-by-step instructions and links     for deeper learning.</p> Cloud CyVerse R Python GitHub Docker OASIS Data Analytics Containers QUICK START PAGE overview &amp; orientation Introduction to Cloud Starting with CyVerse Starting with R Data Exploration in the Cloud Starting with GitHub Starting Docker Container Starting with OASIS Data Library Analytics Library Container Image Library Advanced Textbook","tags":["quickstart"]},{"location":"quickstart/advanced-textbook/","title":"Advanced Textbook","text":"","tags":["education","raster"]},{"location":"quickstart/advanced-textbook/#sell-it","title":"Sell It","text":"<p>The Advanced Textbook offers in-depth lessons and interactive notebooks to level up your environmental data science skills. Chapters blend narrative and code so you learn concepts and immediately apply them.</p>","tags":["education","raster"]},{"location":"quickstart/advanced-textbook/#show-it","title":"Show It","text":"<p>Browse chapters that mix narrative, code, and exercises at the Advanced Textbook site. Each section links to a runnable notebook in the cloud or instructions for local execution.</p>","tags":["education","raster"]},{"location":"quickstart/advanced-textbook/#do-it","title":"Do It","text":"<ol> <li>Open the textbook. Explore the table of contents to see available    topics.</li> <li>Pick a chapter. Start with an area relevant to your work or follow the    recommended progression.</li> <li>Launch the notebook. Use the provided link to open the companion notebook    in the cloud or download it to your machine.</li> <li>Work through exercises. Run the example code and answer practice    questions, noting any challenges.</li> <li>Dig deeper. Follow links and the glossary for background information.</li> </ol>","tags":["education","raster"]},{"location":"quickstart/advanced-textbook/#review-it","title":"Review It","text":"<p>Capture what you learned and bookmark sections to revisit or share with colleagues. Consider contributing fixes or additional examples through the textbook's GitHub repository.</p>","tags":["education","raster"]},{"location":"quickstart/analytics-library/","title":"Analytics Library","text":""},{"location":"quickstart/analytics-library/#sell-it","title":"Sell It","text":"<p>The Analytics Library hosts reusable workflows and functions so you can build analyses faster and with consistent methods. Each entry is tested and documented, saving you from reinventing common analysis steps.</p>"},{"location":"quickstart/analytics-library/#show-it","title":"Show It","text":"<p>Each workflow page includes step-by-step instructions and example outputs at the Analytics Library. Many provide Jupyter notebooks demonstrating how to use the functions.</p>"},{"location":"quickstart/analytics-library/#do-it","title":"Do It","text":"<ol> <li>Browse the catalog. Visit the library and search for a workflow that fits    your project.</li> <li>Read the documentation. Open the workflow's README to learn about    required inputs and dependencies.</li> <li>Clone or download. Get the workflow code onto your machine.</li> <li>Install dependencies. Use the instructions in the README to install    required packages or container images.</li> <li>Run the workflow. Execute it on the provided sample data before applying    it to your own dataset.</li> </ol>"},{"location":"quickstart/analytics-library/#review-it","title":"Review It","text":"<p>Compare your results to the example outputs and note any differences. If you improve the workflow or create a new one, consider contributing it back to the library for others to use.</p>"},{"location":"quickstart/cloud/","title":"Cloud Reproducibility Triangle (CRT)","text":"<p>What you\u2019ll learn Launch a CyVerse Discovery Environment (DE) JupyterLab session, connect to GitHub with SSH through the Git sidebar, move data with GoCommands, and\u2014most importantly\u2014keep your work alive after an ephemeral compute session ends.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#1-why-the-triangle","title":"1. Why the Triangle?","text":"<p>Across science platforms like BinderHub, JupyterHub classrooms, and cloud services such as Azure ML or Vertex AI Workbench, the same pattern repeats:</p> <ol> <li>GitHub (or GitLab, etc.) for code &amp; collaboration </li> <li>Ephemeral compute (JupyterLab, containerized notebooks, VICE apps) for analysis </li> <li>Persistent storage (cloud volumes, object stores, CyVerse Data Store) for datasets and results</li> </ol> <p>This is the Cloud Triangle. It\u2019s not bureaucracy\u2014it\u2019s the habit that keeps your work reproducible, durable, and shareable.</p> <p>Think of it like this:</p> <ul> <li>Compute = your scratch pad: fast but disposable  </li> <li>GitHub = your lab notebook: versioned, accountable, and collaborative  </li> <li>Data Store = your filing cabinet: heavy, permanent, and backed up</li> </ul> <p></p> <p>Golden Rule Never let the only copy of your work live on the instance. If you don\u2019t push to GitHub or save to the Data Store, it will vanish when compute stops.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#2-the-routine-clone-compute-preserve","title":"2. The Routine (clone \u2192 compute \u2192 preserve)","text":"<ol> <li> <p>Launch an instance.    In the Discovery Environment, start a JupyterLab session using the course\u2019s containerized image so everyone shares the same software.</p> </li> <li> <p>Connect GitHub (no shell required).    Open the Git sidebar \u2192 Clone a Repository \u2192 paste your SSH URL <code>git@github.com:ORG/REPO.git</code> (e.g., <code>git@github.com:CU-ESIIL/Ty_ed_demo.git</code> to practice).    Stage with checkboxes, write a message, Commit, then Push.</p> </li> <li> <p>Fetch data (stream only what you need).    Use GoCommands (resumable transfers) to pull datasets from the CyVerse Data Store.    With the <code>i:</code> prefix you copy exactly what you need into <code>./data/</code> for this run.</p> </li> <li> <p>Compute (scratch space mindset).    Treat the instance like a scratch pad. Run notebooks, generate intermediates, iterate fast.    Remember: the VM is temporary; don\u2019t assume its local files will persist.</p> </li> <li> <p>Preserve (separate code from data). </p> </li> <li>Code &amp; small artifacts \u2192 GitHub </li> <li>Large outputs \u2192 Data Store    When both are saved, shut down the instance\u2014nothing important is left behind.</li> </ol> <p>This rhythm\u2014clone \u2192 compute \u2192 preserve\u2014is the Cloud Triangle in motion. </p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#3-anatomy-of-the-cloud-triangle","title":"3. Anatomy of the Cloud Triangle","text":"<p>A) Compute \u2014 CyVerse DE / Jetstream2 - Instance (VM / VICE app) What: A running virtual machine in CyVerse DE that pairs a hardware profile with a container image. Why: Everyone starts identical; you \u201crent\u201d compute only while it\u2019s on; local files are temporary. How (here): DE \u2192 Apps \u2192 JupyterLab \u2192 launch with the course preset.</p> <ul> <li> <p>Image \u2192 Container What: An image is a frozen software recipe; a container is that image running inside your instance. Why: Reproducibility\u2014no \u201cworks on my laptop\u201d drift. How (here): Use the ESIIL large spatial analysis image (satellite-focused). For repeat needs, bake changes into a new image rather than ad\u2011hoc installs.</p> </li> <li> <p>Hardware profile What: vCPUs, RAM, GPUs, and disk size for your instance. Why: Right-size performance while sharing resources fairly. How (here): Pick the Jetstream2 course profile provided in DE.</p> </li> <li> <p>Jetstream2 What: NSF cloud where many DE apps run. Why: Academic cloud at scale for on-demand classrooms and research. How (here): Your instance is scheduled on Jetstream2 when you launch JupyterLab.</p> </li> <li> <p>ACCESS-CI (when you need more) What: Program to obtain larger/longer compute allocations. Why: Run bigger jobs without straining shared classroom quotas. How (here): If you consistently outgrow presets, request an allocation and attach that hardware pack to your DE launches.</p> </li> <li> <p>Local disk (instance scratch) What: The filesystem inside your VM (e.g., your working directory <code>~</code>). Why: Fast for computation and intermediates but ephemeral\u2014can disappear when the app stops. How (here): Work in <code>./data/</code> or <code>./tmp/</code> during the session; before stopping, move deliverables to persistent storage (see Storage).</p> </li> </ul> <p>B) Persistent Storage \u2014 CyVerse Data Store (UArizona) - Remote disk (Data Store / iRODS) What: Your durable home at <code>/iplant/home/&lt;username&gt;</code> hosted at the University of Arizona, behind a firewall. Why: Backed\u2011up, shareable, and persists beyond any single VM. How (here): From compute, use the <code>i:</code> prefix for paths, e.g., <code>i:/iplant/home/&lt;username&gt;/projects/...</code>.</p> <ul> <li> <p>Network layout (why local \u2260 remote) What: Compute (Jetstream2) and storage (UArizona Data Store) live in different places and are separated by a firewall. Why: Expect latency; uploading/downloading is a deliberate step\u2014don\u2019t assume local files persist. How (here): Pull only what you need for a run; push results back when done.</p> </li> <li> <p>GoCommands (data mover) What: Cross\u2011platform CLI for robust, resumable, checksum\u2011verified transfers to/from the Data Store. Why: Reliable for \u201cdata are heavy\u201d workflows and long\u2011running moves. How (here): Use <code>i:</code> paths with GoCommands to move between the VM and <code>/iplant/home/&lt;username&gt;/...</code>. For small items, the DE file browser is fine.</p> </li> </ul> <p>C) GitHub \u2014 Code, Website &amp; Auth (Jupyter\u2011first) - Git (JupyterLab Git widget) \u2014 Clone vs Repo modes What: Built\u2011in panel for clone, stage, commit, push\u2014no terminal needed. Why: Safer staging (you can see file sizes), fewer auth pitfalls, harder to accidentally commit huge data. How (here):   1) Pre\u2011req: Set up SSH key or Web Authentication (2FA via browser) first.   2) Be in your top\u2011level working folder in the file browser (the place you want the repo folder to appear).   3) Open the Git sidebar (left toolbar).     \u2022 Not in a repo yet: You\u2019ll see three blue buttons (e.g., Clone a Repository, Create a New Repository, Open a Repository). Click Clone a Repository and paste your SSH URL <code>git@github.com:ORG/REPO.git</code> (e.g., <code>git@github.com:CU-ESIIL/Ty_ed_demo.git</code> to practice).     \u2022 Already inside a repo folder: You\u2019ll see the status panel (Unstaged \u2194 Staged changes, commit message box) with Commit / Push / Pull buttons. Use checkboxes to stage only what you intend to version.</p> <p>Tip: Keep big data out of Git. The widget\u2019s file\u2011by\u2011file staging makes it obvious when you\u2019re about to add large artifacts\u2014uncheck them and use the Data Store instead.</p> <ul> <li> <p>SSH key (for GitHub) What: Password\u2011less credential stored in <code>~/.ssh/</code> inside the instance. Why: Works cleanly with 2FA; avoids repeated prompts. How (here): Generate once on an instance, add the public key to GitHub (Settings \u2192 SSH keys). The Git widget will push/pull over SSH automatically.</p> </li> <li> <p>Web Authentication (2FA via browser) \u2014 optional HTTPS path What: Uses the <code>gh</code> CLI to open a browser and store a token; Git delegates credentials to <code>gh</code>. Why: Handy on ephemeral VMs when you don\u2019t want to manage SSH keys. How (here): Run the provided notebook cell once per new instance to log in via web; after that the Git widget can operate over HTTPS.</p> </li> <li> <p>GitHub Pages (project website) What: A static site built directly from your repo. Why: Push once; your docs/tutorials publish automatically. How (here): In GitHub \u2192 Settings \u2192 Pages \u2192 Deploy from a branch \u2192 <code>main</code> + <code>/docs</code>. Put site files in <code>/docs/</code> (e.g., <code>docs/index.md</code>), then Commit/Push from the widget.  </p> <p>From pages in <code>docs/quickstart/...</code>, use <code>../assets/...</code> (or <code>{{ '/assets/... ' | relative_url }}</code> if using Jekyll) so paths don\u2019t duplicate <code>docs/</code>.</p> </li> <li> <p>Markdown (docs &amp; Pages authoring) What: Plain\u2011text syntax for structured docs (headings, lists, links, images, code). Renders consistently on GitHub and on your Pages site. Why: Fast, versionable, and reviewable; encourages clear communication. Perfect for lab notes, assignments, and how\u2011tos. How (here): </p> </li> <li>Put site files in <code>/docs/</code> (e.g., <code>docs/index.md</code>, <code>docs/quickstart/getting-started.md</code>).  </li> <li>Use relative links between pages: <code>[Next \u2192](../quickstart/getting-started.md)</code>.  </li> <li>Images from a page under <code>docs/quickstart/</code>: <code>../assets/...</code> (or Jekyll: <code>{{ '/assets/... ' | relative_url }}</code>).  </li> <li>Code fences for reproducible snippets:      <pre><code># example\nprint(\"hello, world\")\n</code></pre></li> <li> <p>Optional Jekyll front matter for page metadata:      <pre><code>---\ntitle: Getting Started\nlayout: default\n---\n</code></pre></p> </li> <li> <p>GitHub CLI (<code>gh</code>) \u2014 optional power tool What: Command\u2011line helper for GitHub tasks (auth, PRs, releases). Why: Useful for scripted workflows and browser\u2011based 2FA login on ephemeral instances. How (here): Use <code>gh auth login --web</code> to establish HTTPS credentials, or for advanced operations (opening PRs, viewing issues). Caveat: The CLI doesn\u2019t \u201cprotect\u201d you from committing large files\u2014Git will still reject pushes over size limits. The widget\u2019s staging UI makes it easier to avoid adding big data in the first place. Keep large artifacts in the CyVerse Data Store, not in Git.</p> </li> </ul> <p>A sensible <code>.gitignore</code> starter (to reduce accidents) <pre><code># keep heavy data out of git\ndata/\n*.tif\n*.tiff\n*.h5\n*.hdf5\n*.nc\n*.zip\n*.tar\n*.parquet\n*.feather\n\n# notebooks: keep checkpoints out\n.ipynb_checkpoints/\n\n# environments &amp; caches\n.env\n.venv/\n__pycache__/\n*.pyc\n</code></pre></p> <p>Golden workflow Clone (GitHub) \u2192 Compute (local scratch) \u2192 Preserve (GitHub + Data Store) \u2192 Shut down. Remember: compute runs on Jetstream2; persistent storage is at UArizona behind a firewall\u2014plan transfers, don\u2019t assume local files persist.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#4-go-through-the-routine-yourself-step-by-step","title":"4. Go through the routine yourself (step-by-step)","text":"<ol> <li> <p>Launch an instance.    Choose the course JupyterLab image and start it in CyVerse DE. </p> </li> <li> <p>Set up GitHub auth (one-time on each new VM).</p> </li> </ol> <p>2A. SSH (recommended) \u2014 works cleanly with 2FA, no passwords.</p> <pre><code>import os, subprocess, textwrap\n\ndef add_github_to_known_hosts():\n    ssh_dir = os.path.expanduser(\"~/.ssh\")\n    known_hosts = os.path.join(ssh_dir, \"known_hosts\")\n    os.makedirs(ssh_dir, exist_ok=True)\n    out = subprocess.run(\n        [\"ssh-keyscan\", \"-t\", \"rsa,ed25519\", \"github.com\"],\n        capture_output=True, text=True, check=True\n    ).stdout.strip()\n    with open(known_hosts, \"a\") as fh:\n        if out:\n            fh.write(out + \"\\n\")\n    print(\"github.com added to known_hosts\")\n\ndef configure():\n    username = input(\"GitHub username: \")\n    email = input(\"GitHub email: \")\n\n    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", username], check=True)\n    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", email], check=True)\n\n    ssh_dir = os.path.expanduser(\"~/.ssh\")\n    os.makedirs(ssh_dir, exist_ok=True)\n    key_path = os.path.join(ssh_dir, \"github\")  # ~./ssh/github and github.pub\n\n    # Create Ed25519 keypair (no passphrase for this ephemeral VM)\n    subprocess.run([\"ssh-keygen\", \"-t\", \"ed25519\", \"-f\", key_path, \"-N\", \"\"], check=True)\n\n    # Minimal SSH config entry\n    cfg_path = os.path.join(ssh_dir, \"config\")\n    block = textwrap.dedent(f\"\"\"\\\n    Host github.com\n      HostName github.com\n      User git\n      IdentityFile {key_path}\n    \"\"\")\n    with open(cfg_path, \"a\") as fh:\n        fh.write(block)\n\n    # Start agent and add key\n    subprocess.run(f'eval \"$(ssh-agent -s)\" &amp;&amp; ssh-add {key_path}', shell=True, check=True)\n\n    add_github_to_known_hosts()\n\n    with open(key_path + \".pub\") as fh:\n        pub = fh.read().strip()\n    print(\"\\nPublic key \u2014 copy to GitHub \u2192 Settings \u2192 SSH keys:\\n\")\n    print(pub, \"\\n\")\n\nconfigure()\n</code></pre> <p>Keys live in <code>~/.ssh/</code> (your home). Never use <code>/.ssh/</code> (the root).</p> <p>Add this key to your GitHub profile (one time):    1. Copy the public key the script printed (the line starting with <code>ssh-ed25519</code> or <code>ssh-rsa</code>).    2. In GitHub: click your avatar \u2192 Settings \u2192 SSH and GPG keys \u2192 New SSH key.    3. Title: <code>CyVerse VM</code> (or similar).    4. Key: paste the public key from the instance.    5. Click Add SSH key.    After this, GitHub recognizes this instance as you, and the JupyterLab Git widget can push/pull over SSH.</p> <p>2B. Web Authentication (optional HTTPS path) \u2014 if you prefer browser-based 2FA token via GitHub CLI:</p> <pre><code># Activate env and login via web; then wire git -&gt; gh credential helper\n!eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate macrosystems &amp;&amp; yes | gh auth login --hostname github.com --web -p https\n!git config --global credential.helper '!eval \"$(conda shell.bash hook)\" &amp;&amp; conda activate macrosystems &amp;&amp; gh auth git-credential'\n!gh auth status\n</code></pre> <ol> <li>Clone with the Git widget (Jupyter sidebar). </li> <li>In the file browser, navigate to your top-level working folder (where you want the repo folder created).  </li> <li> <p>Open the Git sidebar (left toolbar).  </p> <ul> <li>If you\u2019re not inside a repo yet: you\u2019ll see three blue buttons (e.g., Clone a Repository, Create a New Repository, Open a Repository). Click Clone a Repository and paste your SSH URL <code>git@github.com:ORG/REPO.git</code> (e.g., <code>git@github.com:CU-ESIIL/Ty_ed_demo.git</code> to practice).  </li> <li>If you are inside a repo folder: you\u2019ll see the status panel (Unstaged/Staged changes, commit box) with Commit / Push / Pull.   <p>Tip: Use the checkboxes to avoid staging large data files\u2014keep those in the Data Store.</p> </li> </ul> </li> <li> <p>Compute (scratch-space mindset).    Run notebooks, generate intermediates locally. Assume local files can disappear when the app stops.</p> </li> <li> <p>Install GoCommands (data mover). <pre><code>GOCMD_VER=$(curl -L -s https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt); \\\ncurl -L -s https://github.com/cyverse/gocommands/releases/download/${GOCMD_VER}/gocmd-${GOCMD_VER}-linux-amd64.tar.gz | tar zxvf -\n./gocmd init\n./gocmd whoami\n</code></pre></p> </li> </ol> <p>During <code>./gocmd init</code>, you\u2019ll be prompted for: <pre><code>iRODS Host [data.cyverse.org]:\niRODS Port [1247]:\niRODS Zone [iplant]:\niRODS Username: &lt;your CyVerse username&gt;\niRODS Password: &lt;your CyVerse password&gt;\n</code></pre>    - Press Enter to accept the defaults in brackets for Host/Port/Zone.    - Use your CyVerse (DE) username/password for the credentials prompts.    - If you mistype anything, just rerun <code>./gocmd init</code> to correct it.    - Settings are saved under <code>~/.irods/</code> and you can verify the login with <code>./gocmd whoami</code>.</p> <ol> <li> <p>Fetch data (only what you need). <pre><code># 1) Create a local folder for this run (or use an existing folder you already have)\nmkdir -p data\n\n# 2) Copy from the CyVerse Data Store into ./data/\n#    IMPORTANT: replace &lt;username&gt; with your CyVerse username (NO angle brackets)\n#    Example path: i:/iplant/home/alovelace/projects/myproj/data/\n./gocmd get --icat --retry 3 -d -k -r \\\n  i:/iplant/home/&lt;username&gt;/projects/myproj/data/ \\\n  ./data/\n```bash\nmkdir -p data\n./gocmd get --icat --retry 3 -d -k -r \\\n  i:/iplant/home/&lt;username&gt;/projects/myproj/data/ \\\n  ./data/\n</code></pre></p> </li> <li> <p>Push outputs back (preserve results). <pre><code># 1) Push from local outputs/ to the Data Store\n#    IMPORTANT: replace &lt;username&gt; with your CyVerse username (NO angle brackets)\n#    Example remote folder: i:/iplant/home/alovelace/projects/myproj/outputs/\n./gocmd put --diff --icat --retry 3 -d -k \\\n  outputs/run-20240101/ \\\n  i:/iplant/home/&lt;username&gt;/projects/myproj/outputs/\n```bash\n./gocmd put --diff --icat --retry 3 -d -k \\\n  outputs/run-20240101/ \\\n  i:/iplant/home/&lt;username&gt;/projects/myproj/outputs/\n</code></pre></p> </li> <li> <p>Shut down safely (manage instances in DE). </p> </li> <li>In the CyVerse DE, go to the Analyses tab to see all running instances.  </li> <li>Auto-shutdown: Analyses are set to stop automatically after 4 hours unless you extend the time.  </li> <li>Extend runtime: Click the hourglass next to your analysis to extend the time window. You can repeat this as needed for longer sessions.  </li> <li>Analysis ID: The Analyses tab shows the Analysis ID \u2014 include this in support requests if something goes wrong.  </li> <li>Terminate: Use the red stop button to end the instance when you\u2019re done.</li> </ol> <p>Ethics of resource use Choose an instance size and runtime that match the job. Big or long is fine when justified; what's not okay is locking up resources you aren't using. If a run is idle, extend only when you return, and terminate when finished.</p> <p>Code is in GitHub; data is in the Data Store\u2014nothing important is left on the VM.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#5-good-project-habits","title":"5. Good Project Habits","text":"<p>Design your repo so collaborators (and future\u2011you) can rerun it without guessing.</p> <pre><code>repo/\n  README.md                 # what this does; how to run; links to Pages site\n  env/                      # environment.yml or requirements.txt; container tag notes\n  src/                      # source code (functions, modules)\n  notebooks/                # small notebooks only (exploration, figures)\n  configs/                  # YAML/JSON configs for runs; record parameters here\n  data/                     # small samples only; real data lives in the Data Store\n    README.md               # i: paths + size notes + fetch instructions\n  outputs/                  # tiny examples only; real outputs go to Data Store\n  scripts/                  # fetch_data.sh, push_outputs.sh, tiny helpers\n</code></pre> <p>Why this layout? Code in <code>src/</code> is reusable; notebooks stay lightweight; big artifacts live in persistent storage (Data Store), not in Git. This keeps clones fast and version history clean.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#datareadmemd-template-use-this-verbatim-and-edit","title":"<code>data/README.md</code> template (use this verbatim and edit)","text":"<pre><code># Data for &lt;project&gt;\n\nPrimary storage: i:/iplant/home/&lt;username&gt;/projects/&lt;proj&gt;/data/\n\nTo fetch a working subset into this repo (replace &lt;username&gt; with YOUR CyVerse username \u2014 no angle brackets):\n\n./gocmd get --icat --retry 3 -d -k -r \\\n  i:/iplant/home/&lt;username&gt;/projects/&lt;proj&gt;/data/ \\\n  ./data/\n\nNotes:\n- Approx size: &lt;fill in&gt;\n- Provenance / last update: &lt;fill in&gt;\n</code></pre> <p>Tip: put just enough sample data under <code>data/</code> (kilobytes/megabytes) so examples run quickly. Everything else: fetch with GoCommands.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#tiny-helper-scripts-optional","title":"Tiny helper scripts (optional)","text":"<p>scripts/fetch_data.sh <pre><code>#!/usr/bin/env bash\nset -euo pipefail\nmkdir -p data\n# Replace USERNAME and PROJ\n./gocmd get --icat --retry 3 -d -k -r \\\n  i:/iplant/home/USERNAME/projects/PROJ/data/ \\\n  ./data/\n</code></pre></p> <p>scripts/push_outputs.sh <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n# Replace USERNAME and PROJ\n./gocmd put --diff --icat --retry 3 -d -k \\\n  outputs/ \\\n  i:/iplant/home/USERNAME/projects/PROJ/outputs/\n</code></pre></p> <p>Make them executable: <code>chmod +x scripts/*.sh</code>.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#6-before-you-power-down-checklist","title":"6. Before You Power Down (checklist)","text":"<ul> <li>Git: Git sidebar shows no unstaged changes; latest commit pushed.</li> <li>Data Store: Required outputs uploaded with <code>./gocmd put</code>; spot\u2011check in DE File Browser or via <code>./gocmd ls i:/iplant/home/&lt;username&gt;/...</code>.</li> <li>Repro notes: Container/image tag recorded in <code>env/</code> and parameters in <code>configs/</code>.</li> <li>Analysis info: Copy your Analysis ID from the DE Analyses tab (include it in support requests).</li> <li>Kernel &amp; instance: Restarted kernel if memory was tight; consider a larger hardware profile next launch if justified; then terminate the instance in Analyses.</li> </ul> <p>Golden rule: nothing important left only on the VM. Code \u2192 GitHub; data/results \u2192 Data Store.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#7-troubleshooting-by-symptom","title":"7. Troubleshooting (by symptom)","text":"<ul> <li> <p>Lost files after shutdown   \u2192 They only lived on the instance. Rerun, re\u2011fetch data, and preserve work to GitHub/Data Store next time.</p> </li> <li> <p>Push failed / asked for a password   \u2192 Remote is HTTPS. Re\u2011clone via SSH (<code>git@github.com:ORG/REPO.git</code>) and add your SSH key to GitHub (Settings \u2192 SSH keys). In a terminal, you can test with: <code>ssh -T git@github.com</code>.</p> </li> <li> <p>\u201cPermission denied (publickey)\u201d in the Git widget   \u2192 You didn\u2019t add the instance\u2019s public key to GitHub yet, or the agent isn\u2019t loaded. Add the key (Settings \u2192 SSH keys) and relaunch the widget; if needed, rerun the SSH setup cell.</p> </li> <li> <p>Widget shows three blue buttons but you expected Commit/Push   \u2192 You\u2019re not inside a repo folder. In the file browser, open the repo directory you cloned; the status panel will appear.</p> </li> <li> <p>GoCommands can\u2019t see storage / auth fails   \u2192 Rerun <code>./gocmd init</code>, accept defaults for host/port/zone, enter your CyVerse credentials; verify with <code>./gocmd whoami</code>.</p> </li> <li> <p>\u201c.h5 signature not found\u201d or corrupted file   \u2192 You likely saved an error page as <code>.h5</code>. Re\u2011download with <code>./gocmd get</code> and verify sizes.</p> </li> <li> <p>Slow browser uploads   \u2192 Keep browser uploads \u2272 2 GB. Use GoCommands for large transfers.</p> </li> <li> <p>Out of memory / killed kernel   \u2192 Save your work, shut down, then relaunch with a larger hardware profile (only if the job truly requires it). See the ethics note below.</p> </li> </ul>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#8-why-this-works","title":"8. Why This Works","text":"<p>Across BinderHub, JupyterHub, and cloud notebooks you\u2019ll find the same pattern:</p> <ul> <li>Ephemeral compute for fast iteration (your scratch pad)</li> <li>Git\u2011backed collaboration for code and small artifacts (your lab notebook)</li> <li>Persistent storage for datasets and large outputs (your filing cabinet)</li> </ul> <p>CyVerse DE + Jetstream2 + Data Store map cleanly onto this triangle. The result is work that\u2019s robust, repeatable, and explainable.</p> <p>Ethics of resource use Choose an instance size and runtime that match the job. Big/long runs are fine when justified; don\u2019t lock up resources you aren\u2019t using. Extend time in the Analyses tab only when you\u2019re actively working, and terminate when finished.</p>","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/cloud/#9-selfcheck-2-minutes","title":"9. Self\u2011Check (2 minutes)","text":"<ol> <li>What survives when the instance stops?  </li> <li>Which tool do you use for a 20 GB transfer?  </li> <li>Why SSH for GitHub instead of HTTPS?  </li> <li>Where does the canonical copy of your dataset live?  </li> <li>Where do you extend runtime or find your Analysis ID?</li> </ol> Answers  1. Only what you pushed to GitHub or saved in the Data Store.   2. **GoCommands** (resumable, checksum\u2011aware, `i:` paths).   3. SSH avoids passwords, works with 2FA, and integrates with the Git widget cleanly.   4. In the **Data Store**, not on the instance.   5. In the DE **Analyses** tab (hourglass to extend; ID shown there).","tags":["quickstart","cloud","cyverse","CRT triangle"]},{"location":"quickstart/container-library/","title":"Container Image Library","text":""},{"location":"quickstart/container-library/#sell-it","title":"Sell It","text":"<p>Pre-built container images give you ready-to-run environments without installing software on your machine. They ensure your code runs the same on every system and make sharing workflows straightforward.</p>"},{"location":"quickstart/container-library/#show-it","title":"Show It","text":"<p>The Container Image Library lists images with descriptions and tags. Each entry links to a Docker or Apptainer image that you can pull to see a fully configured environment.</p>"},{"location":"quickstart/container-library/#do-it","title":"Do It","text":"<ol> <li>Browse the library. Look for an image that matches the tools you need.</li> <li>Pull the image. Use <code>docker pull &lt;image&gt;</code> or    <code>apptainer pull &lt;image&gt;</code> to download it.</li> <li>Run it. Start the container with <code>docker run -it &lt;image&gt; /bin/bash</code> or    the equivalent Apptainer command to open a shell inside the environment.</li> <li>Mount data. Add <code>-v /path/to/data:/data</code> to access files from your    machine inside the container.</li> </ol>"},{"location":"quickstart/container-library/#review-it","title":"Review It","text":"<p>Check the tools and versions inside the container and note which images fit your workflow. Reuse the image in future projects or use it as a base for your own custom container.</p>"},{"location":"quickstart/cyverse/","title":"Connecting Cyverse to GitHub","text":""},{"location":"quickstart/cyverse/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account)</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment https://de.cyverse.org, and log in with your new account.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p> <ol> <li>We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access</li> </ol>"},{"location":"quickstart/cyverse/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li> <p>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu    </p> </li> <li> <p>Select <code>JupyterLab ESIIL</code> </p> </li> <li> <p>Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>Click <code>Go to analysis</code>:    </p> </li> <li> <p>Now you should see Jupyter Lab!    </p> </li> </ol>"},{"location":"quickstart/cyverse/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"quickstart/cyverse/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li> <p>From Jupyter Lab, click on the Git Extension icon on the left menu:    </p> </li> <li> <p>Click <code>Clone a Repository</code> and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click <code>Clone</code>:    </p> </li> <li> <p>You should now see the <code>cyverse-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code> </p> </li> <li> <p>Go into the <code>cyverse-utils</code> folder:    </p> </li> <li> <p>open up the <code>create_github_keypair.ipynb</code> notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: </p> </li> <li> <p>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):    </p> </li> <li> <p>Select <code>SSH and GPG keys</code> </p> </li> <li> <p>Select <code>New SSH key</code> </p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:    </p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!    </p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"quickstart/docker/","title":"Starting Docker Containers","text":""},{"location":"quickstart/docker/#sell-it","title":"Sell It","text":"<p>Docker packages your software and all of its dependencies into portable containers that run the same way on laptops, servers, or the cloud. Instead of configuring each machine manually, you capture the environment once and share it with collaborators.</p>"},{"location":"quickstart/docker/#show-it","title":"Show It","text":"<p>A simple <code>Dockerfile</code> defines an environment. Each instruction adds a layer to the final image so it can be rebuilt or modified easily:</p> <pre><code>FROM python:3.11-slim\nCOPY . /app\nRUN pip install -r requirements.txt\n</code></pre>"},{"location":"quickstart/docker/#do-it","title":"Do It","text":"<ol> <li>Install Docker. Download Docker Desktop or the Docker Engine for your    operating system from docker.com.</li> <li>Create a project. Make a folder with a <code>Dockerfile</code> like the example    above or choose an existing image from Docker Hub.</li> <li>Build the image. From the folder run <code>docker build -t myimage .</code> to turn    the <code>Dockerfile</code> into a reusable image.</li> <li>Run the container. Launch it with <code>docker run -it myimage /bin/bash</code> to    open a shell inside the container.</li> <li>Share it. Tag and push the image to a registry such as Docker Hub with    <code>docker tag myimage myuser/myimage</code> followed by <code>docker push myuser/myimage</code>.</li> </ol>"},{"location":"quickstart/docker/#review-it","title":"Review It","text":"<p>Use <code>docker ps -a</code> to list containers and <code>docker images</code> to see available images. Remove test containers with <code>docker rm</code> and clean up images with <code>docker rmi</code> when you're done.</p>"},{"location":"quickstart/github/","title":"Github essentials","text":""},{"location":"quickstart/github/#i-introduction-2-minutes","title":"I. Introduction (2 minutes)","text":""},{"location":"quickstart/github/#a-brief-overview-of-github","title":"A. Brief overview of GitHub:","text":"<p>GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.</p> <p></p> <p>Image source: Artwork by @allison_horst</p>"},{"location":"quickstart/github/#b-introduce-github-desktop-and-jupyterhub-github-widget","title":"B. Introduce GitHub Desktop and JupyterHub GitHub widget:","text":"<p>GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control.</p>"},{"location":"quickstart/github/#1-download-github-desktop","title":"1. Download GitHub Desktop","text":""},{"location":"quickstart/github/#step-1-download-github-desktop","title":"Step 1: Download GitHub Desktop","text":"<p>Go to the GitHub Desktop download page: https://desktop.github.com/</p> <p>Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically.</p>"},{"location":"quickstart/github/#step-2-install-github-desktop","title":"Step 2: Install GitHub Desktop","text":"<p>For Windows:</p> <p>Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer.</p> <p>Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired.</p> <p>Once the installation is complete, GitHub Desktop will launch automatically. For macOS:</p> <p>Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application.</p> <p>Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder.</p> <p>Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application.</p>"},{"location":"quickstart/github/#step-3-set-up-github-desktop","title":"Step 3: Set up GitHub Desktop","text":"<p>When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join.</p> <p>Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d</p> <p>You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup.</p> <p>Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub.</p>"},{"location":"quickstart/github/#1-download-github-for-jupyterhub-cloud-service","title":"1. Download GitHub for JupyterHub cloud service","text":""},{"location":"quickstart/github/#step-1-accessing-jupyterhub-on-the-cloud","title":"Step 1: Accessing JupyterHub on the cloud","text":"<p>Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization).</p> <p>Sign in with your credentials or authenticate using a third-party service if required.</p>"},{"location":"quickstart/github/#step-2-launch-a-new-jupyter-notebook-or-open-an-existing-one","title":"Step 2: Launch a new Jupyter Notebook or open an existing one","text":"<p>Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser.</p> <p>Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code.</p>"},{"location":"quickstart/github/#step-3-install-and-enable-the-jupyterlab-git-extension","title":"Step 3: Install and enable the JupyterLab Git extension","text":"<p>In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension:</p> <p>!pip install jupyterlab-git</p> <p>Restart the Jupyter Notebook server for the changes to take effect.</p>"},{"location":"quickstart/github/#step-4-using-the-jupyterhub-github-widget","title":"Step 4: Using the JupyterHub GitHub widget","text":"<p>In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget.</p> <p>To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. You can now navigate through the cloned repository, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository.</p> <p>To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface.</p> <p>To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget.</p> <p>Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier.</p>"},{"location":"quickstart/github/#c-github-in-rstudio","title":"C. GitHub in Rstudio:","text":"<p>Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub.</p>"},{"location":"quickstart/github/#step-1-install-git","title":"Step 1: Install Git","text":"<p>Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system.</p>"},{"location":"quickstart/github/#step-2-configure-rstudio-to-work-with-git","title":"Step 2: Configure RStudio to work with Git","text":"<p>Open RStudio.</p> <p>Go to \u201cTools\u201d &gt; \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab.</p> <p>Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory).</p> <p>Click \u201cOK\u201d to save the changes.</p>"},{"location":"quickstart/github/#step-3-create-or-open-an-rstudio-project","title":"Step 3: Create or open an RStudio project","text":"<p>To create a new RStudio project, go to \u201cFile\u201d &gt; \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project.</p> <p>To open an existing RStudio project, go to \u201cFile\u201d &gt; \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file.</p>"},{"location":"quickstart/github/#step-4-link-your-rstudio-project-to-a-github-repository","title":"Step 4: Link your RStudio project to a GitHub repository","text":"<p>In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d &gt; \u201cProject Setup.\u201d</p> <p>In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d</p> <p>A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio.</p> <p>To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository.</p> <p>Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d).</p> <p>In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d &gt; \u201cShell.\u201d</p> <p>In the shell, run the following command to add the remote repository:</p> <p>git remote add origin https://github.com/username/repository.git</p> <p>Replace the URL with the one you copied from your GitHub repository.</p> <p>Push your changes to the remote repository by running the following command in the shell:</p> <p>git push -u origin master</p> <p>Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one.</p> <p>By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface.</p>"},{"location":"quickstart/github/#ii-github-basics-4-minutes","title":"II. GitHub Basics (4 minutes)","text":""},{"location":"quickstart/github/#a-repository","title":"A. Repository:","text":"<p>A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators.</p>"},{"location":"quickstart/github/#b-fork-and-clone","title":"B. Fork and Clone:","text":"<p>Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget.</p>"},{"location":"quickstart/github/#c-branches","title":"C. Branches:","text":"<p>Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget.</p>"},{"location":"quickstart/github/#d-replace-master-with-main","title":"D. Replace \u2018master\u2019 with \u2018main\u2019:","text":"<p>In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community.</p>"},{"location":"quickstart/github/#iii-collaboration-and-version-control-5-minutes","title":"III. Collaboration and Version Control (5 minutes)","text":""},{"location":"quickstart/github/#a-commits","title":"A. Commits:","text":"<p>Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button.</p>"},{"location":"quickstart/github/#b-push","title":"B. Push:","text":"<p>In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress.</p> <p>In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors.</p>"},{"location":"quickstart/github/#c-pull-requests","title":"C. Pull Requests:","text":"<p>Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch.</p>"},{"location":"quickstart/github/#d-merging-and-resolving-conflicts","title":"D. Merging and Resolving Conflicts:","text":"<p>Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge.</p>"},{"location":"quickstart/github/#iv-additional-features-2-minutes","title":"IV. Additional Features (2 minutes)","text":""},{"location":"quickstart/github/#a-issues-and-project-management","title":"A. Issues and Project Management:","text":"<p>Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects.</p>"},{"location":"quickstart/github/#b-github-pages","title":"B. GitHub Pages:","text":"<p>GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others.</p>"},{"location":"quickstart/github/#v-conclusion-2-minutes","title":"V. Conclusion (2 minutes)","text":""},{"location":"quickstart/github/#a-recap-of-the-essentials-of-github","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"quickstart/github/#b-encourage-further-exploration-and-learning","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"quickstart/github/#c-share-resources-for-learning-more-about-github","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"quickstart/github/#v-conclusion-2-minutes_1","title":"V. Conclusion (2 minutes)","text":""},{"location":"quickstart/github/#a-recap-of-the-essentials-of-github_1","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"quickstart/github/#b-encourage-further-exploration-and-learning_1","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"quickstart/github/#c-share-resources-for-learning-more-about-github_1","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"quickstart/oasis/","title":"Starting with OASIS","text":"","tags":["quickstart","oasis","docker"]},{"location":"quickstart/oasis/#sell-it","title":"Sell It","text":"<p>OASIS is ESIIL's platform for open science workflows. It links datasets, code, and computing resources in one place so you can move from idea to analysis without juggling multiple tools. Projects live online, making it easy to share reproducible research with collaborators or the public.</p>","tags":["quickstart","oasis","docker"]},{"location":"quickstart/oasis/#show-it","title":"Show It","text":"<p>A typical OASIS project combines documentation and runnable notebooks. The web interface lets you view narratives, launch Jupyter notebooks, and track datasets, all within a single project dashboard.</p>","tags":["quickstart","oasis","docker"]},{"location":"quickstart/oasis/#do-it","title":"Do It","text":"<ol> <li>Sign in. Visit the OASIS homepage and log in with your    credentials.</li> <li>Create a project. Click New Project, give it a descriptive name, and    add a short description.</li> <li>Explore the workspace. Open the project and launch an analysis notebook    to see how data and code connect.</li> <li>Add data. Upload a file or link a dataset from the Data Library.</li> <li>Share it. Invite collaborators or publish the project when you're ready.</li> </ol>","tags":["quickstart","oasis","docker"]},{"location":"quickstart/oasis/#review-it","title":"Review It","text":"<p>Verify that you can open and run the sample notebook. Review the project dashboard and note how OASIS keeps your code, data, and documentation together for future work.</p>","tags":["quickstart","oasis","docker"]},{"location":"quickstart/python/","title":"Data Exploration and Collaboration in the Cloud","text":"<p>Today\u2019s theme is Creative Data Exploration in the Cloud. We\u2019ll learn how to stream data into our CyVerse instance, define key concepts (GDAL, VSI, STAC), and practice opening cloud-hosted datasets directly without downloading them.</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#1-startup-procedure-opening-the-lab","title":"1. Startup Procedure: Opening the Lab","text":"<p>We\u2019ll use the OASIS QuickStart: Cloud Triangle to start our CyVerse instance.</p> <ul> <li>Log in to JupyterHub through CyVerse.  </li> <li>Confirm the triangle environment is running.  </li> <li>Open a terminal and check that <code>gocmd</code> (refer to 7.1 to initialise gocmd before you run them) is ready:</li> </ul> <pre><code>gocmd --help\n</code></pre> <p>If this works, your persistent storage connection is set up.</p> <p>Checkpoint: You now have a shared cloud lab ready to use.</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#2-collaboration-github-workflow","title":"2. Collaboration: GitHub Workflow","text":"<p>We\u2019ll use GitHub to work collaboratively. As last week:</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#preferred-method-github-widget","title":"Preferred Method: GitHub Widget","text":"<ul> <li>Open the GitHub tab in JupyterLab (left sidebar).  </li> <li>Log in with your GitHub account if prompted.  </li> <li>Navigate to your repository.  </li> <li>Edit a <code>.md</code> file, save, and use the widget buttons to Pull \u2192 Commit \u2192 Push.  </li> <li>Confirm the edit appears on GitHub in your browser.</li> </ul>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#alternative-method-command-line","title":"Alternative Method: Command Line","text":"<pre><code>git pull origin main\n# edit your file in JupyterLab, then save\ngit add file.md\ngit commit -m \"edit example\"\ngit push origin main\n</code></pre> <p>Checkpoint: Everyone has successfully pushed an edit to GitHub.</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#3-key-concepts-gdal-vsi-and-stac","title":"3. Key Concepts: GDAL, VSI, and STAC","text":"","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#gdal","title":"GDAL","text":"<ul> <li>Geospatial Data Abstraction Library.  </li> <li>Reads/writes geospatial data (raster and vector).  </li> <li>Used widely in Python (<code>from osgeo import gdal</code>).</li> </ul>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#vsi-virtual-file-system","title":"VSI (Virtual File System)","text":"<ul> <li>A GDAL feature that lets you open remote files as if they were local.  </li> <li>Examples:  </li> <li><code>/vsicurl/</code> \u2192 stream from HTTP/HTTPS  </li> <li><code>/vsis3/</code> \u2192 stream from Amazon S3 buckets  </li> <li><code>/vsizip/</code> \u2192 read inside zipped archives without extracting  </li> </ul>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#stac-spatiotemporal-asset-catalog","title":"STAC (SpatioTemporal Asset Catalog)","text":"<ul> <li>A standard API for discovering and describing geospatial data.  </li> <li>Used by catalogs like Planetary Computer.  </li> <li>Metadata describes spatial extent, time, and available assets.  </li> <li>Tools: <code>pystac-client</code>, <code>planetary_computer</code>.</li> </ul>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#4-example-streaming-with-gdal-vsi","title":"4. Example: Streaming with GDAL + VSI","text":"<pre><code>from osgeo import gdal\n\n# Example: Landsat band 4 hosted on AWS\nurl = \"/vsicurl/https://landsat-pds.s3.amazonaws.com/c1/L8/001/002/LC08_L1TP_001002_20200810_20200823_01_T1/LC08_L1TP_001002_20200810_20200823_01_T1_B4.TIF\"\n\n# Open remote raster\nds = gdal.Open(url)\nprint(\"Raster size:\", ds.RasterXSize, ds.RasterYSize, \"Bands:\", ds.RasterCount)\n\n# Read first band\nband = ds.GetRasterBand(1).ReadAsArray()\nprint(\"Array shape:\", band.shape)\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#5-example-discovering-data-with-stac","title":"5. Example: Discovering Data with STAC","text":"<pre><code># pip install pystac-client planetary-computer\n\nfrom pystac_client import Client\nimport planetary_computer as pc\n\n# Open Planetary Computer STAC\ncatalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n# Search Sentinel-2 over Boulder, CO in July 2021\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[-105.3, 39.9, -105.1, 40.1],\n    datetime=\"2021-07-01/2021-07-31\",\n    limit=1,\n)\n\nitems = list(search.items())\nitem = items[0]\nprint(\"Item ID:\", item.id)\n\n# Get signed URL for red band (B04)\nasset = item.assets[\"B04\"]\nsigned_href = pc.sign(asset.href)\nprint(\"Signed URL:\", signed_href)\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#6-example-combining-stac-vsi","title":"6. Example: Combining STAC + VSI","text":"<p>Use the signed STAC URL with GDAL\u2019s <code>/vsicurl/</code> to stream directly:</p> <pre><code>from osgeo import gdal\n\nvsi_url = \"/vsicurl/\" + signed_href\nds = gdal.Open(vsi_url)\n\nprint(\"Size:\", ds.RasterXSize, ds.RasterYSize, \"Bands:\", ds.RasterCount)\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#7-saving-results-to-the-cyverse-data-store","title":"7. Saving Results to the CyVerse Data Store","text":"<p>Use the CyVerse Data Store for persistence beyond your session. This mirrors last week\u2019s steps (install \u2192 init \u2192 transfer).</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#71-install-and-initialize-gocmd","title":"7.1 Install and Initialize <code>gocmd</code>","text":"<pre><code># Fetch latest release version and install locally\nGOCMD_VER=$(curl -L -s https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt); \\\ncurl -L -s https://github.com/cyverse/gocommands/releases/download/${GOCMD_VER}/gocmd-${GOCMD_VER}-linux-amd64.tar.gz | tar zxvf -\n\n# Initialize with your CyVerse credentials\n./gocmd init\n\n# Confirm login/profile\n./gocmd whoami\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#72-transfer-files-to-the-data-store-upload-initialize-gocmd-for-the-cyverse-data-store","title":"7.2 Transfer files to the Data Store (upload) Initialize <code>gocmd</code> for the CyVerse Data Store","text":"<pre><code># Initialize and follow prompts (use your CyVerse username/password)\n# Common defaults:\n#   Host: data.cyverse.org\n#   Port: 1247\n#   Zone: iplant\n# Accept defaults if pre-filled by the image.\n\ngocmd init\n\n# Confirm login/profile\ngocmd whoami\n\n# Optional: list your home in the Data Store\ngocmd ls i:/\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#73-transfer-files-to-the-data-store-upload","title":"7.3 Transfer files to the Data Store (upload)","text":"<pre><code># Create a folder (first time only)\ngocmd mkdir -p i:my_folder\n\n# Upload a file from the current working directory \u2192 Data Store\n# Example: result from your analysis\ngocmd transfer prism_tipping_point.png i:my_folder/prism_tipping_point.png\n\n# Upload multiple files (globs are expanded by the shell)\ngocmd transfer *.png i:my_folder/\n</code></pre>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#74-transfer-files-from-the-data-store-download","title":"7.4 Transfer files from the Data Store (download)","text":"<pre><code># Download from Data Store \u2192 current directory\ngocmd transfer i:my_folder/prism_tipping_point.png .\n\n# Download a whole folder recursively (to a local folder)\nmkdir -p downloads\ngocmd transfer -r i:my_folder/ downloads/\n</code></pre> <p>This ensures your results are safe and can be accessed later by you or collaborators.</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#key-learning-goals","title":"Key Learning Goals","text":"<ul> <li>Start a CyVerse instance and confirm <code>gocmd</code> works.  </li> <li>Use the GitHub widget for collaborative editing (CLI as backup).  </li> <li>Understand GDAL, VSI, and STAC.  </li> <li>Stream remote datasets with GDAL + VSI.  </li> <li>Discover and sign assets with STAC.  </li> <li>Save outputs to the CyVerse Data Store with <code>gocmd transfer</code>.  </li> </ul>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/python/#theme-recap","title":"Theme Recap","text":"<p>Today we explored streaming data in the cloud. By combining GDAL, VSI, and STAC, we can work collaboratively in our CyVerse instance, pull in new data without downloads, and persist our creative explorations in GitHub and the CyVerse Data Store.</p>","tags":["cloud","streaming","GDAL","VSI","STAC","collaboration"]},{"location":"quickstart/r/","title":"Starting with R","text":""},{"location":"quickstart/r/#sell-it","title":"Sell It","text":"<p>R is a powerful language for statistics and visualization with an active community. Packages like tidyverse make data wrangling intuitive, and the language was designed with data analysis in mind from the start.</p>"},{"location":"quickstart/r/#show-it","title":"Show It","text":"<p>An example script plots a built-in dataset to produce an immediate visualization:</p> <pre><code>plot(cars)\n</code></pre>"},{"location":"quickstart/r/#do-it","title":"Do It","text":"<ol> <li>Install R. Download the latest version from the    R Project.</li> <li>Install RStudio. Get the free    RStudio Desktop IDE for a    user-friendly interface.</li> <li>Open RStudio. Create a new script file and paste in the example above.</li> <li>Install a package. Run <code>install.packages(\"tidyverse\")</code> to add common    tools for data science.</li> <li>Run the script. Highlight the code and click Run or press <code>Ctrl+Enter</code>    to generate the plot.</li> </ol>"},{"location":"quickstart/r/#review-it","title":"Review It","text":"<p>Confirm the plot appears in the Plots pane and use <code>sessionInfo()</code> to see package versions. Explore the Environment and Help tabs to get comfortable with the interface.</p>"},{"location":"quickstart/data-library/","title":"Data Library","text":""},{"location":"quickstart/data-library/#sell-it","title":"Sell It","text":"<p>The Data Library is ESIIL's central hub for sharing vetted datasets so you can start analyzing immediately. Each dataset includes rich metadata, provenance, and ready-to-use code snippets.</p>"},{"location":"quickstart/data-library/#show-it","title":"Show It","text":"<p>Visit a dataset page to see descriptive metadata, citation information, and copy-and-paste code snippets, like on the library homepage.</p>"},{"location":"quickstart/data-library/#do-it","title":"Do It","text":"<ol> <li>Head to the site. Open the    Data Library.</li> <li>Search or browse. Use search or tags to find a dataset relevant to your    question.</li> <li>Read the metadata. Review the description, license, and code snippets.</li> <li>Download and explore. Follow the dataset's instructions or linked    tutorials to load it into your environment.</li> </ol>"},{"location":"quickstart/data-library/#review-it","title":"Review It","text":"<p>Reflect on how the dataset fits your project and explore the guides: - How to Use the Data Library - How to Contribute to the Data Library Consider noting any data quality issues or additional metadata that would help future users.</p>"},{"location":"quickstart/data-library/how-to-use/","title":"How to Use the Data Library","text":"","tags":["data library","quickstart","usage"]},{"location":"quickstart/data-library/how-to-use/#sell-it","title":"Sell It","text":"<p>The Data Library lets you discover curated datasets maintained by ESIIL, saving time on searching and cleaning data. Each entry includes context and code snippets so you can load the data immediately.</p>","tags":["data library","quickstart","usage"]},{"location":"quickstart/data-library/how-to-use/#show-it","title":"Show It","text":"<p>Each dataset page includes rich metadata and code snippets for R or Python to load the data directly. You can preview files and see how others have used the dataset.</p>","tags":["data library","quickstart","usage"]},{"location":"quickstart/data-library/how-to-use/#do-it","title":"Do It","text":"<ol> <li>Navigate to the site. Go to the    Data Library.</li> <li>Browse or search. Look for a dataset that matches your project goals.</li> <li>Open the dataset page. Read the description and preview any available    files.</li> <li>Copy the code snippet. Use the R or Python snippet to download the data    into your environment.</li> <li>Explore the data. Follow the example usage or start your own analysis.</li> </ol>","tags":["data library","quickstart","usage"]},{"location":"quickstart/data-library/how-to-use/#review-it","title":"Review It","text":"<p>Check that the dataset loads correctly in your session and note any questions or improvements to share with the community. Bookmark the dataset page and cite it in your work to give credit to the data providers.</p>","tags":["data library","quickstart","usage"]},{"location":"resources/","title":"Resources hub","text":"<p>Explore curated guides, reference material, and notes that support the OASIS community. Use the collections below to jump to popular resources, or browse the Resources section in the navigation menu for the full list.</p>"},{"location":"resources/#cyverse-how-to-guides","title":"CyVerse how-to guides","text":"<ul> <li>CyVerse basics</li> <li>Start a CyVerse instance</li> <li>Shut down your instance</li> <li>Move and save data in CyVerse</li> <li>Stream data from CyVerse</li> </ul>"},{"location":"resources/#data-skills-and-tooling","title":"Data skills and tooling","text":"<ul> <li>Data analysis workflow</li> <li>Data processing essentials</li> <li>Working with visualizations</li> <li>Markdown basics</li> <li>GitHub basics</li> </ul>"},{"location":"resources/#notes-and-documentation","title":"Notes and documentation","text":"<ul> <li>ESIIL training overview</li> <li>Working groups and postdoc notes</li> <li>Meeting notes and agendas</li> </ul> <p>Looking for something else? Check the navigation sidebar or use the site search to locate additional worksheets, reference material, and project documentation.</p>"},{"location":"resources/art_gallery/","title":"science art","text":"<p>2024-01-25</p>"},{"location":"resources/art_gallery/#tys-art-opinion","title":"Ty\u2019s art opinion","text":"<p>In the context of the ongoing discussions for the redesign of our ESIIL office space, I would like to offer my personal perspective on the art and aesthetic that might enrich our environment:</p> <p>Urban Realism with a Personal Touch: I have a strong appreciation for artworks that reflect a realistic depiction of nature and urban life but with an imaginative twist. Art that integrates with and elevates our daily surroundings could offer a fresh perspective on the mundane.</p> <p>Nature in the Workplace: On a personal note, I find that art which brings elements of the outdoors inside can create a serene and motivating atmosphere, conducive to the values of sustainability that ESIIL embodies.</p> <p>Interactive Art: I believe that art installations which invite interaction or present a playful exaggeration of reality can energize our space. They have the potential to foster a creative dialogue among the team and with visitors.</p> <p>Dimensionality and Engagement: From my viewpoint, art that breaks out of the traditional two-dimensional space and engages with the viewer in three dimensions can transform the feel of an office. Such dynamic pieces could encourage innovative thinking and collaboration.</p> <p>Art with a Message: It\u2019s my opinion that the art we choose should subtly reflect our collective social and environmental commitments. Pieces that prompt introspection about our role in larger societal issues could resonate well with our team\u2019s ethos.</p> <p>Community Connection: Lastly, I feel that our office should not just be a place for work but also a space that invites community interaction. Art can be a bridge between ESIIL and the public, making our office a hub for inspiration and engagement.</p> <p></p> <p></p> <p> </p>"},{"location":"resources/citations/","title":"Citation Management and Notes Collection in Markdown","text":""},{"location":"resources/citations/#introduction","title":"Introduction","text":"<p>This document serves as a guide for managing citations and collecting research notes for our project. We'll use a combination of a <code>.bib</code> file for bibliographic references and Markdown for note-taking.</p>"},{"location":"resources/citations/#part-1-setting-up-your-bib-file-for-citations","title":"Part 1: Setting Up Your .bib File for Citations","text":""},{"location":"resources/citations/#creating-a-bib-file","title":"Creating a .bib File","text":"<ol> <li>Create a new file with a <code>.bib</code> extension, for example, <code>project_references.bib</code>.</li> <li>Add bibliographic entries to this file. Each entry should follow the BibTeX format.</li> </ol>"},{"location":"resources/citations/#example-of-a-bib-entry","title":"Example of a .bib Entry","text":"<p>```bibtex @article{Doe2021,   author  = {Jane Doe and John Smith},   title   = {Insights into Environmental Data Science},   journal = {Journal of Data Science},   year    = {2021},   volume  = {15},   number  = {4},   pages   = {123-145},   doi     = {10.1000/jds.2021.15.4} }</p>"},{"location":"resources/citations/#part-2-using-citations-in-markdown","title":"Part 2: Using Citations in Markdown","text":""},{"location":"resources/citations/#citing-in-your-markdown-document","title":"Citing in Your Markdown Document","text":"<ul> <li>Refer to works in your <code>.bib</code> file using citation keys, like <code>[@Doe2021]</code>.</li> </ul>"},{"location":"resources/citations/#converting-markdown-to-pdf-with-citations","title":"Converting Markdown to PDF with Citations","text":"<ul> <li>Use Pandoc: <code>pandoc yourdoc.md --bibliography=project_references.bib --citeproc -o output.pdf</code></li> </ul>"},{"location":"resources/citations/#part-3-collecting-citations-and-research-notes","title":"Part 3: Collecting Citations and Research Notes","text":""},{"location":"resources/citations/#structuring-your-notes","title":"Structuring Your Notes","text":""},{"location":"resources/citations/#notes-on-doe-2021-doe2021","title":"Notes on Doe 2021 <code>[@Doe2021]</code>","text":"<ul> <li>Key Points:</li> <li>Summary of the article's main arguments.</li> <li> <p>Notable methodologies.</p> </li> <li> <p>Relevance to Our Project:</p> </li> <li>How this research informs our project.</li> <li>Applicable methodologies or theories.</li> </ul>"},{"location":"resources/citations/#notes-on-another-article-another2021","title":"Notes on Another Article <code>[@Another2021]</code>","text":"<ul> <li>Key Points:</li> <li> <p>...</p> </li> <li> <p>Relevance to Our Project:</p> </li> <li>...</li> </ul>"},{"location":"resources/citations/#conclusion","title":"Conclusion","text":"<p>This document facilitates efficient management of references and collaborative knowledge building for our project.</p>"},{"location":"resources/cyverse_basics/","title":"Connecting Cyverse to GitHub","text":""},{"location":"resources/cyverse_basics/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account)</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment https://de.cyverse.org, and log in with your new account.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p> <ol> <li>We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access</li> </ol>"},{"location":"resources/cyverse_basics/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li> <p>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu    </p> </li> <li> <p>Select <code>JupyterLab ESIIL</code> </p> </li> <li> <p>Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>Click <code>Go to analysis</code>:    </p> </li> <li> <p>Now you should see Jupyter Lab!    </p> </li> </ol>"},{"location":"resources/cyverse_basics/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"resources/cyverse_basics/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li> <p>From Jupyter Lab, click on the Git Extension icon on the left menu:    </p> </li> <li> <p>Click <code>Clone a Repository</code> and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click <code>Clone</code>:    </p> </li> <li> <p>You should now see the <code>cyverse-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code> </p> </li> <li> <p>Go into the <code>cyverse-utils</code> folder:    </p> </li> <li> <p>open up the <code>create_github_keypair.ipynb</code> notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: </p> </li> <li> <p>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):    </p> </li> <li> <p>Select <code>SSH and GPG keys</code> </p> </li> <li> <p>Select <code>New SSH key</code> </p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:    </p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!    </p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"resources/cyverse_hacks/","title":"Cyverse fixes","text":""},{"location":"resources/cyverse_hacks/#earth-lab-data-storage","title":"Earth Lab Data Storage","text":"<ul> <li>Path: <code>/home/jovyan/data-store/iplant/home/shared/earthlab/</code></li> <li>Ensure your project has a directory within the Earth Lab data storage.</li> </ul>"},{"location":"resources/cyverse_hacks/#setup","title":"Setup","text":"<ol> <li>CyVerse Account:</li> <li>Create an account if not already owned.</li> <li>Contact Tyson for account upgrades after maximizing current limits.</li> </ol>"},{"location":"resources/cyverse_hacks/#github-connection","title":"GitHub Connection","text":"<ul> <li>Follow Elsa Culler's guide for connecting GitHub to CyVerse.</li> <li>Select \u201cJupyterLab ESIIL\u201d and choose \u201cmacrosystems\u201d in the version dropdown.</li> <li>Clone into <code>/home/jovyan/data-store</code>.</li> <li>Clone <code>innovation-summit-utils</code> for SSH connection to GitHub.</li> <li>Run <code>conda install -c conda-forge openssh</code> in the terminal if encountering errors.</li> <li>GitHub authentication is session-specific.</li> </ul>"},{"location":"resources/cyverse_hacks/#rstudio-in-de","title":"RStudio in DE","text":"<ol> <li>Copy your instance ID. It can be found in your analyis URL in form https://.cyverse.run/lab. <li>Use your ID in these links:  </li> <li><code>https://&lt;id&gt;.cyverse.run/rstudio/auth-sign-in</code> </li> <li><code>https://&lt;id&gt;.cyverse.run/rstudio/</code> </li>"},{"location":"resources/cyverse_hacks/#package-requests","title":"Package Requests","text":"<ul> <li>List desired packages here for future container updates.</li> </ul>"},{"location":"resources/cyverse_hacks/#data-transfer-to-cyverse","title":"Data Transfer to CyVerse","text":"<ul> <li>Use GoCommands for HPC/CyVerse transfers.</li> <li>Installation:</li> <li>Linux: (Command)</li> <li>Windows Powershell: (Command)</li> <li>Usage: </li> <li>Use <code>put</code> for upload and <code>get</code> for download.</li> <li>Ensure correct CyVerse directory path.</li> </ul>"},{"location":"resources/cyverse_move_and_save_data/","title":"Connecting Cyverse to GitHub","text":""},{"location":"resources/cyverse_move_and_save_data/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account)</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment https://de.cyverse.org, and log in with your new account.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p> <ol> <li>We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access</li> </ol>"},{"location":"resources/cyverse_move_and_save_data/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li> <p>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu    </p> </li> <li> <p>Select <code>JupyterLab ESIIL</code> </p> </li> <li> <p>Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>Click <code>Go to analysis</code>:    </p> </li> <li> <p>Now you should see Jupyter Lab!    </p> </li> </ol>"},{"location":"resources/cyverse_move_and_save_data/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"resources/cyverse_move_and_save_data/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li> <p>From Jupyter Lab, click on the Git Extension icon on the left menu:    </p> </li> <li> <p>Click <code>Clone a Repository</code> and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click <code>Clone</code>:    </p> </li> <li> <p>You should now see the <code>cyverse-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code> </p> </li> <li> <p>Go into the <code>cyverse-utils</code> folder:    </p> </li> <li> <p>open up the <code>create_github_keypair.ipynb</code> notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: </p> </li> <li> <p>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):    </p> </li> <li> <p>Select <code>SSH and GPG keys</code> </p> </li> <li> <p>Select <code>New SSH key</code> </p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:    </p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!    </p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"resources/cyverse_shutdown/","title":"Connecting Cyverse to GitHub","text":""},{"location":"resources/cyverse_shutdown/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account)</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment https://de.cyverse.org, and log in with your new account.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p> <ol> <li>We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access</li> </ol>"},{"location":"resources/cyverse_shutdown/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li> <p>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu    </p> </li> <li> <p>Select <code>JupyterLab ESIIL</code> </p> </li> <li> <p>Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>Click <code>Go to analysis</code>:    </p> </li> <li> <p>Now you should see Jupyter Lab!    </p> </li> </ol>"},{"location":"resources/cyverse_shutdown/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"resources/cyverse_shutdown/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li> <p>From Jupyter Lab, click on the Git Extension icon on the left menu:    </p> </li> <li> <p>Click <code>Clone a Repository</code> and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click <code>Clone</code>:    </p> </li> <li> <p>You should now see the <code>cyverse-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code> </p> </li> <li> <p>Go into the <code>cyverse-utils</code> folder:    </p> </li> <li> <p>open up the <code>create_github_keypair.ipynb</code> notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: </p> </li> <li> <p>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):    </p> </li> <li> <p>Select <code>SSH and GPG keys</code> </p> </li> <li> <p>Select <code>New SSH key</code> </p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:    </p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!    </p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"resources/cyverse_startup/","title":"Connecting Cyverse to GitHub","text":""},{"location":"resources/cyverse_startup/#log-in-to-cyverse","title":"Log in to Cyverse","text":"<ol> <li>Go to the Cyverse user account website https://user.cyverse.org/</li> </ol> <ol> <li>Click <code>Sign up</code> (if you do not already have an account)</li> </ol> <ol> <li>Head over to the Cyverse Discovery Environment https://de.cyverse.org, and log in with your new account.</li> </ol> <p>You should now see the Discovery Environment:</p> <p></p> <ol> <li>We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access</li> </ol>"},{"location":"resources/cyverse_startup/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","title":"Open up an analysis with the hackathon environment (Jupyter Lab)","text":"<ol> <li> <p>From the Cyverse Discovery Environment, click on <code>Apps</code> in the left menu    </p> </li> <li> <p>Select <code>JupyterLab ESIIL</code> </p> </li> <li> <p>Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>Click <code>Go to analysis</code>:    </p> </li> <li> <p>Now you should see Jupyter Lab!    </p> </li> </ol>"},{"location":"resources/cyverse_startup/#set-up-your-github-credentials","title":"Set up your GitHub credentials","text":""},{"location":"resources/cyverse_startup/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:","text":"<ol> <li> <p>From Jupyter Lab, click on the Git Extension icon on the left menu:    </p> </li> <li> <p>Click <code>Clone a Repository</code> and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click <code>Clone</code>:    </p> </li> <li> <p>You should now see the <code>cyverse-utils</code> folder in your directory tree (provided you haven't changed directories from the default <code>/home/jovyan/data-store</code> </p> </li> <li> <p>Go into the <code>cyverse-utils</code> folder:    </p> </li> <li> <p>open up the <code>create_github_keypair.ipynb</code> notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: </p> </li> <li> <p>Now you should see the notebook open. Click the <code>play</code> button at the top. You will be prompted to enter your GitHub username and email:    </p> </li> </ol> <p></p> <p></p> <ol> <li> <p>You should now see your Public Key. Copy the WHOLE LINE including <code>ssh-ed25519</code> at the beginning and the <code>jovyan@...</code> at the end </p> </li> <li> <p>Go to your GitHub settings page (you may need to log in to GitHub first):    </p> </li> <li> <p>Select <code>SSH and GPG keys</code> </p> </li> <li> <p>Select <code>New SSH key</code> </p> </li> <li> <p>Give your key a descriptive name, paste your ENTIRE public key in the <code>Key</code> input box, and click <code>Add SSH Key</code>. You may need to re-authenticate with your password or two-factor authentication.:    </p> </li> <li> <p>You should now see your new SSH key in your <code>Authentication Keys</code> list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis!    </p> </li> </ol> <p>NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.</p>"},{"location":"resources/cyverse_stream_data/","title":"Streaming Data in the Cloud with GDAL, VSI, and STAC","text":"<p>Today\u2019s theme is Creative Data Exploration in the Cloud. We\u2019ll learn how to stream data into our CyVerse instance, define key concepts (GDAL, VSI, STAC), and practice opening cloud-hosted datasets directly without downloading them.</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#1-startup-procedure-opening-the-lab","title":"1. Startup Procedure: Opening the Lab","text":"<p>We\u2019ll use the OASIS QuickStart: Cloud Triangle to start our CyVerse instance.</p> <ul> <li>Log in to JupyterHub through CyVerse.  </li> <li>Confirm the triangle environment is running.  </li> <li>Open a terminal and check that <code>gocmd</code> is ready:</li> </ul> <pre><code>gocmd --help\n</code></pre> <p>If this works, your persistent storage connection is set up.</p> <p>Checkpoint: You now have a shared cloud lab ready to use.</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#2-collaboration-github-workflow","title":"2. Collaboration: GitHub Workflow","text":"<p>We\u2019ll use GitHub to work collaboratively. As last week:</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#preferred-method-github-widget","title":"Preferred Method: GitHub Widget","text":"<ul> <li>Open the GitHub tab in JupyterLab (left sidebar).  </li> <li>Log in with your GitHub account if prompted.  </li> <li>Navigate to your repository.  </li> <li>Edit a <code>.md</code> file, save, and use the widget buttons to Pull \u2192 Commit \u2192 Push.  </li> <li>Confirm the edit appears on GitHub in your browser.</li> </ul>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#alternative-method-command-line","title":"Alternative Method: Command Line","text":"<pre><code>git pull origin main\n# edit your file in JupyterLab, then save\ngit add file.md\ngit commit -m \"edit example\"\ngit push origin main\n</code></pre> <p>Checkpoint: Everyone has successfully pushed an edit to GitHub.</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#3-key-concepts-gdal-vsi-and-stac","title":"3. Key Concepts: GDAL, VSI, and STAC","text":"","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#gdal","title":"GDAL","text":"<ul> <li>Geospatial Data Abstraction Library.  </li> <li>Reads/writes geospatial data (raster and vector).  </li> <li>Used widely in Python (<code>from osgeo import gdal</code>).</li> </ul>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#vsi-virtual-file-system","title":"VSI (Virtual File System)","text":"<ul> <li>A GDAL feature that lets you open remote files as if they were local.  </li> <li>Examples:  </li> <li><code>/vsicurl/</code> \u2192 stream from HTTP/HTTPS  </li> <li><code>/vsis3/</code> \u2192 stream from Amazon S3 buckets  </li> <li><code>/vsizip/</code> \u2192 read inside zipped archives without extracting  </li> </ul>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#stac-spatiotemporal-asset-catalog","title":"STAC (SpatioTemporal Asset Catalog)","text":"<ul> <li>A standard API for discovering and describing geospatial data.  </li> <li>Used by catalogs like Planetary Computer.  </li> <li>Metadata describes spatial extent, time, and available assets.  </li> <li>Tools: <code>pystac-client</code>, <code>planetary_computer</code>.</li> </ul>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#4-example-streaming-with-gdal-vsi","title":"4. Example: Streaming with GDAL + VSI","text":"<pre><code>from osgeo import gdal\n\n# Example: Landsat band 4 hosted on AWS\nurl = \"/vsicurl/https://landsat-pds.s3.amazonaws.com/c1/L8/001/002/LC08_L1TP_001002_20200810_20200823_01_T1/LC08_L1TP_001002_20200810_20200823_01_T1_B4.TIF\"\n\n# Open remote raster\nds = gdal.Open(url)\nprint(\"Raster size:\", ds.RasterXSize, ds.RasterYSize, \"Bands:\", ds.RasterCount)\n\n# Read first band\nband = ds.GetRasterBand(1).ReadAsArray()\nprint(\"Array shape:\", band.shape)\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#5-example-discovering-data-with-stac","title":"5. Example: Discovering Data with STAC","text":"<pre><code># pip install pystac-client planetary-computer\n\nfrom pystac_client import Client\nimport planetary_computer as pc\n\n# Open Planetary Computer STAC\ncatalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n# Search Sentinel-2 over Boulder, CO in July 2021\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[-105.3, 39.9, -105.1, 40.1],\n    datetime=\"2021-07-01/2021-07-31\",\n    limit=1,\n)\n\nitems = list(search.items())\nitem = items[0]\nprint(\"Item ID:\", item.id)\n\n# Get signed URL for red band (B04)\nasset = item.assets[\"B04\"]\nsigned_href = pc.sign(asset.href)\nprint(\"Signed URL:\", signed_href)\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#6-example-combining-stac-vsi","title":"6. Example: Combining STAC + VSI","text":"<p>Use the signed STAC URL with GDAL\u2019s <code>/vsicurl/</code> to stream directly:</p> <pre><code>from osgeo import gdal\n\nvsi_url = \"/vsicurl/\" + signed_href\nds = gdal.Open(vsi_url)\n\nprint(\"Size:\", ds.RasterXSize, ds.RasterYSize, \"Bands:\", ds.RasterCount)\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#7-saving-results-to-the-cyverse-data-store","title":"7. Saving Results to the CyVerse Data Store","text":"<p>Use the CyVerse Data Store for persistence beyond your session. This mirrors last week\u2019s steps (install \u2192 init \u2192 transfer).</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#71-install-gocmd-if-not-already-available","title":"7.1 Install <code>gocmd</code> (if not already available)","text":"<pre><code># Check if gocmd is available\nwhich gocmd || echo \"gocmd not found\"\n\n# Install (user scope) if needed\npython -m pip install --user gocmd\n\n# Ensure user-level bin is on PATH (Jupyter terminals sometimes need this)\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Verify\ngocmd --version\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#72-initialize-gocmd-for-the-cyverse-data-store","title":"7.2 Initialize <code>gocmd</code> for the CyVerse Data Store","text":"<pre><code># Initialize and follow prompts (use your CyVerse username/password)\n# Common defaults:\n#   Host: data.cyverse.org\n#   Port: 1247\n#   Zone: iplant\n# Accept defaults if pre-filled by the image.\n\ngocmd init\n\n# Confirm login/profile\ngocmd whoami\n\n# Optional: list your home in the Data Store\ngocmd ls i:/\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#73-transfer-files-to-the-data-store-upload","title":"7.3 Transfer files to the Data Store (upload)","text":"<pre><code># Create a folder (first time only)\ngocmd mkdir -p i:my_folder\n\n# Upload a file from the current working directory \u2192 Data Store\n# Example: result from your analysis\ngocmd transfer prism_tipping_point.png i:my_folder/prism_tipping_point.png\n\n# Upload multiple files (globs are expanded by the shell)\ngocmd transfer *.png i:my_folder/\n</code></pre>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#74-transfer-files-from-the-data-store-download","title":"7.4 Transfer files from the Data Store (download)","text":"<pre><code># Download from Data Store \u2192 current directory\ngocmd transfer i:my_folder/prism_tipping_point.png .\n\n# Download a whole folder recursively (to a local folder)\nmkdir -p downloads\ngocmd transfer -r i:my_folder/ downloads/\n</code></pre> <p>This ensures your results are safe and can be accessed later by you or collaborators.</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#key-learning-goals","title":"Key Learning Goals","text":"<ul> <li>Start a CyVerse instance and confirm <code>gocmd</code> works.  </li> <li>Use the GitHub widget for collaborative editing (CLI as backup).  </li> <li>Understand GDAL, VSI, and STAC.  </li> <li>Stream remote datasets with GDAL + VSI.  </li> <li>Discover and sign assets with STAC.  </li> <li>Save outputs to the CyVerse Data Store with <code>gocmd transfer</code>.  </li> </ul>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/cyverse_stream_data/#theme-recap","title":"Theme Recap","text":"<p>Today we explored streaming data in the cloud. By combining GDAL, VSI, and STAC, we can work collaboratively in our CyVerse instance, pull in new data without downloads, and persist our creative explorations in GitHub and the CyVerse Data Store.</p>","tags":["cloud","streaming","gdal","vsi","stac","collaboration"]},{"location":"resources/data_analysis/","title":"Data Analysis Documentation","text":""},{"location":"resources/data_analysis/#overview","title":"Overview","text":"<p>Brief overview of the data analysis goals and the analytical questions being addressed.</p>"},{"location":"resources/data_analysis/#analysis-methodology","title":"Analysis Methodology","text":"<p>Description of the analytical approach, methods used, and justification for the chosen techniques.</p>"},{"location":"resources/data_analysis/#code-overview","title":"Code Overview","text":"<p>Explanation of the structure of the analysis code, including key functions and their roles.</p>"},{"location":"resources/data_analysis/#running-the-analysis","title":"Running the Analysis","text":"<p>Instructions and example commands for executing the analysis scripts. <pre><code>python analysis_script.py\n</code></pre></p>"},{"location":"resources/data_analysis/#analysis-results","title":"Analysis Results","text":"<p>Summary of key findings from the analysis, including interpretation and relevance.</p>"},{"location":"resources/data_analysis/#challenges-and-solutions","title":"Challenges and Solutions","text":"<p>Discussion of challenges faced during the analysis and solutions or workarounds implemented.</p>"},{"location":"resources/data_analysis/#conclusions","title":"Conclusions","text":"<p>Concluding remarks on the analysis, insights gained, and their potential impact.</p>"},{"location":"resources/data_analysis/#future-work","title":"Future Work","text":"<p>Suggestions for extending or refining the analysis and potential areas for further research.</p>"},{"location":"resources/data_analysis/#references","title":"References","text":"<p>Citations or references to external sources or literature used.</p>"},{"location":"resources/data_processing/","title":"Data Processing Documentation","text":""},{"location":"resources/data_processing/#overview","title":"Overview","text":"<p>Brief description of the data processing objectives and scope. Reminder to adhere to data ownership and usage guidelines.</p>"},{"location":"resources/data_processing/#data-sources","title":"Data Sources","text":"<p>List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines.</p>"},{"location":"resources/data_processing/#cyverse-discovery-environment","title":"CyVerse Discovery Environment","text":"<p>Instructions for setting up and using the CyVerse Discovery Environment for data processing. Tips for cloud-based data access and processing.</p>"},{"location":"resources/data_processing/#data-processing-steps","title":"Data Processing Steps","text":""},{"location":"resources/data_processing/#using-gdal-vsi","title":"Using GDAL VSI","text":"<p>Guidance on using GDAL VSI (Virtual System Interface) for data access and processing. Example commands or scripts: <pre><code>gdal_translate /vsicurl/http://example.com/data.tif output.tif\n</code></pre></p>"},{"location":"resources/data_processing/#cloud-optimized-data","title":"Cloud-Optimized Data","text":"<p>Advantages of using cloud-optimized data formats and processing data without downloading. Instructions for such processes.</p>"},{"location":"resources/data_processing/#data-storage","title":"Data Storage","text":"<p>Information on storing processed data, with guidelines for choosing between the repository and CyVerse Data Store.</p>"},{"location":"resources/data_processing/#best-practices","title":"Best Practices","text":"<p>Recommendations for efficient and responsible data processing in the cloud. Tips to ensure data integrity and reproducibility.</p>"},{"location":"resources/data_processing/#challenges-and-troubleshooting","title":"Challenges and Troubleshooting","text":"<p>Common challenges in data processing and potential solutions. Resources for troubleshooting in the CyVerse Discovery Environment.</p>"},{"location":"resources/data_processing/#conclusions","title":"Conclusions","text":"<p>Summary of the data processing phase and its outcomes. Reflect on the methods used.</p>"},{"location":"resources/data_processing/#references","title":"References","text":"<p>Citations of tools, data sources, and other references used in the data processing phase.</p>"},{"location":"resources/esiil_training/","title":"ESIIL Working Groups training sessions","text":""},{"location":"resources/esiil_training/#introduction-to-esiil-training","title":"Introduction to ESIIL Training","text":"<ul> <li>Brief overview of the training program.</li> <li>Objectives and expected outcomes for the working groups.</li> </ul>"},{"location":"resources/esiil_training/#session-1-the-science-of-team-science-2-hours","title":"Session 1: The Science of Team Science (2 Hours)","text":""},{"location":"resources/esiil_training/#part-1-creating-ethical-and-innovative-work-spaces","title":"Part 1: Creating Ethical and Innovative Work Spaces","text":"<ul> <li>Strategies for fostering ethical and inclusive environments.</li> <li>Techniques for encouraging innovation and creativity in team settings.</li> </ul>"},{"location":"resources/esiil_training/#part-2-effective-communication-and-collaboration","title":"Part 2: Effective Communication and Collaboration","text":"<ul> <li>Best practices for ensuring every team member's voice is heard.</li> <li>Approaches for maintaining productivity and positive team dynamics.</li> <li>Overview of the code of conduct and participant agreement.</li> </ul>"},{"location":"resources/esiil_training/#session-2-foundations-of-environmental-data-science-2-hours","title":"Session 2: Foundations of Environmental Data Science (2 Hours)","text":""},{"location":"resources/esiil_training/#part-1-data-management-ethics-and-github-usage","title":"Part 1: Data Management, Ethics, and GitHub Usage","text":"<ul> <li>Principles of data management in environmental science.</li> <li>Understanding data ethics and ownership guidelines.</li> <li>Tour of GitHub repositories and setup instructions for effective collaboration.</li> </ul>"},{"location":"resources/esiil_training/#part-2-essential-tools-and-technologies","title":"Part 2: Essential Tools and Technologies","text":"<ul> <li>Introduction to key tools and technologies used in ESIIL.</li> <li>Basic training on software and platforms essential for data analysis.</li> </ul>"},{"location":"resources/esiil_training/#session-3-practical-application-and-project-execution-2-hours","title":"Session 3: Practical Application and Project Execution (2 Hours)","text":""},{"location":"resources/esiil_training/#part-1-travel-planning-and-reimbursement","title":"Part 1: Travel Planning and Reimbursement","text":"<ul> <li>Learn how to manage finances and submit paperwork to the University. </li> </ul>"},{"location":"resources/esiil_training/#part-2-hands-on-data-analysis-workflow","title":"Part 2: Hands-on Data Analysis Workflow","text":"<ul> <li>Interactive session on constructing a data analysis pipeline using ESIIL/CyVerse tools.</li> <li>Practical exercises on data processing, analysis, and visualization techniques.</li> <li>Troubleshooting common issues and optimizing workflow efficiency.</li> </ul>"},{"location":"resources/esiil_training/#part-3-wrap-up-and-project-planning","title":"Part 3: Wrap-up and Project Planning","text":"<ul> <li>Strategies for sustaining project momentum and managing long-term research goals.</li> <li>Planning for publication, data sharing, and broader impact.</li> <li>Final Q&amp;A session to address any outstanding questions or concerns.</li> </ul>"},{"location":"resources/esiil_training/#conclusion-and-feedback","title":"Conclusion and Feedback","text":"<ul> <li>Summary of key learnings from all sessions.</li> <li>Encouragement for participants to apply these skills in their respective projects.</li> <li>Collection of feedback for future training improvements.</li> </ul>"},{"location":"resources/esiil_training/#additional-resources","title":"Additional Resources","text":"<ul> <li>List of resources for further learning and exploration.</li> <li>Links to community forums or groups for ongoing support and collaboration.</li> </ul>"},{"location":"resources/esiil_training/#roundtable-event-1-piteam-leads-discussion-2-hours","title":"Roundtable Event 1: PI/Team Leads Discussion (2 Hours)","text":"<ul> <li>A roundtable discussion for Principal Investigators and team leads.</li> <li>Sharing experiences, challenges, and strategies among group leaders.</li> <li>Fostering a collaborative network and problem-solving atmosphere.</li> </ul>"},{"location":"resources/esiil_training/#roundtable-event-2-technical-leads-office-hours-2-hours","title":"Roundtable Event 2: Technical Leads Office Hours (2 Hours)","text":"<ul> <li>A roundtable and office hours session for technical leads.</li> <li>Ensuring a thorough understanding of the ESIIL/CyVerse cyberinfrastructure.</li> <li>Providing technical support and knowledge exchange.</li> </ul>"},{"location":"resources/esiil_training/#conclusion-and-feedback_1","title":"Conclusion and Feedback","text":"<ul> <li>Recap of key takeaways from the training sessions and roundtables.</li> <li>Collection of feedback for continuous improvement of the training program.</li> </ul>"},{"location":"resources/esiil_training/#additional-resources_1","title":"Additional Resources","text":"<ul> <li>Supplementary materials, reading lists, and links to online tutorials and documentation.</li> </ul>"},{"location":"resources/first_meeting_notes/","title":"Primary Meeting 1","text":""},{"location":"resources/first_meeting_notes/#day-1-5-project-kickoff-and-strategy","title":"Day 1-5: Project Kickoff and Strategy","text":""},{"location":"resources/first_meeting_notes/#meeting-details","title":"Meeting Details","text":"<ul> <li>Dates:</li> <li>Times:</li> <li>Location:</li> <li>Facilitator:</li> </ul>"},{"location":"resources/first_meeting_notes/#attendees","title":"Attendees","text":"<ul> <li>List of attendees</li> </ul>"},{"location":"resources/first_meeting_notes/#daily-agenda","title":"Daily Agenda","text":""},{"location":"resources/first_meeting_notes/#day-1-setting-the-stage","title":"Day 1: Setting the Stage","text":""},{"location":"resources/first_meeting_notes/#opening-remarks","title":"Opening Remarks","text":"<ul> <li>Welcoming speech and outline of the week's objectives.</li> </ul>"},{"location":"resources/first_meeting_notes/#project-overview","title":"Project Overview","text":"<ul> <li>Presentation of the project goals and significance.</li> </ul>"},{"location":"resources/first_meeting_notes/#theoretical-framework","title":"Theoretical Framework","text":"<ul> <li>Discussion on the theoretical underpinnings of the project.</li> </ul>"},{"location":"resources/first_meeting_notes/#data-overview","title":"Data Overview","text":"<ul> <li>Review available data and any gaps that need addressing.</li> </ul>"},{"location":"resources/first_meeting_notes/#day-2-4-deep-dives","title":"Day 2-4: Deep Dives","text":""},{"location":"resources/first_meeting_notes/#daily-goals","title":"Daily Goals","text":"<ul> <li>Outline specific goals for each day.</li> </ul>"},{"location":"resources/first_meeting_notes/#task-assignments","title":"Task Assignments","text":"<ul> <li>Assign tasks and areas of responsibility to team members.</li> </ul>"},{"location":"resources/first_meeting_notes/#theory-and-data-synthesis","title":"Theory and Data Synthesis","text":"<ul> <li>Host focused discussions on how theory will inform data analysis.</li> <li>Explore different methodological approaches and data integration strategies.</li> </ul>"},{"location":"resources/first_meeting_notes/#evening-social-and-soft-work-sessions","title":"Evening Social and Soft Work Sessions","text":"<ul> <li>Casual gatherings to further discuss ideas and foster team bonding.</li> </ul>"},{"location":"resources/first_meeting_notes/#day-5-roadmap-and-closure","title":"Day 5: Roadmap and Closure","text":""},{"location":"resources/first_meeting_notes/#project-roadmap","title":"Project Roadmap","text":"<ul> <li>Draft a detailed plan of action for the project going forward.</li> </ul>"},{"location":"resources/first_meeting_notes/#responsibilities","title":"Responsibilities","text":"<ul> <li>Confirm individual responsibilities and deadlines.</li> </ul>"},{"location":"resources/first_meeting_notes/#review-and-feedback","title":"Review and Feedback","text":"<ul> <li>Reflect on the week's discussions and adjust the project plan as needed.</li> </ul>"},{"location":"resources/first_meeting_notes/#closing-remarks","title":"Closing Remarks","text":"<ul> <li>Summarize achievements and express appreciation for the team's efforts.</li> </ul>"},{"location":"resources/first_meeting_notes/#detailed-notes","title":"Detailed Notes","text":""},{"location":"resources/first_meeting_notes/#day-1-notes","title":"Day 1 Notes","text":"<ul> <li>Summary of discussions, decisions, and key points.</li> </ul>"},{"location":"resources/first_meeting_notes/#day-2-notes","title":"Day 2 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/first_meeting_notes/#day-3-notes","title":"Day 3 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/first_meeting_notes/#day-4-notes","title":"Day 4 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/first_meeting_notes/#day-5-notes","title":"Day 5 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/first_meeting_notes/#action-items","title":"Action Items","text":"<ul> <li> Specific task: Assigned to - Deadline</li> <li> Specific task: Assigned to - Deadline</li> <li>...</li> </ul>"},{"location":"resources/first_meeting_notes/#reflections-and-comments","title":"Reflections and Comments","text":"<ul> <li>(Space for any additional thoughts, insights, or personal reflections on the meeting.)</li> </ul>"},{"location":"resources/first_meeting_notes/#next-steps","title":"Next Steps","text":"<ul> <li>Schedule for follow-up meetings or checkpoints.</li> <li>Outline of expected progress before the next primary meeting.</li> </ul>"},{"location":"resources/first_meeting_notes/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>(Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)</li> </ul>"},{"location":"resources/github_basics/","title":"Github essentials","text":""},{"location":"resources/github_basics/#i-introduction-2-minutes","title":"I. Introduction (2 minutes)","text":""},{"location":"resources/github_basics/#a-brief-overview-of-github","title":"A. Brief overview of GitHub:","text":"<p>GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.</p> <p></p> <p>Image source: Artwork by @allison_horst</p>"},{"location":"resources/github_basics/#b-introduce-github-desktop-and-jupyterhub-github-widget","title":"B. Introduce GitHub Desktop and JupyterHub GitHub widget:","text":"<p>GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control.</p>"},{"location":"resources/github_basics/#1-download-github-desktop","title":"1. Download GitHub Desktop","text":""},{"location":"resources/github_basics/#step-1-download-github-desktop","title":"Step 1: Download GitHub Desktop","text":"<p>Go to the GitHub Desktop download page: https://desktop.github.com/</p> <p>Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically.</p>"},{"location":"resources/github_basics/#step-2-install-github-desktop","title":"Step 2: Install GitHub Desktop","text":"<p>For Windows:</p> <p>Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer.</p> <p>Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired.</p> <p>Once the installation is complete, GitHub Desktop will launch automatically. For macOS:</p> <p>Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application.</p> <p>Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder.</p> <p>Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application.</p>"},{"location":"resources/github_basics/#step-3-set-up-github-desktop","title":"Step 3: Set up GitHub Desktop","text":"<p>When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join.</p> <p>Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d</p> <p>You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup.</p> <p>Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub.</p>"},{"location":"resources/github_basics/#1-download-github-for-jupyterhub-cloud-service","title":"1. Download GitHub for JupyterHub cloud service","text":""},{"location":"resources/github_basics/#step-1-accessing-jupyterhub-on-the-cloud","title":"Step 1: Accessing JupyterHub on the cloud","text":"<p>Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization).</p> <p>Sign in with your credentials or authenticate using a third-party service if required.</p>"},{"location":"resources/github_basics/#step-2-launch-a-new-jupyter-notebook-or-open-an-existing-one","title":"Step 2: Launch a new Jupyter Notebook or open an existing one","text":"<p>Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser.</p> <p>Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code.</p>"},{"location":"resources/github_basics/#step-3-install-and-enable-the-jupyterlab-git-extension","title":"Step 3: Install and enable the JupyterLab Git extension","text":"<p>In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension:</p> <p>!pip install jupyterlab-git</p> <p>Restart the Jupyter Notebook server for the changes to take effect.</p>"},{"location":"resources/github_basics/#step-4-using-the-jupyterhub-github-widget","title":"Step 4: Using the JupyterHub GitHub widget","text":"<p>In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget.</p> <p>To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. You can now navigate through the cloned repository, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository.</p> <p>To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface.</p> <p>To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget.</p> <p>Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier.</p>"},{"location":"resources/github_basics/#c-github-in-rstudio","title":"C. GitHub in Rstudio:","text":"<p>Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub.</p>"},{"location":"resources/github_basics/#step-1-install-git","title":"Step 1: Install Git","text":"<p>Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system.</p>"},{"location":"resources/github_basics/#step-2-configure-rstudio-to-work-with-git","title":"Step 2: Configure RStudio to work with Git","text":"<p>Open RStudio.</p> <p>Go to \u201cTools\u201d &gt; \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab.</p> <p>Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory).</p> <p>Click \u201cOK\u201d to save the changes.</p>"},{"location":"resources/github_basics/#step-3-create-or-open-an-rstudio-project","title":"Step 3: Create or open an RStudio project","text":"<p>To create a new RStudio project, go to \u201cFile\u201d &gt; \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project.</p> <p>To open an existing RStudio project, go to \u201cFile\u201d &gt; \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file.</p>"},{"location":"resources/github_basics/#step-4-link-your-rstudio-project-to-a-github-repository","title":"Step 4: Link your RStudio project to a GitHub repository","text":"<p>In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d &gt; \u201cProject Setup.\u201d</p> <p>In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d</p> <p>A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio.</p> <p>To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository.</p> <p>Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d).</p> <p>In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d &gt; \u201cShell.\u201d</p> <p>In the shell, run the following command to add the remote repository:</p> <p>git remote add origin https://github.com/username/repository.git</p> <p>Replace the URL with the one you copied from your GitHub repository.</p> <p>Push your changes to the remote repository by running the following command in the shell:</p> <p>git push -u origin master</p> <p>Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one.</p> <p>By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface.</p>"},{"location":"resources/github_basics/#ii-github-basics-4-minutes","title":"II. GitHub Basics (4 minutes)","text":""},{"location":"resources/github_basics/#a-repository","title":"A. Repository:","text":"<p>A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators.</p>"},{"location":"resources/github_basics/#b-fork-and-clone","title":"B. Fork and Clone:","text":"<p>Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget.</p>"},{"location":"resources/github_basics/#c-branches","title":"C. Branches:","text":"<p>Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget.</p>"},{"location":"resources/github_basics/#d-replace-master-with-main","title":"D. Replace \u2018master\u2019 with \u2018main\u2019:","text":"<p>In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community.</p>"},{"location":"resources/github_basics/#iii-collaboration-and-version-control-5-minutes","title":"III. Collaboration and Version Control (5 minutes)","text":""},{"location":"resources/github_basics/#a-commits","title":"A. Commits:","text":"<p>Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button.</p>"},{"location":"resources/github_basics/#b-push","title":"B. Push:","text":"<p>In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress.</p> <p>In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors.</p>"},{"location":"resources/github_basics/#c-pull-requests","title":"C. Pull Requests:","text":"<p>Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch.</p>"},{"location":"resources/github_basics/#d-merging-and-resolving-conflicts","title":"D. Merging and Resolving Conflicts:","text":"<p>Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge.</p>"},{"location":"resources/github_basics/#iv-additional-features-2-minutes","title":"IV. Additional Features (2 minutes)","text":""},{"location":"resources/github_basics/#a-issues-and-project-management","title":"A. Issues and Project Management:","text":"<p>Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects.</p>"},{"location":"resources/github_basics/#b-github-pages","title":"B. GitHub Pages:","text":"<p>GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others.</p>"},{"location":"resources/github_basics/#v-conclusion-2-minutes","title":"V. Conclusion (2 minutes)","text":""},{"location":"resources/github_basics/#a-recap-of-the-essentials-of-github","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"resources/github_basics/#b-encourage-further-exploration-and-learning","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"resources/github_basics/#c-share-resources-for-learning-more-about-github","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"resources/github_basics/#v-conclusion-2-minutes_1","title":"V. Conclusion (2 minutes)","text":""},{"location":"resources/github_basics/#a-recap-of-the-essentials-of-github_1","title":"A. Recap of the essentials of GitHub:","text":"<p>In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.</p>"},{"location":"resources/github_basics/#b-encourage-further-exploration-and-learning_1","title":"B. Encourage further exploration and learning:","text":"<p>While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.</p>"},{"location":"resources/github_basics/#c-share-resources-for-learning-more-about-github_1","title":"C. Share resources for learning more about GitHub:","text":"<p>There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.</p>"},{"location":"resources/manuscript/","title":"Manuscript Title","text":""},{"location":"resources/manuscript/#authors","title":"Authors","text":"<ul> <li>Author 1, Affiliation</li> <li>Author 2, Affiliation</li> <li>...</li> </ul>"},{"location":"resources/manuscript/#abstract","title":"Abstract","text":"<ul> <li>A brief summary of the research, its objectives, main findings, and conclusions.</li> </ul>"},{"location":"resources/manuscript/#introduction","title":"Introduction","text":"<ul> <li>Background information and context setting for the research.</li> <li>Statement of the problem and research objectives.</li> <li>Overview of the methodology and approach.</li> </ul>"},{"location":"resources/manuscript/#literature-review","title":"Literature Review","text":"<ul> <li>Discussion of relevant previous work and how this research contributes to the field.</li> </ul>"},{"location":"resources/manuscript/#methodology","title":"Methodology","text":"<ul> <li>Detailed description of the research methodology.</li> <li>Explanation of data collection and analysis techniques.</li> <li>Justification for methodological choices.</li> </ul>"},{"location":"resources/manuscript/#results","title":"Results","text":"<ul> <li>Presentation of the research findings.</li> <li>Use of tables, graphs, and figures to illustrate key points.</li> <li>Analysis and interpretation of the results.</li> </ul>"},{"location":"resources/manuscript/#discussion","title":"Discussion","text":"<ul> <li>Discussion of the implications of the findings.</li> <li>Comparison with previous research in the field.</li> <li>Consideration of the limitations of the study.</li> </ul>"},{"location":"resources/manuscript/#conclusion","title":"Conclusion","text":"<ul> <li>Summary of the main findings.</li> <li>Reflection on the research's significance and potential impact.</li> <li>Suggestions for future research directions.</li> </ul>"},{"location":"resources/manuscript/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>Acknowledgement of any assistance, funding, or contributions from others.</li> </ul>"},{"location":"resources/manuscript/#references","title":"References","text":"<ul> <li>Bibliographic details of the cited works.</li> <li>Use a consistent citation style throughout.</li> </ul>"},{"location":"resources/manuscript/#appendices","title":"Appendices","text":"<ul> <li>Additional material that supports the manuscript but is too detailed for the main sections.</li> </ul>"},{"location":"resources/markdown_basics/","title":"Markdown for the Modern Researcher at ESIIL","text":"<p>Join us on a HackMD page to practice Markdown</p>"},{"location":"resources/markdown_basics/#section-1-mastering-markdown-syntax","title":"Section 1: Mastering Markdown Syntax","text":""},{"location":"resources/markdown_basics/#1-fundamentals-of-text-formatting","title":"1. Fundamentals of Text Formatting","text":"<ul> <li>Headings: Use <code>#</code> for different levels of headings.</li> <li> </li> <li> </li> <li> </li> <li> <p>Lists: Bulleted lists use asterisks, numbers for ordered lists.</p> </li> <li>Item 1</li> <li>Item 2<ul> <li>Subitem 2.1</li> <li>Subitem 2.2</li> </ul> </li> <li> <ol> <li>First item</li> </ol> </li> <li> <ol> <li>Second item</li> </ol> </li> <li> <p>Bold and Italics: Use asterisks or underscores.</p> </li> <li>Bold Text</li> <li>Italic Text</li> </ul>"},{"location":"resources/markdown_basics/#heading-level-1","title":"Heading Level 1","text":""},{"location":"resources/markdown_basics/#heading-level-2","title":"Heading Level 2","text":""},{"location":"resources/markdown_basics/#heading-level-3","title":"Heading Level 3","text":""},{"location":"resources/markdown_basics/#2-advanced-structures","title":"2. Advanced Structures","text":"<ul> <li>Tables: Create tables using dashes and pipes.</li> <li> Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data </li> <li> <p>Add a \":\"\" to change text justification. Here the : is added on the left for left justification.     | Header 1 | Header 2 | Header 3 |     |---------:|--------- |----------|     | Row 1    | Data     | Data     |     | Row 2    | Data     | Data     |</p> </li> <li> A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C </li> <li> <p>If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. </p> </li> </ul> <p>Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition.</p> 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 534678912 672195348 198342567 859761423 426853791 713924856 961537284 287419635 345286179 <ul> <li>Blockquotes: Use <code>&gt;</code> for blockquotes.</li> <li> <p>This is a blockquote.</p> </li> <li> <p>It can span multiple lines.</p> </li> </ul>"},{"location":"resources/markdown_basics/#3-integrating-multimedia","title":"3. Integrating Multimedia","text":"<ul> <li>Images: Add images using the format <code>![alt text](image_url)</code>.</li> <li> </li> <li> <p>Videos: Embed videos using HTML in Markdown.</p> </li> <li><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"resources/markdown_basics/#4-diagrams-with-mermaid","title":"4. Diagrams with Mermaid","text":"<ul> <li>Flowcharts:</li> </ul> <pre><code>    graph TD\n    A[Start] --&gt; B[Analyze Data]\n    B --&gt; C{Is Data Large?}\n    C --&gt;|Yes| D[Apply Big Data Solutions]\n    C --&gt;|No| E[Use Traditional Methods]\n    D --&gt; F[Machine Learning]\n    E --&gt; G[Statistical Analysis]\n    F --&gt; H{Model Accurate?}\n    G --&gt; I[Report Results]\n    H --&gt;|Yes| J[Deploy Model]\n    H --&gt;|No| K[Refine Model]\n    J --&gt; L[Monitor Performance]\n    K --&gt; F\n    L --&gt; M[End: Success]\n    I --&gt; N[End: Report Generated]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style M fill:#9f9,stroke:#333,stroke-width:2px\n    style N fill:#9f9,stroke:#333,stroke-width:2px</code></pre> <ul> <li> <p>Mind Maps: <pre><code>    mindmap\n  root((ESIIL))\n    section Data Sources\n      Satellite Imagery\n        ::icon(fa fa-satellite)\n      Remote Sensing Data\n        Drones\n        Aircraft\n      On-ground Sensors\n        Weather Stations\n        IoT Devices\n      Open Environmental Data\n        Public Datasets\n        ::icon(fa fa-database)\n    section Research Focus\n      Climate Change Analysis\n        Ice Melt Patterns\n        Sea Level Rise\n      Biodiversity Monitoring\n        Species Distribution\n        Habitat Fragmentation\n      Geospatial Analysis Techniques\n        Machine Learning Models\n        Predictive Analytics\n    section Applications\n      Conservation Strategies\n        ::icon(fa fa-leaf)\n      Urban Planning\n        Green Spaces\n      Disaster Response\n        Flood Mapping\n        Wildfire Tracking\n    section Tools and Technologies\n      GIS Software\n        QGIS\n        ArcGIS\n      Programming Languages\n        Python\n        R\n      Cloud Computing Platforms\n        AWS\n        Google Earth Engine\n      Data Visualization\n        D3.js\n        Tableau</code></pre></p> </li> <li> <p>Timelines:</p> </li> </ul> <pre><code>gantt\n    title ESIIL Year 2 Project Schedule\n    dateFormat  YYYY-MM-DD\n    section CI\n    Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30\n    OASIS documentation                    :2024-09-15, 70d\n    Data cube OASIS via cyverse account    :2024-09-15, 100d\n    Integrate with ESIIL User Management system :2024-08-01, 2024-11-30\n    Build badges to deploy DE from mkdoc   :2024-09-01, 2024-12-15\n    Streamline Github ssh key management   :2024-10-01, 2024-12-31\n    Cyverse support (R proxy link)         :2024-11-01, 2024-12-31\n    Cyverse use summary and statistics     :2024-08-01, 2024-12-15\n\n    section CI Consultation and Education\n    Conferences/Invited talks              :2024-08-01, 2024-12-31\n    Office hours                           :2024-08-15, 2024-12-15\n    Proposals                              :2024-09-01, 2024-11-15\n    Private lessons                        :2024-09-15, 2024-11-30\n    Pre-event trainings                    :2024-10-01, 2024-12-15\n    Textbook development w/ education team :2024-08-01, 2024-12-15\n    Train the trainers / group lessons     :2024-08-15, 2024-11-30\n    Tribal engagement                      :2024-09-01, 2024-12-15\n    Ethical Space training                 :2024-09-15, 2024-12-31\n\n    section CI Design and Build\n    Data library (repository)              :2024-08-01, 2024-10-30\n    Analytics library (repository)         :2024-08-15, 2024-11-15\n    Containers (repository)                :2024-09-01, 2024-11-30\n    Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15\n    Tribal resilience Data Cube            :2024-10-01, 2024-12-31</code></pre> <pre><code>\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%%\ngitGraph\n  commit id: \"Start from template\"\n  branch c1\n  commit id: \"Set up SSH key pair\"\n  commit id: \"Modify _config.yml for GitHub Pages\"\n  commit id: \"Initial website structure\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\n  commit id: \"Edit existing pages\"\n  commit id: \"Delete old markdown pages\"\n  commit id: \"Finalize website updates\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\ncheckout c1\n\n  branch b1\n\n  commit\n  commit\n  checkout c1\n  merge b1</code></pre> <pre><code>%%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%%\nquadrantChart\n  x-axis Urgent --&gt; Not Urgent\n  y-axis Not Important --&gt; \"Important \u2764\"\n  quadrant-1 Plan\n  quadrant-2 Do\n  quadrant-3 Delegate\n  quadrant-4 Delete</code></pre> <pre><code>timeline\n    title Major Events in Environmental Science and Data Science\n    section Environmental Science\n        19th century : Foundations in Ecology and Conservation\n        1962 : Publication of 'Silent Spring' by Rachel Carson\n        1970 : First Earth Day\n        1987 : Brundtland Report introduces Sustainable Development\n        1992 : Rio Earth Summit\n        2015 : Paris Agreement on Climate Change\n    section Data Science\n        1960s-1970s : Development of Database Management Systems\n        1980s : Emergence of Data Warehousing\n        1990s : Growth of the World Wide Web and Data Mining\n        2000s : Big Data and Predictive Analytics\n        2010s : AI and Machine Learning Revolution\n        2020s : Integration of AI in Environmental Research</code></pre> <pre><code>erDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }</code></pre> <pre><code>---\nconfig:\n  sankey:\n    showValues: false\n---\nsankey-beta\n\nNASA Data,Big Data Harmonization,100\n    Satellite Imagery,Big Data Harmonization,80\n    Open Environmental Data,Big Data Harmonization,70\n    Remote Sensing Data,Big Data Harmonization,90\n    Big Data Harmonization, Data Analysis and Integration,340\n    Data Analysis and Integration,Climate Change Research,100\n    Data Analysis and Integration,Biodiversity Monitoring,80\n    Data Analysis and Integration,Geospatial Mapping,60\n    Data Analysis and Integration,Urban Planning,50\n    Data Analysis and Integration,Disaster Response,50</code></pre>"},{"location":"resources/markdown_basics/#5-interactive-elements","title":"5. Interactive Elements","text":"<ul> <li>Hyperlinks: Use the format <code>[link text](URL)</code>.</li> <li>Google</li> <li> <p>Play Tetris</p> </li> <li> <p>Embedding Interactive Content: Use HTML tags or specific platform embed codes.</p> </li> <li><code>&lt;iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"resources/markdown_basics/#6-math-notation","title":"6. Math Notation","text":"<p>Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX).</p> <ul> <li>Inline Math: Use single dollar signs for inline math expressions. Representing the normal distribution.</li> </ul> <p>Example: The probability density function of the normal distribution is given by \\(f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\).`</p> <ul> <li>Display Math: Use double dollar signs for standalone equations.</li> </ul> <p>Example:   $$   f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e<sup>{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)</sup>2}   $$</p> <ul> <li>Common LaTeX Elements for Environmental Data Science:</li> <li>Statistical Distributions:<ul> <li>Normal Distribution: <code>\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}</code> for \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)</li> <li>Poisson Distribution: <code>P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</code> for \\(P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)</li> </ul> </li> <li>Coordinate Systems:<ul> <li>Spherical Coordinates: <code>(r, \\theta, \\phi)</code> for \\((r, \\theta, \\phi)\\)</li> <li>Cartesian Coordinates: <code>(x, y, z)</code> for \\((x, y, z)\\)</li> </ul> </li> <li>Geospatial Equations:<ul> <li>Haversine Formula for Distance: <code>a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)</code> for \\(a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)\\)</li> </ul> </li> </ul> <p>Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities.</p>"},{"location":"resources/markdown_basics/#7-effective-citations-in-markdown","title":"7. Effective Citations in Markdown","text":""},{"location":"resources/markdown_basics/#inline-citations","title":"Inline Citations","text":"<ul> <li>Objective: Learn how to use inline citations in Markdown.</li> <li>Example Usage:</li> <li>Inline citation of a single work: <ul> <li>Some text with an inline citation. [@jones:envstudy:2020]</li> </ul> </li> <li>Inline citation with specific page or section: <ul> <li>More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2]</li> </ul> </li> <li>Contrasting views: <ul> <li>Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78]</li> </ul> </li> </ul>"},{"location":"resources/markdown_basics/#footnote-citations","title":"Footnote Citations","text":"<ul> <li>Objective: Understand how to use footnote citations in Markdown.</li> <li>Example Usage:</li> <li>Citing with a footnote: <ul> <li>Some statement in the text.<sup>1</sup></li> </ul> </li> <li>Multiple references to the same footnote: <ul> <li>Another statement referring to the same source.<sup>1</sup></li> </ul> </li> <li>A different citation: <ul> <li>Additional comment with a new citation.<sup>2</sup></li> </ul> </li> </ul>"},{"location":"resources/markdown_basics/#creating-footnotes","title":"Creating Footnotes","text":"<ul> <li>Example Syntax:</li> <li></li> <li></li> </ul> <ol> <li> <p>First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"resources/notes_from_readings/","title":"Literature Reading Notes","text":""},{"location":"resources/notes_from_readings/#reference-information","title":"Reference Information","text":"<ul> <li>Title: </li> <li>Authors: </li> <li>Publication Year: </li> <li>Journal/Source: </li> <li>DOI/URL: </li> </ul>"},{"location":"resources/notes_from_readings/#summary","title":"Summary","text":"<ul> <li>Brief summary of the main objective, research question, or thesis of the literature.</li> </ul>"},{"location":"resources/notes_from_readings/#key-findings","title":"Key Findings","text":"<ul> <li>Major findings or conclusions:</li> <li>Finding 1</li> <li>Finding 2</li> <li>...</li> </ul>"},{"location":"resources/notes_from_readings/#methodology","title":"Methodology","text":"<ul> <li>Description of research methodology, techniques, or approaches.</li> <li>Notable tools, datasets, or analytical methods used.</li> </ul>"},{"location":"resources/notes_from_readings/#theoretical-framework","title":"Theoretical Framework","text":"<ul> <li>Theoretical models or frameworks underpinning the research.</li> <li>Positioning within the broader field.</li> </ul>"},{"location":"resources/notes_from_readings/#critical-analysis","title":"Critical Analysis","text":"<ul> <li>Strengths: Well-executed aspects or convincing arguments.</li> <li>Limitations: Weaknesses, gaps, or biases.</li> <li>Insights: New understandings or perspectives gained.</li> </ul>"},{"location":"resources/notes_from_readings/#connections-to-other-work","title":"Connections to Other Work","text":"<ul> <li>Similarities or differences with other readings.</li> <li>Complementarity to other studies.</li> </ul>"},{"location":"resources/notes_from_readings/#quotations-and-notes","title":"Quotations and Notes","text":"<ul> <li>Significant quotes:</li> <li>\"Quote here.\" - Author Name, page number</li> <li>Additional notes or comments.</li> </ul>"},{"location":"resources/notes_from_readings/#personal-reflections","title":"Personal Reflections","text":"<ul> <li>Influence on understanding or perspective.</li> <li>Potential impact on future research or studies.</li> </ul>"},{"location":"resources/notes_from_readings/#action-items","title":"Action Items","text":"<ul> <li>Follow-up actions such as readings, discussions, or research activities:</li> <li> Action item 1</li> <li> Action item 2</li> <li>...</li> </ul>"},{"location":"resources/post_meeting_notes/","title":"Post-Meeting Notes Template","text":""},{"location":"resources/post_meeting_notes/#meeting-details","title":"Meeting Details","text":"<ul> <li>Date:</li> <li>Time:</li> <li>Location:</li> <li>Facilitator:</li> </ul>"},{"location":"resources/post_meeting_notes/#attendees","title":"Attendees","text":"<ul> <li>List of attendees</li> </ul>"},{"location":"resources/post_meeting_notes/#agenda","title":"Agenda","text":""},{"location":"resources/post_meeting_notes/#1-review-of-meeting-goals","title":"1. Review of Meeting Goals","text":"<ul> <li>Recap the primary objectives and if they were met.</li> </ul>"},{"location":"resources/post_meeting_notes/#2-manuscript-development","title":"2. Manuscript Development","text":"<ul> <li>Discuss the status of current manuscript drafts.</li> <li>Assign writing and editing tasks for different sections of the manuscript.</li> <li>Set deadlines for draft completion and review.</li> </ul>"},{"location":"resources/post_meeting_notes/#3-research-highlights","title":"3. Research Highlights","text":"<ul> <li>Identify key findings and outcomes that should be emphasized in the publications.</li> <li>Discuss any new research insights that emerged from the meeting.</li> </ul>"},{"location":"resources/post_meeting_notes/#4-publication-strategy","title":"4. Publication Strategy","text":"<ul> <li>Decide on target journals or conferences for publication submission.</li> <li>Discuss authorship order and contributions.</li> <li>Plan for any additional data or research needed to strengthen the manuscript.</li> </ul>"},{"location":"resources/post_meeting_notes/#5-editing-and-review-process","title":"5. Editing and Review Process","text":"<ul> <li>Establish a peer-review process within the group for initial feedback.</li> <li>Assign members to focus on specific aspects of editing, such as clarity, grammar, and technical accuracy.</li> <li>Agree on a schedule for review rounds to ensure timely submission.</li> </ul>"},{"location":"resources/post_meeting_notes/#6-responsibilities-and-expectations","title":"6. Responsibilities and Expectations","text":"<ul> <li>Clearly define what is expected from each member before the next meeting.</li> <li>Discuss communication methods for progress updates and questions.</li> </ul>"},{"location":"resources/post_meeting_notes/#7-closing-remarks","title":"7. Closing Remarks","text":"<ul> <li>Summarize the discussion and confirm the action plan.</li> <li>Reiterate the importance of meeting the set deadlines and maintaining communication.</li> </ul>"},{"location":"resources/post_meeting_notes/#action-items","title":"Action Items","text":"<ul> <li> Draft introduction section: Responsible person(s) - Deadline</li> <li> Compile and analyze additional data: Responsible person(s) - Deadline</li> <li> Draft methodology section: Responsible person(s) - Deadline</li> <li>...</li> <li> Coordinate manuscript peer review: Responsible person(s) - Deadline</li> </ul>"},{"location":"resources/post_meeting_notes/#next-steps","title":"Next Steps","text":"<ul> <li>Define the timeline for the submission process.</li> <li>Schedule follow-up meetings or check-ins to monitor progress.</li> </ul>"},{"location":"resources/post_meeting_notes/#notes","title":"Notes","text":"<ul> <li>(Additional notes, comments, or observations made during the meeting.)</li> </ul>"},{"location":"resources/pre_meeting_notes/","title":"Pre-Meeting Notes","text":""},{"location":"resources/pre_meeting_notes/#meeting-details","title":"Meeting Details","text":"<ul> <li>Date:</li> <li>Time:</li> <li>Location:</li> <li>Facilitator:</li> </ul>"},{"location":"resources/pre_meeting_notes/#attendees","title":"Attendees","text":"<ul> <li>List of attendees</li> </ul>"},{"location":"resources/pre_meeting_notes/#agenda","title":"Agenda","text":""},{"location":"resources/pre_meeting_notes/#1-opening-remarks","title":"1. Opening Remarks","text":"<ul> <li>Brief welcome and overview of the meeting's objectives.</li> </ul>"},{"location":"resources/pre_meeting_notes/#2-introductions","title":"2. Introductions","text":"<ul> <li>Roundtable introductions for all attendees.</li> <li>Share a personal note or interesting fact to foster camaraderie.</li> </ul>"},{"location":"resources/pre_meeting_notes/#3-planning","title":"3. Planning","text":"<ul> <li>Discuss the agenda for the primary meetings.</li> <li>Outline the key topics and issues to address.</li> <li>Assign roles for note-taking, timekeeping, and facilitation in primary meetings.</li> </ul>"},{"location":"resources/pre_meeting_notes/#4-goal-setting","title":"4. Goal Setting","text":"<ul> <li>Establish clear, actionable goals for the upcoming period.</li> <li>Identify specific outcomes desired from the primary meetings.</li> <li>Agree on metrics or indicators of success for these goals.</li> </ul>"},{"location":"resources/pre_meeting_notes/#5-camaraderie-building","title":"5. Camaraderie Building","text":"<ul> <li>Icebreaker activity or team-building exercise.</li> <li>Share expectations and aspirations for the group's progress.</li> <li>Highlight the importance of collaboration and mutual support.</li> </ul>"},{"location":"resources/pre_meeting_notes/#6-open-discussion","title":"6. Open Discussion","text":"<ul> <li>Allow for any additional topics, concerns, or ideas to be brought forward.</li> </ul>"},{"location":"resources/pre_meeting_notes/#7-closing-remarks","title":"7. Closing Remarks","text":"<ul> <li>Summarize the discussions and confirm the next steps.</li> <li>Confirm dates and times for primary meetings.</li> <li>Express appreciation for participation.</li> </ul>"},{"location":"resources/pre_meeting_notes/#action-items","title":"Action Items","text":"<ul> <li> Action item 1: Responsible person(s) - Deadline</li> <li> Action item 2: Responsible person(s) - Deadline</li> <li>...</li> </ul>"},{"location":"resources/pre_meeting_notes/#notes","title":"Notes","text":"<ul> <li>(Any additional notes or comments about the meeting.)</li> </ul>"},{"location":"resources/second_meeting_notes/","title":"Primary Meeting Day 6-10: Progress and Development","text":""},{"location":"resources/second_meeting_notes/#meeting-details","title":"Meeting Details","text":"<ul> <li>Dates:</li> <li>Times:</li> <li>Location:</li> <li>Facilitator:</li> </ul>"},{"location":"resources/second_meeting_notes/#attendees","title":"Attendees","text":"<ul> <li>List of attendees</li> </ul>"},{"location":"resources/second_meeting_notes/#daily-agenda","title":"Daily Agenda","text":""},{"location":"resources/second_meeting_notes/#day-6-review-and-refine","title":"Day 6: Review and Refine","text":""},{"location":"resources/second_meeting_notes/#recap-of-previous-sessions","title":"Recap of Previous Sessions","text":"<ul> <li>Summary of progress made since the last meeting.</li> <li>Review of action items and milestones achieved.</li> </ul>"},{"location":"resources/second_meeting_notes/#refinement-of-goals-and-tasks","title":"Refinement of Goals and Tasks","text":"<ul> <li>Reassessment and adjustment of goals based on current progress.</li> <li>Identification of any new challenges or opportunities.</li> </ul>"},{"location":"resources/second_meeting_notes/#day-7-9-in-depth-work-sessions","title":"Day 7-9: In-Depth Work Sessions","text":""},{"location":"resources/second_meeting_notes/#daily-goals","title":"Daily Goals","text":"<ul> <li>Clear objectives for each day\u2019s work sessions.</li> </ul>"},{"location":"resources/second_meeting_notes/#task-progress-updates","title":"Task Progress Updates","text":"<ul> <li>Brief reports from team members on their assigned tasks.</li> <li>Collaborative problem-solving for any issues encountered.</li> </ul>"},{"location":"resources/second_meeting_notes/#theory-and-data-integration","title":"Theory and Data Integration","text":"<ul> <li>Continued discussions on aligning theoretical frameworks with data analysis.</li> <li>Workshops or breakout sessions for detailed aspects of the project.</li> </ul>"},{"location":"resources/second_meeting_notes/#evening-collaborative-activities","title":"Evening Collaborative Activities","text":"<ul> <li>Informal sessions to encourage ongoing dialogue and collaboration.</li> </ul>"},{"location":"resources/second_meeting_notes/#day-10-mid-point-review","title":"Day 10: Mid-Point Review","text":""},{"location":"resources/second_meeting_notes/#progress-evaluation","title":"Progress Evaluation","text":"<ul> <li>Assessment of the work done during the week.</li> <li>Feedback sessions to ensure quality and consistency in outputs.</li> </ul>"},{"location":"resources/second_meeting_notes/#documentation-and-record-keeping","title":"Documentation and Record-Keeping","text":"<ul> <li>Ensure thorough documentation of methods, results, and decisions.</li> <li>Establish a system for organizing and sharing this documentation.</li> </ul>"},{"location":"resources/second_meeting_notes/#planning-forward","title":"Planning Forward","text":"<ul> <li>Setting objectives for the next phase of the project.</li> <li>Adjusting the roadmap as necessary based on insights from the week\u2019s work.</li> </ul>"},{"location":"resources/second_meeting_notes/#detailed-notes","title":"Detailed Notes","text":""},{"location":"resources/second_meeting_notes/#day-6-notes","title":"Day 6 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#day-7-notes","title":"Day 7 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#day-8-notes","title":"Day 8 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#day-9-notes","title":"Day 9 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#day-10-notes","title":"Day 10 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#action-items","title":"Action Items","text":"<ul> <li> Specific task: Assigned to - Deadline</li> <li> Specific task: Assigned to - Deadline</li> <li>...</li> </ul>"},{"location":"resources/second_meeting_notes/#reflections-and-comments","title":"Reflections and Comments","text":"<ul> <li>(Space for any additional thoughts, insights, or personal reflections on the meeting.)</li> </ul>"},{"location":"resources/third_meeting_notes/","title":"Primary Meeting Day 11-15: Finalization and Conclusion","text":""},{"location":"resources/third_meeting_notes/#meeting-details","title":"Meeting Details","text":"<ul> <li>Dates:</li> <li>Times:</li> <li>Location:</li> <li>Facilitator:</li> </ul>"},{"location":"resources/third_meeting_notes/#attendees","title":"Attendees","text":"<ul> <li>List of attendees</li> </ul>"},{"location":"resources/third_meeting_notes/#daily-agenda","title":"Daily Agenda","text":""},{"location":"resources/third_meeting_notes/#day-11-alignment-and-focus","title":"Day 11: Alignment and Focus","text":""},{"location":"resources/third_meeting_notes/#realigning-objectives","title":"Realigning Objectives","text":"<ul> <li>Review the project's main goals to ensure alignment with the final output.</li> <li>Address any misalignments or deviations from the original plan.</li> </ul>"},{"location":"resources/third_meeting_notes/#prioritization-of-tasks","title":"Prioritization of Tasks","text":"<ul> <li>Identify critical tasks that need to be completed.</li> <li>Allocate resources and efforts to ensure these priorities are met.</li> </ul>"},{"location":"resources/third_meeting_notes/#day-12-14-intensive-work-period","title":"Day 12-14: Intensive Work Period","text":""},{"location":"resources/third_meeting_notes/#task-completion","title":"Task Completion","text":"<ul> <li>Dedicated time for team members to complete their individual contributions.</li> <li>Regular check-ins to track progress and address any blockers.</li> </ul>"},{"location":"resources/third_meeting_notes/#integration-of-work","title":"Integration of Work","text":"<ul> <li>Begin to combine individual contributions into a cohesive whole.</li> <li>Review the integration to ensure consistency and coherency across the project.</li> </ul>"},{"location":"resources/third_meeting_notes/#final-reviews-and-edits","title":"Final Reviews and Edits","text":"<ul> <li>Conduct thorough reviews of the project's outputs.</li> <li>Perform final edits to refine the quality of the work.</li> </ul>"},{"location":"resources/third_meeting_notes/#day-15-closure-and-celebration","title":"Day 15: Closure and Celebration","text":""},{"location":"resources/third_meeting_notes/#final-presentation","title":"Final Presentation","text":"<ul> <li>Present the completed project to the group.</li> <li>Discuss any last-minute adjustments or refinements needed.</li> </ul>"},{"location":"resources/third_meeting_notes/#reflective-session","title":"Reflective Session","text":"<ul> <li>Reflect on the achievements and learnings from the project.</li> <li>Share appreciation for the team's hard work and dedication.</li> </ul>"},{"location":"resources/third_meeting_notes/#celebration","title":"Celebration","text":"<ul> <li>Acknowledge the successful completion of the project.</li> <li>Plan for any dissemination of the project's findings or outputs.</li> </ul>"},{"location":"resources/third_meeting_notes/#detailed-notes","title":"Detailed Notes","text":""},{"location":"resources/third_meeting_notes/#day-11-notes","title":"Day 11 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#day-12-notes","title":"Day 12 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#day-13-notes","title":"Day 13 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#day-14-notes","title":"Day 14 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#day-15-notes","title":"Day 15 Notes","text":"<ul> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#action-items","title":"Action Items","text":"<ul> <li> Finalize manuscript for publication: Assigned to - Deadline</li> <li> Prepare data for repository submission: Assigned to - Deadline</li> <li> Organize project materials for archival: Assigned to - Deadline</li> <li>...</li> </ul>"},{"location":"resources/third_meeting_notes/#reflections-and-comments","title":"Reflections and Comments","text":"<ul> <li>(Space for any additional thoughts, insights, or personal reflections on the meeting and the project as a whole.)</li> </ul>"},{"location":"resources/third_meeting_notes/#next-steps","title":"Next Steps","text":"<ul> <li>Define the publication and dissemination plan.</li> <li>Outline any follow-up research or projects that have stemmed from this work.</li> </ul>"},{"location":"resources/third_meeting_notes/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>(Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)</li> </ul>"},{"location":"resources/visualizations/","title":"Visualization Strategy and Development Documentation","text":""},{"location":"resources/visualizations/#overview","title":"Overview","text":"<ul> <li>Brief overview of the visualization goals and their alignment with the overall project objectives.</li> </ul>"},{"location":"resources/visualizations/#visualization-strategy","title":"Visualization Strategy","text":""},{"location":"resources/visualizations/#identifying-key-messages","title":"Identifying Key Messages","text":"<ul> <li>Discuss main messages or insights to communicate through visualizations.</li> <li>Identify target audience and their specific needs.</li> </ul>"},{"location":"resources/visualizations/#selecting-appropriate-visualization-types","title":"Selecting Appropriate Visualization Types","text":"<ul> <li>Explore different types of visualizations (charts, graphs, 3D, interactive elements) suitable for the data and message.</li> <li>Brainstorm creative visualization approaches.</li> </ul>"},{"location":"resources/visualizations/#visualization-development","title":"Visualization Development","text":""},{"location":"resources/visualizations/#code-generated-visualizations","title":"Code-Generated Visualizations","text":"<ul> <li>Outline initial visualizations generated from the data pipeline.</li> <li>Include code snippets and explanations.</li> </ul> <pre><code># Example Python code for a basic plot\nimport matplotlib.pyplot as plt\nplt.plot(data['x'], data['y'])\nplt.show()\n</code></pre>"},{"location":"resources/visualizations/#enhancing-visualizations","title":"Enhancing Visualizations","text":"<ul> <li>Steps for annotating, animating, creating 3D, immersive, or interactive visualizations.</li> <li>Discuss challenges and solutions in enhancing visuals.</li> </ul>"},{"location":"resources/visualizations/#versioning-and-iterations","title":"Versioning and Iterations","text":"<ul> <li>Document different versions and iterations of visualizations.</li> <li>Reflect on improvements or changes in each version.</li> </ul>"},{"location":"resources/visualizations/#finalizing-visualizations","title":"Finalizing Visualizations","text":"<ul> <li>Process of finalizing visuals for presentation or publication.</li> <li>Feedback incorporation from team or test audiences.</li> </ul>"},{"location":"resources/visualizations/#documentation-of-tools-and-resources","title":"Documentation of Tools and Resources","text":"<ul> <li>List software, libraries, and tools used for visualization.</li> <li>Reference external resources or tutorials.</li> </ul>"},{"location":"resources/visualizations/#conclusions","title":"Conclusions","text":"<ul> <li>Summarize the visualization process and contributions to the project.</li> <li>Reflect on lessons learned and potential future improvements.</li> </ul>"},{"location":"resources/visualizations/#references","title":"References","text":"<ul> <li>Cite external sources, inspirations, or frameworks used in visualization.</li> </ul>"},{"location":"resources/working_groups_and_postdocs/","title":"ESIIL Postdoctoral Researcher Responsibilities and Opportunities","text":""},{"location":"resources/working_groups_and_postdocs/#primary-responsibilities","title":"Primary Responsibilities","text":"<ul> <li>Independent Research:</li> <li>Conducting self-proposed research projects.</li> <li>Adhering to open data principles.</li> <li>Data and Code Storage:</li> <li>Storing all research code and data in the designated ESIIL repository.</li> <li>Use of CyVerse:</li> <li>Utilizing CyVerse as the primary computational platform.</li> </ul>"},{"location":"resources/working_groups_and_postdocs/#opportunities-for-collaboration","title":"Opportunities for Collaboration","text":"<ul> <li>Joining Working Groups:</li> <li>Opportunity to collaborate with working groups within ESIIL, subject to invitation.</li> <li>Networking and Collaboration:</li> <li>Engaging in regular meetings and seminars for networking.</li> </ul>"},{"location":"resources/working_groups_and_postdocs/#additional-responsibilities","title":"Additional Responsibilities","text":"<ul> <li>Reviewing Working Group Applications:</li> <li>Assisting in the review process of working group applications.</li> <li>Supporting Working Groups:</li> <li>Providing support to working groups in various capacities, even if not an author.</li> </ul>"},{"location":"resources/working_groups_and_postdocs/#note","title":"Note","text":"<ul> <li>Primary research commitments should be prioritized unless otherwise directed by supervisors or ESIIL's administrative body.</li> </ul> <p>This framework ensures that ESIIL postdocs balance independent research with collaborative opportunities, adhering to open data principles, and utilizing designated platforms for their work.</p>"},{"location":"tags/","title":"Tags","text":"<ul> <li>analytics</li> <li>Bayesian</li> <li>biomass</li> <li>burn-severity</li> <li>climate</li> <li>cloud</li> <li>collaboration</li> <li>container</li> <li>contribution</li> <li>data library</li> <li>data-cubes</li> <li>data-management</li> <li>development</li> <li>disturbance</li> <li>docker</li> <li>drought</li> <li>ecoregions</li> <li>education</li> <li>epa</li> <li>example</li> <li>featured</li> <li>fia</li> <li>fire</li> <li>forest</li> <li>gdal</li> <li>gedi</li> <li>GLMM</li> <li>INLA</li> <li>inventory</li> <li>landcover</li> <li>landfire</li> <li>LGM</li> <li>lidar</li> <li>modis</li> <li>oasis</li> <li>quickstart</li> <li>R</li> <li>raster</li> <li>remote-sensing</li> <li>schedule</li> <li>sentinel-2</li> <li>spatial</li> <li>spatiotemporal</li> <li>stac</li> <li>streaming</li> <li>treemap</li> <li>tutorial</li> <li>usage</li> <li>usgs</li> <li>vegetation</li> <li>vsi</li> </ul>"},{"location":"tags/#crt-triangle","title":"CRT triangle","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> </ul>"},{"location":"tags/#gdal","title":"GDAL","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#stac","title":"STAC","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#vsi","title":"VSI","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#cloud","title":"cloud","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#collaboration","title":"collaboration","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#container","title":"container","text":"<ul> <li>Example Container</li> </ul>"},{"location":"tags/#cyverse","title":"cyverse","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> </ul>"},{"location":"tags/#data-library","title":"data library","text":"<ul> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#development","title":"development","text":"<ul> <li>Development Requests</li> <li>Development Schedule</li> <li>Pending Development Tasks</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Starting with OASIS</li> </ul>"},{"location":"tags/#education","title":"education","text":"<ul> <li>Advanced Textbook</li> </ul>"},{"location":"tags/#example","title":"example","text":"<ul> <li>Example Container</li> </ul>"},{"location":"tags/#gdal_1","title":"gdal","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#oasis","title":"oasis","text":"<ul> <li>Starting with OASIS</li> </ul>"},{"location":"tags/#quickstart","title":"quickstart","text":"<ul> <li>Quick Start</li> <li>Cloud Reproducibility Triangle (CRT)</li> <li>Starting with OASIS</li> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#raster","title":"raster","text":"<ul> <li>Example Container</li> <li>Advanced Textbook</li> </ul>"},{"location":"tags/#requests","title":"requests","text":"<ul> <li>Development Requests</li> </ul>"},{"location":"tags/#schedule","title":"schedule","text":"<ul> <li>Development Schedule</li> </ul>"},{"location":"tags/#stac_1","title":"stac","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#streaming","title":"streaming","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#usage","title":"usage","text":"<ul> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#vsi_1","title":"vsi","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/Bayesian/","title":"Bayesian","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/GLMM/","title":"GLMM","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/INLA/","title":"INLA","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/LGM/","title":"LGM","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/R/","title":"R","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/analytics/","title":"analytics","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> <li>example-workflow </li> </ul>"},{"location":"tags/biomass/","title":"biomass","text":"<ul> <li>gedi </li> </ul>"},{"location":"tags/burn-severity/","title":"burn-severity","text":"<ul> <li>fire-cbi </li> </ul>"},{"location":"tags/climate/","title":"climate","text":"<ul> <li>drought </li> </ul>"},{"location":"tags/cloud/","title":"cloud","text":"<ul> <li>mounting-via-vsi </li> <li>Introduction to the Cloud Triangle 2024-01-01</li> </ul>"},{"location":"tags/collaboration/","title":"collaboration","text":""},{"location":"tags/container/","title":"container","text":"<ul> <li>example-container </li> </ul>"},{"location":"tags/contribution/","title":"contribution","text":"<ul> <li>how-to-contribute </li> </ul>"},{"location":"tags/data-cubes/","title":"data-cubes","text":"<ul> <li>stac_mount_save </li> </ul>"},{"location":"tags/data-library/","title":"data library","text":"<ul> <li>how-to-use </li> <li>how-to-contribute </li> </ul>"},{"location":"tags/data-management/","title":"data-management","text":"<ul> <li>move-data-to-instance </li> </ul>"},{"location":"tags/development/","title":"development","text":"<ul> <li>dev-schedule 2025-08-14</li> </ul>"},{"location":"tags/disturbance/","title":"disturbance","text":"<ul> <li>landfire-events </li> <li>disturbance-stack </li> </ul>"},{"location":"tags/docker/","title":"docker","text":"<ul> <li>Starting with OASIS 2024-01-01</li> </ul>"},{"location":"tags/drought/","title":"drought","text":"<ul> <li>drought </li> <li>disturbance-stack </li> </ul>"},{"location":"tags/ecoregions/","title":"ecoregions","text":"<ul> <li>epa-ecoregions </li> </ul>"},{"location":"tags/education/","title":"education","text":"<ul> <li>advanced-textbook </li> </ul>"},{"location":"tags/epa/","title":"epa","text":"<ul> <li>epa-ecoregions </li> </ul>"},{"location":"tags/example/","title":"example","text":"<ul> <li>example-workflow </li> <li>example-container </li> </ul>"},{"location":"tags/featured/","title":"featured","text":"<ul> <li>Pull_Sentinal2_l2_data </li> </ul>"},{"location":"tags/fia/","title":"fia","text":"<ul> <li>fia </li> </ul>"},{"location":"tags/fire/","title":"fire","text":"<ul> <li>landfire-events </li> <li>fire-cbi </li> </ul>"},{"location":"tags/forest/","title":"forest","text":"<ul> <li>fia </li> <li>treemap </li> </ul>"},{"location":"tags/gdal/","title":"gdal","text":"<ul> <li>mounting-via-vsi </li> </ul>"},{"location":"tags/gedi/","title":"gedi","text":"<ul> <li>gedi </li> </ul>"},{"location":"tags/inventory/","title":"inventory","text":"<ul> <li>fia </li> </ul>"},{"location":"tags/landcover/","title":"landcover","text":"<ul> <li>lcmap </li> </ul>"},{"location":"tags/landfire/","title":"landfire","text":"<ul> <li>landfire-events </li> <li>disturbance-stack </li> </ul>"},{"location":"tags/lidar/","title":"lidar","text":"<ul> <li>gedi </li> </ul>"},{"location":"tags/modis/","title":"modis","text":"<ul> <li>modis-vcf </li> </ul>"},{"location":"tags/oasis/","title":"oasis","text":"<ul> <li>Starting with OASIS 2024-01-01</li> </ul>"},{"location":"tags/quickstart/","title":"quickstart","text":"<ul> <li>Starting with OASIS 2024-01-01</li> <li>how-to-use </li> <li>how-to-contribute </li> <li>Introduction to the Cloud Triangle 2024-01-01</li> </ul>"},{"location":"tags/raster/","title":"raster","text":"<ul> <li>example-workflow </li> <li>lcmap </li> <li>example-container </li> <li>advanced-textbook </li> </ul>"},{"location":"tags/remote-sensing/","title":"remote-sensing","text":"<ul> <li>lcmap </li> <li>Pull_Sentinal2_l2_data </li> <li>modis-vcf </li> <li>treemap </li> </ul>"},{"location":"tags/schedule/","title":"schedule","text":"<ul> <li>dev-schedule 2025-08-14</li> </ul>"},{"location":"tags/sentinel-2/","title":"sentinel-2","text":"<ul> <li>Pull_Sentinal2_l2_data </li> </ul>"},{"location":"tags/spatial/","title":"spatial","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/spatiotemporal/","title":"spatiotemporal","text":"<ul> <li>INLA \u2014 Drop-in Analytics Module </li> </ul>"},{"location":"tags/stac/","title":"stac","text":"<ul> <li>stac_mount_save </li> <li>stac_simple </li> </ul>"},{"location":"tags/streaming/","title":"streaming","text":""},{"location":"tags/treemap/","title":"treemap","text":"<ul> <li>treemap </li> </ul>"},{"location":"tags/tutorial/","title":"tutorial","text":"<ul> <li>mounting-via-vsi </li> <li>stac_mount_save </li> <li>move-data-to-instance </li> <li>stac_simple </li> </ul>"},{"location":"tags/usage/","title":"usage","text":"<ul> <li>how-to-use </li> </ul>"},{"location":"tags/usgs/","title":"usgs","text":"<ul> <li>lcmap </li> </ul>"},{"location":"tags/vegetation/","title":"vegetation","text":"<ul> <li>modis-vcf </li> </ul>"},{"location":"tags/vsi/","title":"vsi","text":""},{"location":"trainings/training_2_code/","title":"Pre-summit training","text":""},{"location":"trainings/training_one/","title":"Markdown for the Modern Researcher at ESIIL","text":""},{"location":"trainings/training_one/#introduction","title":"Introduction","text":"<ul> <li>Overview of Markdown's relevance and utility in modern research.</li> <li>How Markdown streamlines documentation in diverse scientific and coding environments.</li> </ul>"},{"location":"trainings/training_one/#section-1-mastering-markdown-syntax","title":"Section 1: Mastering Markdown Syntax","text":"<ul> <li>Objective: Equip researchers with a thorough understanding of Markdown syntax and its diverse applications.</li> <li>Topics Covered:</li> <li>Fundamentals of Text Formatting (headings, lists, bold, italics)</li> <li>Advanced Structures (tables, blockquotes)</li> <li>Integrating Multimedia (image and video links)</li> <li>Diagrams with Mermaid (creating flowcharts, mind maps, timelines)</li> <li>Interactive Elements (hyperlinks, embedding interactive content)</li> <li>Activities:</li> <li>Crafting a Markdown document with various formatting elements.</li> <li>Developing diagrams using Mermaid for research presentations.</li> <li>Embedding multimedia elements in a Markdown document for enhanced communication.</li> </ul>"},{"location":"trainings/training_one/#section-2-markdown-in-research-tools","title":"Section 2: Markdown in Research Tools","text":"<ul> <li>Objective: Showcase the integration of Markdown in RStudio and Jupyter Notebooks for scientific documentation.</li> <li>Topics Covered:</li> <li>Implementing Markdown in RStudio (R Markdown, knitting to HTML/PDF)</li> <li>Utilizing Markdown in Jupyter Notebooks (code and Markdown cells)</li> <li>Best practices for documenting research code</li> <li>Including code outputs and visualizations in documentation</li> <li>Activities:</li> <li>Creating and sharing an R Markdown document with annotated research data.</li> <li>Building a comprehensive Jupyter Notebook with integrated Markdown annotations.</li> </ul>"},{"location":"trainings/training_one/#section-3-disseminating-research-with-markdown-and-github-pages","title":"Section 3: Disseminating Research with Markdown and GitHub Pages","text":"<ul> <li>Objective: Teach researchers how to publish and manage Markdown-based documentation as web pages.</li> <li>Topics Covered:</li> <li>Setting up a GitHub repository for hosting documentation</li> <li>Transforming Markdown files into web-friendly formats</li> <li>Customizing web page layouts and themes</li> <li>Advanced features using Jekyll</li> <li>Version control and content management for documentation</li> <li>Activities:</li> <li>Publishing a research project documentation on GitHub Pages.</li> <li>Applying custom themes and layouts to enhance online documentation.</li> </ul>"},{"location":"trainings/training_one/#conclusion","title":"Conclusion","text":"<ul> <li>Review of Markdown's role in enhancing research efficiency and clarity.</li> <li>Encouraging the integration of Markdown into daily research activities for improved documentation and dissemination.</li> </ul>"},{"location":"trainings/training_one/#additional-resources","title":"Additional Resources","text":"<ul> <li>Curated list of advanced Markdown tutorials, guides for GitHub Pages, and Jekyll resources for researchers.</li> </ul>"},{"location":"trainings/training_one/#section-1-mastering-markdown-syntax_1","title":"Section 1: Mastering Markdown Syntax","text":""},{"location":"trainings/training_one/#1-fundamentals-of-text-formatting","title":"1. Fundamentals of Text Formatting","text":"<ul> <li>Headings: Use <code>#</code> for different levels of headings.</li> <li> </li> <li> </li> <li> </li> <li> <p>Lists: Bulleted lists use asterisks, numbers for ordered lists.</p> </li> <li>Item 1</li> <li>Item 2<ul> <li>Subitem 2.1</li> <li>Subitem 2.2</li> </ul> </li> <li> <ol> <li>First item</li> </ol> </li> <li> <ol> <li>Second item</li> </ol> </li> <li> <p>Bold and Italics: Use asterisks or underscores.</p> </li> <li>Bold Text</li> <li>Italic Text</li> </ul>"},{"location":"trainings/training_one/#heading-level-1","title":"Heading Level 1","text":""},{"location":"trainings/training_one/#heading-level-2","title":"Heading Level 2","text":""},{"location":"trainings/training_one/#heading-level-3","title":"Heading Level 3","text":""},{"location":"trainings/training_one/#2-advanced-structures","title":"2. Advanced Structures","text":"<ul> <li>Tables: Create tables using dashes and pipes.</li> <li> Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data </li> <li> <p>Add a \":\"\" to change text justification. Here the : is added on the left for left justification.     | Header 1 | Header 2 | Header 3 |     |---------:|--------- |----------|     | Row 1    | Data     | Data     |     | Row 2    | Data     | Data     |</p> </li> <li> A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C </li> <li> <p>If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. </p> </li> </ul> <p>Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition.</p> 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 534678912 672195348 198342567 859761423 426853791 713924856 961537284 287419635 345286179 <ul> <li>Blockquotes: Use <code>&gt;</code> for blockquotes.</li> <li> <p>This is a blockquote.</p> </li> <li> <p>It can span multiple lines.</p> </li> </ul>"},{"location":"trainings/training_one/#3-integrating-multimedia","title":"3. Integrating Multimedia","text":"<ul> <li>Images: Add images using the format <code>![alt text](image_url)</code>.</li> <li> </li> <li> <p>Videos: Embed videos using HTML in Markdown.</p> </li> <li><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"trainings/training_one/#4-diagrams-with-mermaid","title":"4. Diagrams with Mermaid","text":"<ul> <li>Flowcharts:</li> </ul> <pre><code>    graph TD\n    A[Start] --&gt; B[Analyze Data]\n    B --&gt; C{Is Data Large?}\n    C --&gt;|Yes| D[Apply Big Data Solutions]\n    C --&gt;|No| E[Use Traditional Methods]\n    D --&gt; F[Machine Learning]\n    E --&gt; G[Statistical Analysis]\n    F --&gt; H{Model Accurate?}\n    G --&gt; I[Report Results]\n    H --&gt;|Yes| J[Deploy Model]\n    H --&gt;|No| K[Refine Model]\n    J --&gt; L[Monitor Performance]\n    K --&gt; F\n    L --&gt; M[End: Success]\n    I --&gt; N[End: Report Generated]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style M fill:#9f9,stroke:#333,stroke-width:2px\n    style N fill:#9f9,stroke:#333,stroke-width:2px</code></pre> <ul> <li> <p>Mind Maps: <pre><code>    mindmap\n  root((ESIIL))\n    section Data Sources\n      Satellite Imagery\n        ::icon(fa fa-satellite)\n      Remote Sensing Data\n        Drones\n        Aircraft\n      On-ground Sensors\n        Weather Stations\n        IoT Devices\n      Open Environmental Data\n        Public Datasets\n        ::icon(fa fa-database)\n    section Research Focus\n      Climate Change Analysis\n        Ice Melt Patterns\n        Sea Level Rise\n      Biodiversity Monitoring\n        Species Distribution\n        Habitat Fragmentation\n      Geospatial Analysis Techniques\n        Machine Learning Models\n        Predictive Analytics\n    section Applications\n      Conservation Strategies\n        ::icon(fa fa-leaf)\n      Urban Planning\n        Green Spaces\n      Disaster Response\n        Flood Mapping\n        Wildfire Tracking\n    section Tools and Technologies\n      GIS Software\n        QGIS\n        ArcGIS\n      Programming Languages\n        Python\n        R\n      Cloud Computing Platforms\n        AWS\n        Google Earth Engine\n      Data Visualization\n        D3.js\n        Tableau</code></pre></p> </li> <li> <p>Timelines:</p> </li> </ul> <pre><code>gantt\n    title ESIIL Year 2 Project Schedule\n    dateFormat  YYYY-MM-DD\n    section CI\n    Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30\n    OASIS documentation                    :2024-09-15, 70d\n    Data cube OASIS via cyverse account    :2024-09-15, 100d\n    Integrate with ESIIL User Management system :2024-08-01, 2024-11-30\n    Build badges to deploy DE from mkdoc   :2024-09-01, 2024-12-15\n    Streamline Github ssh key management   :2024-10-01, 2024-12-31\n    Cyverse support (R proxy link)         :2024-11-01, 2024-12-31\n    Cyverse use summary and statistics     :2024-08-01, 2024-12-15\n\n    section CI Consultation and Education\n    Conferences/Invited talks              :2024-08-01, 2024-12-31\n    Office hours                           :2024-08-15, 2024-12-15\n    Proposals                              :2024-09-01, 2024-11-15\n    Private lessons                        :2024-09-15, 2024-11-30\n    Pre-event trainings                    :2024-10-01, 2024-12-15\n    Textbook development w/ education team :2024-08-01, 2024-12-15\n    Train the trainers / group lessons     :2024-08-15, 2024-11-30\n    Tribal engagement                      :2024-09-01, 2024-12-15\n    Ethical Space training                 :2024-09-15, 2024-12-31\n\n    section CI Design and Build\n    Data library (repository)              :2024-08-01, 2024-10-30\n    Analytics library (repository)         :2024-08-15, 2024-11-15\n    Containers (repository)                :2024-09-01, 2024-11-30\n    Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15\n    Tribal resilience Data Cube            :2024-10-01, 2024-12-31</code></pre> <pre><code>\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%%\ngitGraph\n  commit id: \"Start from template\"\n  branch c1\n  commit id: \"Set up SSH key pair\"\n  commit id: \"Modify _config.yml for GitHub Pages\"\n  commit id: \"Initial website structure\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\n  commit id: \"Edit existing pages\"\n  commit id: \"Delete old markdown pages\"\n  commit id: \"Finalize website updates\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\ncheckout c1\n\n  branch b1\n\n  commit\n  commit\n  checkout c1\n  merge b1</code></pre> <pre><code>%%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%%\nquadrantChart\n  x-axis Urgent --&gt; Not Urgent\n  y-axis Not Important --&gt; \"Important \u2764\"\n  quadrant-1 Plan\n  quadrant-2 Do\n  quadrant-3 Delegate\n  quadrant-4 Delete</code></pre> <pre><code>timeline\n    title Major Events in Environmental Science and Data Science\n    section Environmental Science\n        19th century : Foundations in Ecology and Conservation\n        1962 : Publication of 'Silent Spring' by Rachel Carson\n        1970 : First Earth Day\n        1987 : Brundtland Report introduces Sustainable Development\n        1992 : Rio Earth Summit\n        2015 : Paris Agreement on Climate Change\n    section Data Science\n        1960s-1970s : Development of Database Management Systems\n        1980s : Emergence of Data Warehousing\n        1990s : Growth of the World Wide Web and Data Mining\n        2000s : Big Data and Predictive Analytics\n        2010s : AI and Machine Learning Revolution\n        2020s : Integration of AI in Environmental Research</code></pre> <pre><code>erDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }</code></pre> <pre><code>---\nconfig:\n  sankey:\n    showValues: false\n---\nsankey-beta\n\nNASA Data,Big Data Harmonization,100\n    Satellite Imagery,Big Data Harmonization,80\n    Open Environmental Data,Big Data Harmonization,70\n    Remote Sensing Data,Big Data Harmonization,90\n    Big Data Harmonization, Data Analysis and Integration,340\n    Data Analysis and Integration,Climate Change Research,100\n    Data Analysis and Integration,Biodiversity Monitoring,80\n    Data Analysis and Integration,Geospatial Mapping,60\n    Data Analysis and Integration,Urban Planning,50\n    Data Analysis and Integration,Disaster Response,50</code></pre>"},{"location":"trainings/training_one/#5-interactive-elements","title":"5. Interactive Elements","text":"<ul> <li>Hyperlinks: Use the format <code>[link text](URL)</code>.</li> <li>Google</li> <li> <p>Play Tetris</p> </li> <li> <p>Embedding Interactive Content: Use HTML tags or specific platform embed codes.</p> </li> <li><code>&lt;iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"trainings/training_one/#6-math-notation","title":"6. Math Notation","text":"<p>Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX).</p> <ul> <li>Inline Math: Use single dollar signs for inline math expressions. Representing the normal distribution.</li> </ul> <p>Example: The probability density function of the normal distribution is given by \\(f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\).`</p> <ul> <li>Display Math: Use double dollar signs for standalone equations.</li> </ul> <p>Example:   $$   f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e<sup>{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)</sup>2}   $$</p> <ul> <li>Common LaTeX Elements for Environmental Data Science:</li> <li>Statistical Distributions:<ul> <li>Normal Distribution: <code>\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}</code> for \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)</li> <li>Poisson Distribution: <code>P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</code> for \\(P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)</li> </ul> </li> <li>Coordinate Systems:<ul> <li>Spherical Coordinates: <code>(r, \\theta, \\phi)</code> for \\((r, \\theta, \\phi)\\)</li> <li>Cartesian Coordinates: <code>(x, y, z)</code> for \\((x, y, z)\\)</li> </ul> </li> <li>Geospatial Equations:<ul> <li>Haversine Formula for Distance: <code>a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)</code> for \\(a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)\\)</li> </ul> </li> </ul> <p>Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities.</p>"},{"location":"trainings/training_one/#7-effective-citations-in-markdown","title":"7. Effective Citations in Markdown","text":""},{"location":"trainings/training_one/#inline-citations","title":"Inline Citations","text":"<ul> <li>Objective: Learn how to use inline citations in Markdown.</li> <li>Example Usage:</li> <li>Inline citation of a single work: <ul> <li>Some text with an inline citation. [@jones:envstudy:2020]</li> </ul> </li> <li>Inline citation with specific page or section: <ul> <li>More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2]</li> </ul> </li> <li>Contrasting views: <ul> <li>Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78]</li> </ul> </li> </ul>"},{"location":"trainings/training_one/#footnote-citations","title":"Footnote Citations","text":"<ul> <li>Objective: Understand how to use footnote citations in Markdown.</li> <li>Example Usage:</li> <li>Citing with a footnote: <ul> <li>Some statement in the text.<sup>1</sup></li> </ul> </li> <li>Multiple references to the same footnote: <ul> <li>Another statement referring to the same source.<sup>1</sup></li> </ul> </li> <li>A different citation: <ul> <li>Additional comment with a new citation.<sup>2</sup></li> </ul> </li> </ul>"},{"location":"trainings/training_one/#creating-footnotes","title":"Creating Footnotes","text":"<ul> <li>Example Syntax:</li> <li></li> <li></li> </ul> <ol> <li> <p>First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"trainings/training_three/","title":"Training three","text":"<p>Team Science</p> <p>Community Skills</p> <ul> <li> <p>ESIIL Behavior Expectations Poster</p> </li> <li> <p>Calling People In Handout</p> </li> <li> <p>Tool: Interrupting Microagressions</p> </li> <li> <p>How to Apologize</p> </li> <li> <p>Voices in Concert Jamboard</p> </li> </ul> <p>Cultural Intelligence</p>"},{"location":"trainings/training_two/","title":"Markdown for the Modern Researcher at ESIIL","text":"Definitions <p>\"Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly  available,  accessible  and  reusable  for  everyone,  to  increase  scientific  collaborations  and  sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community.\" - UNESCO Definition</p> <ul> <li>UNESCO's Recommendation on Open Science</li> </ul> <p>\"Open Science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional...\"   Wikipedia definition</p> <p>Open and Collaborative Science Network's Open Science Manifesto</p> Six Pillars  of Open Science <p> Open Access Publications</p> <p> Open Data</p> <p> Open Educational Resources</p> <p> Open Methodology</p> <p> Open Peer Review</p> <p> Open Source Software</p> Wait, how many pillars  of Open Science Really Are There? <p>The number can be from 4  to 8 </p> Foster Open Science Diagram <p> </p> <p>Graphic by Foster Open Science</p>"},{"location":"trainings/training_two/#introduction","title":"Introduction","text":"<ul> <li>Overview of Markdown's relevance and utility in modern research.</li> <li>How Markdown streamlines documentation in diverse scientific and coding environments.</li> </ul>"},{"location":"trainings/training_two/#section-1-mastering-markdown-syntax","title":"Section 1: Mastering Markdown Syntax","text":"<ul> <li>Objective: Equip researchers with a thorough understanding of Markdown syntax and its diverse applications.</li> <li>Topics Covered:</li> <li>Fundamentals of Text Formatting (headings, lists, bold, italics)</li> <li>Advanced Structures (tables, blockquotes)</li> <li>Integrating Multimedia (image and video links)</li> <li>Diagrams with Mermaid (creating flowcharts, mind maps, timelines)</li> <li>Interactive Elements (hyperlinks, embedding interactive content)</li> <li>Activities:</li> <li>Crafting a Markdown document with various formatting elements.</li> <li>Developing diagrams using Mermaid for research presentations.</li> <li>Embedding multimedia elements in a Markdown document for enhanced communication.</li> </ul>"},{"location":"trainings/training_two/#section-2-markdown-in-research-tools","title":"Section 2: Markdown in Research Tools","text":"<ul> <li>Objective: Showcase the integration of Markdown in RStudio and Jupyter Notebooks for scientific documentation.</li> <li>Topics Covered:</li> <li>Implementing Markdown in RStudio (R Markdown, knitting to HTML/PDF)</li> <li>Utilizing Markdown in Jupyter Notebooks (code and Markdown cells)</li> <li>Best practices for documenting research code</li> <li>Including code outputs and visualizations in documentation</li> <li>Activities:</li> <li>Creating and sharing an R Markdown document with annotated research data.</li> <li>Building a comprehensive Jupyter Notebook with integrated Markdown annotations.</li> </ul>"},{"location":"trainings/training_two/#section-3-disseminating-research-with-markdown-and-github-pages","title":"Section 3: Disseminating Research with Markdown and GitHub Pages","text":"<ul> <li>Objective: Teach researchers how to publish and manage Markdown-based documentation as web pages.</li> <li>Topics Covered:</li> <li>Setting up a GitHub repository for hosting documentation</li> <li>Transforming Markdown files into web-friendly formats</li> <li>Customizing web page layouts and themes</li> <li>Advanced features using Jekyll</li> <li>Version control and content management for documentation</li> <li>Activities:</li> <li>Publishing a research project documentation on GitHub Pages.</li> <li>Applying custom themes and layouts to enhance online documentation.</li> </ul>"},{"location":"trainings/training_two/#conclusion","title":"Conclusion","text":"<ul> <li>Review of Markdown's role in enhancing research efficiency and clarity.</li> <li>Encouraging the integration of Markdown into daily research activities for improved documentation and dissemination.</li> </ul>"},{"location":"trainings/training_two/#additional-resources","title":"Additional Resources","text":"<ul> <li>Curated list of advanced Markdown tutorials, guides for GitHub Pages, and Jekyll resources for researchers.</li> </ul>"},{"location":"trainings/training_two/#section-1-mastering-markdown-syntax_1","title":"Section 1: Mastering Markdown Syntax","text":""},{"location":"trainings/training_two/#1-fundamentals-of-text-formatting","title":"1. Fundamentals of Text Formatting","text":"<ul> <li>Headings: Use <code>#</code> for different levels of headings.</li> <li> </li> <li> </li> <li> </li> <li> <p>Lists: Bulleted lists use asterisks, numbers for ordered lists.</p> </li> <li>Item 1</li> <li>Item 2<ul> <li>Subitem 2.1</li> <li>Subitem 2.2</li> </ul> </li> <li> <ol> <li>First item</li> </ol> </li> <li> <ol> <li>Second item</li> </ol> </li> <li> <p>Bold and Italics: Use asterisks or underscores.</p> </li> <li>Bold Text</li> <li>Italic Text</li> </ul>"},{"location":"trainings/training_two/#heading-level-1","title":"Heading Level 1","text":""},{"location":"trainings/training_two/#heading-level-2","title":"Heading Level 2","text":""},{"location":"trainings/training_two/#heading-level-3","title":"Heading Level 3","text":""},{"location":"trainings/training_two/#2-advanced-structures","title":"2. Advanced Structures","text":"<ul> <li>Tables: Create tables using dashes and pipes.</li> <li> Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data </li> <li> <p>Add a \":\"\" to change text justification. Here the : is added on the left for left justification.     | Header 1 | Header 2 | Header 3 |     |---------:|--------- |----------|     | Row 1    | Data     | Data     |     | Row 2    | Data     | Data     |</p> </li> <li> A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C </li> <li> <p>If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. </p> </li> </ul> <p>Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition.</p> 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 534678912 672195348 198342567 859761423 426853791 713924856 961537284 287419635 345286179 <ul> <li>Blockquotes: Use <code>&gt;</code> for blockquotes.</li> <li> <p>This is a blockquote.</p> </li> <li> <p>It can span multiple lines.</p> </li> </ul>"},{"location":"trainings/training_two/#3-integrating-multimedia","title":"3. Integrating Multimedia","text":"<ul> <li>Images: Add images using the format <code>![alt text](image_url)</code>.</li> <li> </li> <li> <p>Videos: Embed videos using HTML in Markdown.</p> </li> <li><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"trainings/training_two/#4-diagrams-with-mermaid","title":"4. Diagrams with Mermaid","text":"<ul> <li>Flowcharts:</li> </ul> <pre><code>    graph TD\n    A[Start] --&gt; B[Analyze Data]\n    B --&gt; C{Is Data Large?}\n    C --&gt;|Yes| D[Apply Big Data Solutions]\n    C --&gt;|No| E[Use Traditional Methods]\n    D --&gt; F[Machine Learning]\n    E --&gt; G[Statistical Analysis]\n    F --&gt; H{Model Accurate?}\n    G --&gt; I[Report Results]\n    H --&gt;|Yes| J[Deploy Model]\n    H --&gt;|No| K[Refine Model]\n    J --&gt; L[Monitor Performance]\n    K --&gt; F\n    L --&gt; M[End: Success]\n    I --&gt; N[End: Report Generated]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style M fill:#9f9,stroke:#333,stroke-width:2px\n    style N fill:#9f9,stroke:#333,stroke-width:2px</code></pre> <ul> <li> <p>Mind Maps: <pre><code>    mindmap\n  root((ESIIL))\n    section Data Sources\n      Satellite Imagery\n        ::icon(fa fa-satellite)\n      Remote Sensing Data\n        Drones\n        Aircraft\n      On-ground Sensors\n        Weather Stations\n        IoT Devices\n      Open Environmental Data\n        Public Datasets\n        ::icon(fa fa-database)\n    section Research Focus\n      Climate Change Analysis\n        Ice Melt Patterns\n        Sea Level Rise\n      Biodiversity Monitoring\n        Species Distribution\n        Habitat Fragmentation\n      Geospatial Analysis Techniques\n        Machine Learning Models\n        Predictive Analytics\n    section Applications\n      Conservation Strategies\n        ::icon(fa fa-leaf)\n      Urban Planning\n        Green Spaces\n      Disaster Response\n        Flood Mapping\n        Wildfire Tracking\n    section Tools and Technologies\n      GIS Software\n        QGIS\n        ArcGIS\n      Programming Languages\n        Python\n        R\n      Cloud Computing Platforms\n        AWS\n        Google Earth Engine\n      Data Visualization\n        D3.js\n        Tableau</code></pre></p> </li> <li> <p>Timelines:</p> </li> </ul> <pre><code>gantt\n    title ESIIL Year 2 Project Schedule\n    dateFormat  YYYY-MM-DD\n    section CI\n    Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30\n    OASIS documentation                    :2024-09-15, 70d\n    Data cube OASIS via cyverse account    :2024-09-15, 100d\n    Integrate with ESIIL User Management system :2024-08-01, 2024-11-30\n    Build badges to deploy DE from mkdoc   :2024-09-01, 2024-12-15\n    Streamline Github ssh key management   :2024-10-01, 2024-12-31\n    Cyverse support (R proxy link)         :2024-11-01, 2024-12-31\n    Cyverse use summary and statistics     :2024-08-01, 2024-12-15\n\n    section CI Consultation and Education\n    Conferences/Invited talks              :2024-08-01, 2024-12-31\n    Office hours                           :2024-08-15, 2024-12-15\n    Proposals                              :2024-09-01, 2024-11-15\n    Private lessons                        :2024-09-15, 2024-11-30\n    Pre-event trainings                    :2024-10-01, 2024-12-15\n    Textbook development w/ education team :2024-08-01, 2024-12-15\n    Train the trainers / group lessons     :2024-08-15, 2024-11-30\n    Tribal engagement                      :2024-09-01, 2024-12-15\n    Ethical Space training                 :2024-09-15, 2024-12-31\n\n    section CI Design and Build\n    Data library (repository)              :2024-08-01, 2024-10-30\n    Analytics library (repository)         :2024-08-15, 2024-11-15\n    Containers (repository)                :2024-09-01, 2024-11-30\n    Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15\n    Tribal resilience Data Cube            :2024-10-01, 2024-12-31</code></pre> <pre><code>\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%%\ngitGraph\n  commit id: \"Start from template\"\n  branch c1\n  commit id: \"Set up SSH key pair\"\n  commit id: \"Modify _config.yml for GitHub Pages\"\n  commit id: \"Initial website structure\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\n  commit id: \"Edit existing pages\"\n  commit id: \"Delete old markdown pages\"\n  commit id: \"Finalize website updates\"\n  commit id: \"Add new markdown pages\"\n  commit id: \"Update navigation tree\"\ncheckout c1\n\n  branch b1\n\n  commit\n  commit\n  checkout c1\n  merge b1</code></pre> <pre><code>%%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%%\nquadrantChart\n  x-axis Urgent --&gt; Not Urgent\n  y-axis Not Important --&gt; \"Important \u2764\"\n  quadrant-1 Plan\n  quadrant-2 Do\n  quadrant-3 Delegate\n  quadrant-4 Delete</code></pre> <pre><code>timeline\n    title Major Events in Environmental Science and Data Science\n    section Environmental Science\n        19th century : Foundations in Ecology and Conservation\n        1962 : Publication of 'Silent Spring' by Rachel Carson\n        1970 : First Earth Day\n        1987 : Brundtland Report introduces Sustainable Development\n        1992 : Rio Earth Summit\n        2015 : Paris Agreement on Climate Change\n    section Data Science\n        1960s-1970s : Development of Database Management Systems\n        1980s : Emergence of Data Warehousing\n        1990s : Growth of the World Wide Web and Data Mining\n        2000s : Big Data and Predictive Analytics\n        2010s : AI and Machine Learning Revolution\n        2020s : Integration of AI in Environmental Research</code></pre> <pre><code>erDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }</code></pre> <pre><code>---\nconfig:\n  sankey:\n    showValues: false\n---\nsankey-beta\n\nNASA Data,Big Data Harmonization,100\n    Satellite Imagery,Big Data Harmonization,80\n    Open Environmental Data,Big Data Harmonization,70\n    Remote Sensing Data,Big Data Harmonization,90\n    Big Data Harmonization, Data Analysis and Integration,340\n    Data Analysis and Integration,Climate Change Research,100\n    Data Analysis and Integration,Biodiversity Monitoring,80\n    Data Analysis and Integration,Geospatial Mapping,60\n    Data Analysis and Integration,Urban Planning,50\n    Data Analysis and Integration,Disaster Response,50</code></pre>"},{"location":"trainings/training_two/#5-interactive-elements","title":"5. Interactive Elements","text":"<ul> <li>Hyperlinks: Use the format <code>[link text](URL)</code>.</li> <li>Google</li> <li> <p>Play Tetris</p> </li> <li> <p>Embedding Interactive Content: Use HTML tags or specific platform embed codes.</p> </li> <li><code>&lt;iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"&gt;&lt;/iframe&gt;</code></li> </ul>"},{"location":"trainings/training_two/#6-math-notation","title":"6. Math Notation","text":"<p>Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX).</p> <ul> <li>Inline Math: Use single dollar signs for inline math expressions. Representing the normal distribution.</li> </ul> <p>Example: The probability density function of the normal distribution is given by \\(f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\).`</p> <ul> <li>Display Math: Use double dollar signs for standalone equations.</li> </ul> <p>Example:   $$   f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e<sup>{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)</sup>2}   $$</p> <ul> <li>Common LaTeX Elements for Environmental Data Science:</li> <li>Statistical Distributions:<ul> <li>Normal Distribution: <code>\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}</code> for \\(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)</li> <li>Poisson Distribution: <code>P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}</code> for \\(P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)</li> </ul> </li> <li>Coordinate Systems:<ul> <li>Spherical Coordinates: <code>(r, \\theta, \\phi)</code> for \\((r, \\theta, \\phi)\\)</li> <li>Cartesian Coordinates: <code>(x, y, z)</code> for \\((x, y, z)\\)</li> </ul> </li> <li>Geospatial Equations:<ul> <li>Haversine Formula for Distance: <code>a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)</code> for \\(a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)\\)</li> </ul> </li> </ul> <p>Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities.</p>"},{"location":"trainings/training_two/#7-effective-citations-in-markdown","title":"7. Effective Citations in Markdown","text":""},{"location":"trainings/training_two/#inline-citations","title":"Inline Citations","text":"<ul> <li>Objective: Learn how to use inline citations in Markdown.</li> <li>Example Usage:</li> <li>Inline citation of a single work: <ul> <li>Some text with an inline citation. [@jones:envstudy:2020]</li> </ul> </li> <li>Inline citation with specific page or section: <ul> <li>More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2]</li> </ul> </li> <li>Contrasting views: <ul> <li>Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78]</li> </ul> </li> </ul>"},{"location":"trainings/training_two/#footnote-citations","title":"Footnote Citations","text":"<ul> <li>Objective: Understand how to use footnote citations in Markdown.</li> <li>Example Usage:</li> <li>Citing with a footnote: <ul> <li>Some statement in the text.<sup>1</sup></li> </ul> </li> <li>Multiple references to the same footnote: <ul> <li>Another statement referring to the same source.<sup>1</sup></li> </ul> </li> <li>A different citation: <ul> <li>Additional comment with a new citation.<sup>2</sup></li> </ul> </li> </ul>"},{"location":"trainings/training_two/#creating-footnotes","title":"Creating Footnotes","text":"<ul> <li>Example Syntax:</li> <li></li> <li></li> </ul> <ol> <li> <p>First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020.\u00a0\u21a9\u21a9</p> </li> <li> <p>Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"worksheets/worksheet_0/","title":"Exploring Resilience with Data in your Third Space (CyVerse)","text":""},{"location":"worksheets/worksheet_0/#instructions","title":"Instructions","text":"<p>Work through the prompts below with the people at your table. Please use a decision-making method to decide before moving to a new section of the activity.</p>"},{"location":"worksheets/worksheet_0/#introductions","title":"Introductions","text":"<p>Please share the following information with your team: - Name - Pronouns  - Where did you travel from? - Reflecting back on the polarities exercise, share one thing you observed about yourself. </p>"},{"location":"worksheets/worksheet_0/#objectives-of-this-group-activity","title":"Objectives of this group activity","text":"<ol> <li>Increase comfort with Cyverse</li> <li>Practice decision-making with a group</li> <li>Get to know other Summit participants</li> <li>Explore how historic policies continue to affect the spatial distribution of environmental amenities.</li> </ol>"},{"location":"worksheets/worksheet_0/#background","title":"Background","text":""},{"location":"worksheets/worksheet_0/#introduction-to-redlining","title":"Introduction to Redlining","text":"<p>This group exploration delves into the long-term impacts of historical redlining on urban greenspace, emphasizing the powerful role of maps in shaping environmental and social landscapes. By drawing on the research by Nardone et al. (2021), you will collaboratively investigate how discriminatory practices encoded in maps have led to persistent disparities in urban settings. This exploration aims to uncover the resilience of communities in adapting to these entrenched injustices and to foster a deeper understanding of how mapping can serve both as a tool of exclusion and as a means for promoting social equity.</p> <p></p>"},{"location":"worksheets/worksheet_0/#understanding-redlining-as-a-systemic-disturbance","title":"Understanding Redlining as a Systemic Disturbance","text":"<p>Redlining originated in the 1930s as a discriminatory practice where the Home Owners' Loan Corporation (HOLC) systematically denied mortgages or offered unfavorable terms based on racial and ethnic compositions. This methodical exclusion, executed through maps that color-coded \"risky\" investment areas in red, marked minority-populated areas, denying them crucial investment and development opportunities and initiating a profound and lasting disturbance in the urban fabric.</p> <p>Maps serve as powerful tools beyond navigation; they communicate and enforce control. By defining neighborhood boundaries through redlining, HOLC maps not only mirrored societal biases but also perpetuated and embedded them into the urban landscape. This manipulation of geographic data set a trajectory that limited economic growth, dictated the allocation of services, and influenced the development or deterioration of community infrastructure.</p> <p>Figure 1: 1938 Map of Atlanta uses colors as grades for neighborhoods. The red swaths identify each area with large African-American populations that were deemed \u201cless safe.\u201d</p> <p></p> <p>Explore the Story Map: Click on the image above to explore the interactive story map about [subject of the story map].</p>"},{"location":"worksheets/worksheet_0/#resilience-and-adaptation-in-urban-environments","title":"Resilience and Adaptation in Urban Environments","text":"<p>The legacy of redlining presents both a challenge and an opportunity for resilience and adaptation. Economically and socially, redlining entrenched cycles of poverty and racial segregation, creating a resilient wealth gap that has been difficult to dismantle. Environmentally, the neighborhoods targeted by redlining continue to face significant challenges\u2014they generally feature less greenspace, suffer from higher pollution levels, and are more vulnerable to the impacts of climate change. These factors compound the health and wellness challenges faced by residents.</p> <p>Despite these adversities, urban communities have continually demonstrated remarkable resilience. Adaptation strategies, such as community-led green initiatives, urban agriculture, and grassroots activism, have emerged as responses to these systemic disturbances. By enhancing green infrastructure and advocating for equitable environmental policies, these communities strive to increase their resilience against both historical inequities and environmental challenges.</p> <p>The following group exercise will uncover the impact of redlining on urban greenspace and highlight the adaptive strategies developed in response to this enduring disturbance. Through mapping and analysis, we aim to illustrate the powerful role that geographic data can play in understanding and fostering urban resilience and social equity.</p>"},{"location":"worksheets/worksheet_0/#references","title":"References","text":"<ul> <li>Nardone, A., Rudolph, K. E., Morello-Frosch, R., &amp; Casey, J. A. (2021). Redlines and Greenspace: The Relationship between Historical Redlining and 2010 Greenspace across the United States. Environmental Health Perspectives, 129(1), 017006. DOI:10.1289/EHP7495.</li> <li>Hoffman, J. S., Shandas, V., &amp; Pendleton, N. (2020). The Effects of Historical Housing Policies on Resident Exposure to Intra-Urban Heat: A Study of 108 US Urban Areas. Climate, 8(1), 12. DOI:10.3390/cli8010012.</li> </ul>"},{"location":"worksheets/worksheet_0/#group-activity","title":"Group Activity","text":""},{"location":"worksheets/worksheet_0/#setting-up-cyverse","title":"Setting up CyVerse","text":"<ul> <li>Log into CyVerse</li> <li>Use the startup procedure to start an instance on CyVerse. Be sure to:<ul> <li>Create an SSH key and add it to your Github account (2<sup>nd</sup> half of start-up procedure instructions)</li> <li>Clone the Innovation-Summit-2024 repository</li> <li>Complete the R Studio hack</li> </ul> </li> <li>Please raise your hand if you have questions or run into technical issues. ESIIL represenatives will be walking around to help.</li> <li>Once you initiate your CyVerse instance, DO NOT close it. You can keep this instance running the entire Summit so you don't have to do the start-up procedure again.</li> </ul>"},{"location":"worksheets/worksheet_0/#create-a-map","title":"Create a Map","text":"<p>We'll be using pre-developed code to visualize redlining impacts on Denver, CO. Please follow these steps: </p> <ul> <li>Open R Studio in CyVerse</li> <li>Use \"files\" (lower right) to navigate to this markdown document:<ul> <li>innovation-summit-2024/code/worksheet_redlining_student_edition.qmd</li> </ul> </li> <li>Start at the beginning of the code and complete the following:<ul> <li>Create a map of historically redlined districts in Denver</li> <li>Overlay current-day NDVI (vegetation greenness) data onto your map</li> </ul> </li> <li>You can choose \"Run All\" to run all the code at once. Note: It will take about 5 minutes to run.</li> </ul> <p></p> <ul> <li>Now, it's your turn to choose a variable to observe. Use the provided code to select the variable you want to add to your map. More detailed instructions are included in the code.</li> </ul> <p>Variable Options:</p> <ol> <li>Tree inventory</li> <li>Traffic accidents</li> <li>Stream sampling effort</li> <li>Soil sampling effort</li> <li>Public art density</li> <li>Liquor license density</li> <li>Crime density</li> </ol>"},{"location":"worksheets/worksheet_0/#decision-making","title":"Decision-Making","text":"<p>Use the gradient of agreement (Kaner 2014) to make a decision as a team about which variable you want to explore.</p> <p></p>"},{"location":"worksheets/worksheet_0/#unique-title","title":"Unique Title","text":"<p>Come up with a unique title for your anaylysis. Write it down on a sticky note at your table. </p>"},{"location":"worksheets/worksheet_0/#discussion-questions","title":"Discussion Questions","text":"<p>After completing your anaylysis, discuss these questions with your group: </p> <ol> <li>What patterns do you notice? What are the immediate questions that come to mind?</li> <li>How does big data help illustrate resilience?</li> <li>Redlining has a long-term impact. How is the impact of redlining still evident today?</li> </ol>"},{"location":"worksheets/worksheet_0/#still-have-time","title":"Still have time?","text":"<p>As a group, choose another variable to explore and then discuss your findings.</p>"},{"location":"worksheets/worksheet_0/#look-through-all-the-variables","title":"Look through all the variables:","text":"<p>Once you're done, you can see all the code and variable maps on the \"Teacher Edition\" version of the activity: https://cu-esiil.github.io/Innovation-Summit-2024/worksheets/worksheet_redlining/</p>"},{"location":"worksheets/worksheet_2/","title":"TEAM ACTIVITY Day 1: Make a plan","text":""},{"location":"worksheets/worksheet_2/#instructions","title":"Instructions","text":"<p>Work through the prompts in order. Please use a decision-making method \u201cto decide\u201d before moving to a new section of the activity.  </p>"},{"location":"worksheets/worksheet_2/#day-1-objectives","title":"Day 1 Objectives","text":"<ol> <li>Get to know your group members.</li> <li>Decide on a research question and project title.</li> <li>Start exploring potential datasets.</li> </ol>"},{"location":"worksheets/worksheet_2/#introductions-approx-time-10-mins-total-or-1-2-breaths-per-prompt","title":"Introductions (approx. time: 10 mins total or \"1-2 breaths\" per prompt)","text":"<p>Please share the following information about yourself. Each team member should type their response in the space below (create more as needed).</p> <ul> <li>Name: [Your Name]</li> <li>Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Continue adding more team members following the same format, with a line break after each person.</li> </ul>"},{"location":"worksheets/worksheet_2/#research-question-innovation-for-inclusion-or-computation-approx-time-5-10-mins","title":"Research Question: Innovation for Inclusion or Computation (approx. time: 5-10 mins)","text":"<p>Write the research question your team selected in the space below. Feel free to revise the original question.</p> <ul> <li>[Insert research question here]</li> </ul>"},{"location":"worksheets/worksheet_2/#project-title-approx-time-5-10-mins","title":"Project Title (approx. time: 5-10 mins)","text":"<p>Craft a catchy title for your team\u2019s project. Think of something that would grab attention at a conference or in a headline.</p> <ul> <li>[Insert title here]</li> </ul>"},{"location":"worksheets/worksheet_2/#promoting-resilience-and-adaptation","title":"Promoting Resilience and Adaptation","text":"<p>Describe how your proposed project aligns with the Summit's themes of resilience and adaptation. Please provide 1-2 sentences that clearly connect your project's goals or methods to these themes.</p> <ul> <li>[Insert your response here]</li> </ul>"},{"location":"worksheets/worksheet_2/#choosing-big-data-sets","title":"Choosing Big Data Sets","text":"<p>Explore potential data sets for your project's topic from the data library. List your options below, organizing them by whether they represent the system you're studying (e.g., deciduous forests) or the disruption to it (e.g., wildfire). Then discuss your choices and indicate your final selections.</p>"},{"location":"worksheets/worksheet_2/#draft-potential-data-sets","title":"Draft Potential Data Sets","text":"<ul> <li>System Being Perturbed/Disrupted:<ul> <li>[List all potential data sets here]</li> </ul> </li> <li>Perturbator/Disrupter:<ul> <li>[List all potential data sets here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_2/#final-choice","title":"Final Choice","text":"<ul> <li>System Being Perturbed/Disrupted (Final Choice):<ul> <li>[Indicate your final selected data set here]</li> </ul> </li> <li>Perturbator/Disrupter (Final Choice):<ul> <li>[Indicate your final selected data set here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_2/#brief-check-in-definition-of-resilience-approx-5-mins","title":"Brief Check-in: Definition of Resilience (approx. 5 mins)","text":"<p>Below is a working definition of the word \"Resilience\" for the Summit. Please edit the definition below based on your earlier discussion and chosen project.</p> <p>\"Resilience is the capacity of a system, community, organization, or individual to absorb stress, recover from disruptions, adapt to change, and continue to develop and thrive.\"</p> <ul> <li>[Edit or reaffirm this definition here]</li> </ul>"},{"location":"worksheets/worksheet_2/#day-1-report-back","title":"Day 1 Report Back","text":"<p>Select one representative from your group to present your proposed project. For the report back, each group will have 30-60 seconds to present their responses to the questions below. Keep it concise and focused. This is just a quick oral presentation - you will not be able to use slides/images.</p> <ul> <li>Project Title:</li> <li>[Insert your team's project title here]</li> <li>Research Question:</li> <li>[Insert your team's refined research question here]</li> <li>Selected Data Sets:</li> <li>[List the data sets your team has chosen to use here]</li> </ul>"},{"location":"worksheets/worksheet_3/","title":"TEAM ACTIVITY 2: Innovate as a Team","text":"<p>Welcome back! We hope today is a productive day getting to know your team and coding.</p>"},{"location":"worksheets/worksheet_3/#day-2-summary","title":"Day 2 summary:","text":"<p>Please complete the warm-up with your team, briefly review today\u2019s objectives, and carefully read the Day 2 and Day 3 report out items to guide your efforts.  </p>"},{"location":"worksheets/worksheet_3/#objectives-for-day-2","title":"Objectives for Day 2","text":"<ol> <li>Work together to decide on the data sets you will use. Reminder: Use a decision-making technique discussed during Day 1. Kaner\u2019s Gradient of Agreement is below for reference.</li> <li>Practice joining your datasets together. </li> <li>Discuss and try creating interesting graphics.</li> <li>Report back on your results at the end of the day. Today\u2019s report back is short and focused on your team process. The Day 3 report back is more detailed. </li> </ol>"},{"location":"worksheets/worksheet_3/#morning-warm-up","title":"Morning Warm-up","text":"<p>Please share the following informaton with your team. (No need to write down your responses this time) - Name - Pronouns - Reflecting on Day 1, what is something that surprised you?</p>"},{"location":"worksheets/worksheet_3/#decision-making","title":"Decision-Making","text":"<p>Use the gradient of agreement (Kaner 20214) to make decisions as a team.</p> <p></p>"},{"location":"worksheets/worksheet_3/#day-2-report-back","title":"Day 2 Report Back","text":"<p>Day 2 report-back questions are about the team process. We are interested in your team\u2019s unique experience. Below are some prompts you might consider. You don't need to address all of them - choose which ones you want to present. Please limit your reflection to 2-3 mins.  </p> <ol> <li>What worked well for your team?</li> <li>What\u2019s one thing you would change?</li> <li>Did your group ever have an \u201cah-ha\u201d moment?  What led up to that moment?</li> <li>Did your group experience the groan zone?  What is one tip you want to share with future groups at the Summit about getting through the groan zone?<ul> <li>[insert your group reflection responses here]</li> </ul> </li> </ol>"},{"location":"worksheets/worksheet_3/#looking-ahead-day-3-report-back","title":"Looking Ahead: Day 3 Report Back","text":"<p>These are the prompts for the final Report Back tomorrow (Day 3) - start thinking about these questions as you work today. Each group will share their Day 3 GitHub page on the screen and give a 4 minute presentation.</p> <ul> <li>Project Title:</li> <li>Research Question:</li> <li>One interesting graphic/finding:</li> <li>What are you thinking about doing next with your team? Long-term, short-term?</li> <li>What\u2019s missing: what resources, people, data sets, etc. does your team need?</li> </ul>"},{"location":"worksheets/worksheet_3/#reminder","title":"Reminder","text":"<p>There is the opportunity for groups to continue working on their projects as an ESIIL Working Group. If you love your team and want to continue working together, considering submitting a Working Group Application this fall. See the ESIIL website for more information: https://esiil.org/working-groups.</p>"},{"location":"worksheets/worksheet_4/","title":"TEAM ACTIVITY 3: Share Your Progress","text":"<p>Use this time to prepare for the final report back, where you'll share an interesting result/outcome from your project and discuss potential future plans.</p>"},{"location":"worksheets/worksheet_4/#day-3-report-back","title":"Day 3 Report Back","text":"<p>Select one or more people from your group to give a final report back. You will share this page on the screen as your presentation. Presentations should be no longer than 4 minutes.</p> <ul> <li>Project Title:<ul> <li>[Insert project title here]</li> </ul> </li> <li>Research Question:<ul> <li>[Insert research question here]</li> </ul> </li> <li>One interesting graphic/finding:<ul> <li>[Insert graphic/finding here]</li> </ul> </li> <li>What are you thinking about doing next with your team? Long-term, short-term?<ul> <li>[Insert response here]</li> </ul> </li> <li>What\u2019s missing: what resources, people, data sets, etc. does your team need?<ul> <li>[Insert response here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_4/#reminder","title":"Reminder","text":"<p>There is the opportunity for groups to continue working on their projects as an ESIIL Working Group. If you love your team and want to continue working together, considering submitting a Working Group Application this fall. See the ESIIL website for more information: https://esiil.org/working-groups.</p> <p>Thank you for participating in the 2024 ESIIL Innovation Summit!!</p>"},{"location":"worksheets/worksheet_5/","title":"TEAM ACTIVITY 2: Make a plan","text":""},{"location":"worksheets/worksheet_5/#instructions","title":"Instructions:","text":"<ul> <li>Follow the Prompts Sequentially:<ul> <li>Work through the prompts in the order they are presented.</li> </ul> </li> <li>Decision-Making Process:<ul> <li>Before advancing to the next section of the handout, use a structured decision-making method. Ensure that all team members agree on the decisions made. This approach helps in maintaining coherence and collective agreement throughout the activities.</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_5/#introductions-approx-time-10-mins-total-or-2-breaths-per-person","title":"Introductions (approx. time: 10 mins total or \"2 breaths\" per person)","text":"<ul> <li> <p>Each team member please share the following information about yourself:</p> </li> <li> <p>Name: [Your Name]</p> </li> <li>Preferred Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Preferred Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Preferred Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Name: [Your Name]</li> <li>Preferred Pronouns: [Your Pronouns]</li> <li>Expertise: [Your Expertise]</li> <li>Environmental Data Science Superpower: [Describe your unique skill or interest in environmental data science]</li> <li>Reflection on Polarities Exercise: [Share one thing you observed about yourself]</li> </ul> <ul> <li>Continue adding more team members following the same format, with a line break after each person.</li> </ul>"},{"location":"worksheets/worksheet_5/#research-question-innovation-for-inclusion-or-computation-approx-time-5-10-mins","title":"Research Question: Innovation for Inclusion or Computation (approx. time: 5-10 mins)","text":"<ul> <li>Refine the initial research question your team developed earlier. Please make any necessary edits or adjustments below:</li> <li>[Edit or refine your team's previously selected research question here]</li> </ul>"},{"location":"worksheets/worksheet_5/#title-innovation-for-inclusion-or-computation-approx-time-5-10-mins","title":"Title: Innovation for Inclusion or Computation (approx. time: 5-10 mins)","text":"<ul> <li>Craft a catchy and public-facing title for your team\u2019s project. Think of something that would grab attention at a conference or in a headline:</li> <li>[Create an engaging title that captures the essence of your project here]</li> </ul>"},{"location":"worksheets/worksheet_5/#promoting-resilience-and-adaptation","title":"Promoting Resilience and Adaptation","text":"<ul> <li>Describe how your proposed project aligns with the summit's themes of resilience and adaptation. Please provide 1-2 sentences that clearly connect your project's goals or methods to these themes:</li> <li>[Insert your response here]</li> </ul>"},{"location":"worksheets/worksheet_5/#which-big-data-sets","title":"Which Big Data Sets","text":"<ul> <li>Explore potential data sets for your project's topic from the data library. List your options below, and after discussion and review, indicate your final choice for both the system being perturbed/disrupted and the perturbator/disrupter.</li> </ul>"},{"location":"worksheets/worksheet_5/#draft-potential-data-sets","title":"Draft Potential Data Sets","text":"<ul> <li>System Being Perturbed/Disrupted:<ul> <li>[List all potential data sets here]</li> </ul> </li> <li>Perturbator/Disrupter:<ul> <li>[List all potential data sets here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_5/#final-choice","title":"Final Choice","text":"<ul> <li>System Being Perturbed/Disrupted (Final Choice):<ul> <li>[Indicate your final selected data set here]</li> </ul> </li> <li>Perturbator/Disrupter (Final Choice):<ul> <li>[Indicate your final selected data set here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_5/#brief-check-in-definition-of-resilience-approx-5-mins","title":"Brief Check-in: Definition of Resilience (approx. 5 mins)","text":"<ul> <li>Review and refine the working definition of 'Resilience' provided below, based on your discussions and insights from earlier sections of this worksheet. Adjust the definition to better align with your team\u2019s understanding or reaffirm it if it resonates with your views:</li> <li>\"Resilience is the capacity of a system, community, organization, or individual to absorb stress, recover from disruptions, adapt to change, and continue to develop and thrive.\"<ul> <li>[Edit or reaffirm this definition here]</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_5/#day-1-report-back","title":"Day 1 Report Back","text":"<ul> <li>Select one representative from your group to present your proposed project to all Summit attendees (~125 people). This is an opportunity for your breakout group to summarize your project\u2019s approach as it relates to the Summit themes of adaptation and resilience.</li> <li> <p>Presentation Content:</p> <ul> <li>Project Title: </li> <li>[Insert your team's project title here]</li> <li>Research Question: </li> <li>[Insert your team's refined research question here]</li> <li>Selected Data Sets: </li> <li>[List the data sets your team has chosen to use here]</li> </ul> </li> <li> <p>Presentation Guidelines:</p> <ul> <li>Duration: Your presentation should last between 30-60 seconds. Keep it concise and focused. This is just a quick oral presentation -you will not be able to use slides/images.</li> <li>Objective: Clearly communicate how your project aligns with the conference themes and highlight actionable insights that can aid decision makers.</li> </ul> </li> </ul>"},{"location":"worksheets/worksheet_redlining/","title":"Redlining","text":""},{"location":"worksheets/worksheet_redlining/#exploring-the-impact-of-historical-redlining-on-urban-greenspace-a-collaborative-examination-of-maps-justice-and-resilience","title":"Exploring the Impact of Historical Redlining on Urban Greenspace: A Collaborative Examination of Maps, Justice, and Resilience","text":""},{"location":"worksheets/worksheet_redlining/#introduction","title":"Introduction","text":"<p>This group exploration delves into the long-term impacts of historical redlining on urban greenspace, emphasizing the powerful role of maps in shaping environmental and social landscapes. By drawing on the research by Nardone et al.\u00a0(2021), you will collaboratively investigate how discriminatory practices encoded in maps have led to persistent disparities in urban settings. This exploration aims to uncover the resilience of communities in adapting to these entrenched injustices and to foster a deeper understanding of how mapping can serve both as a tool of exclusion and as a means for promoting social equity.</p> <p>)</p>"},{"location":"worksheets/worksheet_redlining/#understanding-redlining-as-a-systemic-disturbance","title":"Understanding Redlining as a Systemic Disturbance","text":"<p>Redlining originated in the 1930s as a discriminatory practice where the Home Owners\u2019 Loan Corporation (HOLC) systematically denied mortgages or offered unfavorable terms based on racial and ethnic compositions. This methodical exclusion, executed through maps that color-coded \u201crisky\u201d investment areas in red, marked minority-populated areas, denying them crucial investment and development opportunities and initiating a profound and lasting disturbance in the urban fabric.</p> <p>Maps serve as powerful tools beyond navigation; they communicate and enforce control. By defining neighborhood boundaries through redlining, HOLC maps not only mirrored societal biases but also perpetuated and embedded them into the urban landscape. This manipulation of geographic data set a trajectory that limited economic growth, dictated the allocation of services, and influenced the development or deterioration of community infrastructure.</p> <p>Figure 1: 1938 Map of Atlanta uses colors as grades for neighborhoods. The red swaths identify each area with large African-American populations that were deemed \u201cless safe.\u201d</p> <p></p> <p>ArcGIS Story Map</p> <p>Explore the Story Map: Click on the image above to explore the interactive story map about [subject of the story map].</p>"},{"location":"worksheets/worksheet_redlining/#resilience-and-adaptation-in-urban-environments","title":"Resilience and Adaptation in Urban Environments","text":"<p>The legacy of redlining presents both a challenge and an opportunity for resilience and adaptation. Economically and socially, redlining entrenched cycles of poverty and racial segregation, creating a resilient wealth gap that has been difficult to dismantle. Environmentally, the neighborhoods targeted by redlining continue to face significant challenges\u2014they generally feature less greenspace, suffer from higher pollution levels, and are more vulnerable to the impacts of climate change. These factors compound the health and wellness challenges faced by residents.</p> <p>Despite these adversities, urban communities have continually demonstrated remarkable resilience. Adaptation strategies, such as community-led green initiatives, urban agriculture, and grassroots activism, have emerged as responses to these systemic disturbances. By enhancing green infrastructure and advocating for equitable environmental policies, these communities strive to increase their resilience against both historical inequities and environmental challenges.</p> <p></p> <p>Watch the video</p> <p>Video Title: Exploring the Impacts of Historical Redlining on Urban Development Description: Click on the image above to watch a video that delves into the consequences of historical redlining and its ongoing impact on urban environments. This educational piece offers insights into how such discriminatory practices have shaped cities and what can be learned from them.</p> <p>The following group exercise will not only uncover the impact of redlining on urban greenspace but also highlight the adaptive strategies developed in response to this enduring disturbance. Through mapping and analysis, we aim to illustrate the powerful role that geographic data can play in understanding and fostering urban resilience and social equity.</p>"},{"location":"worksheets/worksheet_redlining/#references","title":"References","text":"<ul> <li>Nardone, A., Rudolph, K. E., Morello-Frosch, R., &amp; Casey, J. A.     (2021). Redlines and Greenspace: The Relationship between Historical     Redlining and 2010 Greenspace across the United States.     Environmental Health Perspectives, 129(1), 017006.     DOI:10.1289/EHP7495.</li> <li>Hoffman, J. S., Shandas, V., &amp; Pendleton, N. (2020). The Effects of     Historical Housing Policies on Resident Exposure to Intra-Urban     Heat: A Study of 108 US Urban Areas. Climate, 8(1), 12.     DOI:10.3390/cli8010012.</li> </ul>"},{"location":"worksheets/worksheet_redlining/#goals-of-this-group-activity","title":"Goals of this group activity","text":"<p>The primary objectives of this tutorial are: 1. To practice coding in CyVerse. 2. To analyze the relationship between HOLC grades and the presence of urban greenspace. 3. To understand how historic policies continue to affect the spatial distribution of environmental amenities.</p>"},{"location":"worksheets/worksheet_redlining/#part-1-accessing-and-visualizing-historic-redlining-data","title":"Part 1: Accessing and Visualizing Historic Redlining Data","text":"<p>We will begin by accessing HOLC maps from the Mapping Inequality project and overlaying this data with modern geographic datasets to visualize the historical impact on contemporary urban landscapes.</p>"},{"location":"worksheets/worksheet_redlining/#data-acquisition","title":"Data Acquisition","text":"<ul> <li>Download HOLC map shapefiles from the University of Richmond\u2019s     Mapping Inequality Project.</li> <li>Utilize satellite imagery and other geospatial data to map current     greenspace using the normalized difference vegetation index (NDVI).</li> </ul>"},{"location":"worksheets/worksheet_redlining/#analysis-methodology","title":"Analysis Methodology","text":"<ul> <li>Replicate the approach used by Nardone et al.\u00a0to calculate NDVI     values for each HOLC neighborhood, assessing greenspace as a     health-promoting resource.</li> <li>Employ statistical methods such as propensity score matching to     control for confounding variables and estimate the true impact of     HOLC grades on urban greenspace.</li> </ul>  R libraries we use in this analysis  <pre><code>if (!requireNamespace(\"tidytext\", quietly = TRUE)) {\n  install.packages(\"tidytext\")\n}\nlibrary(tidytext)\n## Warning: package 'tidytext' was built under R version 4.3.2\nlibrary(sf)\n## Warning: package 'sf' was built under R version 4.3.2\n## Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\nlibrary(ggplot2)\n## Warning: package 'ggplot2' was built under R version 4.3.2\nlibrary(ggthemes)\n## Warning: package 'ggthemes' was built under R version 4.3.2\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(rstac)\n## Warning: package 'rstac' was built under R version 4.3.2\nlibrary(gdalcubes)\n## Warning: package 'gdalcubes' was built under R version 4.3.2\nlibrary(gdalUtils)\n## Please note that rgdal will be retired during October 2023,\n## plan transition to sf/stars/terra functions using GDAL and PROJ\n## at your earliest convenience.\n## See https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\n## rgdal: version: 1.6-7, (SVN revision 1203)\n## Geospatial Data Abstraction Library extensions to R successfully loaded\n## Loaded GDAL runtime: GDAL 3.5.3, released 2022/10/21\n## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/rgdal/gdal\n##  GDAL does not use iconv for recoding strings.\n## GDAL binary built with GEOS: TRUE \n## Loaded PROJ runtime: Rel. 9.1.0, September 1st, 2022, [PJ_VERSION: 910]\n## Path to PROJ shared files: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/gdalcubes/proj\n## PROJ CDN enabled: FALSE\n## Linking to sp version:1.6-1\n## To mute warnings of possible GDAL/OSR exportToProj4() degradation,\n## use options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n## \n## Attaching package: 'gdalUtils'\n## The following object is masked from 'package:sf':\n## \n##     gdal_rasterize\nlibrary(gdalcubes)\nlibrary(colorspace)\nlibrary(terra)\n## Warning: package 'terra' was built under R version 4.3.2\n## terra 1.7.71\n## \n## Attaching package: 'terra'\n## The following object is masked from 'package:colorspace':\n## \n##     RGB\n## The following objects are masked from 'package:gdalcubes':\n## \n##     animate, crop, size\nlibrary(tidyterra)\n## \n## Attaching package: 'tidyterra'\n## The following object is masked from 'package:stats':\n## \n##     filter\nlibrary(basemapR)\nlibrary(tidytext)\nlibrary(ggwordcloud)\nlibrary(osmextract)\n## Data (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\n## Check the package website, https://docs.ropensci.org/osmextract/, for more details.\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(glue)\n## \n## Attaching package: 'glue'\n## The following object is masked from 'package:terra':\n## \n##     trim\n\nlibrary(purrr)\n</code></pre>  FUNCTION: List cities where HOLC data are available  <pre><code># Function to get a list of unique cities and states from the redlining data\nget_city_state_list_from_redlining_data &lt;- function() {\n  # URL to the GeoJSON data\n  url &lt;- \"https://raw.githubusercontent.com/americanpanorama/mapping-inequality-census-crosswalk/main/MIv3Areas_2010TractCrosswalk.geojson\"\n\n  # Read the GeoJSON file into an sf object\n  redlining_data &lt;- tryCatch({\n    read_sf(url)\n  }, error = function(e) {\n    stop(\"Error reading GeoJSON data: \", e$message)\n  })\n\n  # Check for the existence of 'city' and 'state' columns\n  if (!all(c(\"city\", \"state\") %in% names(redlining_data))) {\n    stop(\"The required columns 'city' and/or 'state' do not exist in the data.\")\n  }\n\n  # Extract a unique list of city and state pairs without the geometries\n  city_state_df &lt;- redlining_data %&gt;%\n    select(city, state) %&gt;%\n    st_set_geometry(NULL) %&gt;%  # Drop the geometry to avoid issues with invalid shapes\n    distinct(city, state) %&gt;%\n    arrange(state, city )  # Arrange the list alphabetically by state, then by city\n\n  # Return the dataframe of unique city-state pairs\n  return(city_state_df)\n}\n</code></pre>  Stream list of available HOLC cities  <pre><code>#Retrieve the list of cities and states\ncity_state_list &lt;- get_city_state_list_from_redlining_data()\n\nknitr::kable(city_state_list, format = \"markdown\")\n</code></pre>  | city                             | state | |:---------------------------------|:------| | Birmingham                       | AL    | | Mobile                           | AL    | | Montgomery                       | AL    | | Arkadelphia                      | AR    | | Batesville                       | AR    | | Camden                           | AR    | | Conway                           | AR    | | El Dorado                        | AR    | | Fort Smith                       | AR    | | Little Rock                      | AR    | | Russellville                     | AR    | | Texarkana                        | AR    | | Phoenix                          | AZ    | | Fresno                           | CA    | | Los Angeles                      | CA    | | Oakland                          | CA    | | Sacramento                       | CA    | | San Diego                        | CA    | | San Francisco                    | CA    | | San Jose                         | CA    | | Stockton                         | CA    | | Boulder                          | CO    | | Colorado Springs                 | CO    | | Denver                           | CO    | | Fort Collins                     | CO    | | Fort Morgan                      | CO    | | Grand Junction                   | CO    | | Greeley                          | CO    | | Longmont                         | CO    | | Pueblo                           | CO    | | Bridgeport and Fairfield         | CT    | | Hartford                         | CT    | | New Britain                      | CT    | | New Haven                        | CT    | | Stamford, Darien, and New Canaan | CT    | | Waterbury                        | CT    | | Crestview                        | FL    | | Daytona Beach                    | FL    | | DeFuniak Springs                 | FL    | | DeLand                           | FL    | | Jacksonville                     | FL    | | Miami                            | FL    | | New Smyrna                       | FL    | | Orlando                          | FL    | | Pensacola                        | FL    | | St.\u00a0Petersburg                   | FL    | | Tampa                            | FL    | | Atlanta                          | GA    | | Augusta                          | GA    | | Columbus                         | GA    | | Macon                            | GA    | | Savannah                         | GA    | | Boone                            | IA    | | Cedar Rapids                     | IA    | | Council Bluffs                   | IA    | | Davenport                        | IA    | | Des Moines                       | IA    | | Dubuque                          | IA    | | Sioux City                       | IA    | | Waterloo                         | IA    | | Aurora                           | IL    | | Chicago                          | IL    | | Decatur                          | IL    | | East St.\u00a0Louis                   | IL    | | Joliet                           | IL    | | Peoria                           | IL    | | Rockford                         | IL    | | Springfield                      | IL    | | Evansville                       | IN    | | Fort Wayne                       | IN    | | Indianapolis                     | IN    | | Lake Co.\u00a0Gary                    | IN    | | Muncie                           | IN    | | South Bend                       | IN    | | Terre Haute                      | IN    | | Atchison                         | KS    | | Junction City                    | KS    | | Topeka                           | KS    | | Wichita                          | KS    | | Covington                        | KY    | | Lexington                        | KY    | | Louisville                       | KY    | | New Orleans                      | LA    | | Shreveport                       | LA    | | Arlington                        | MA    | | Belmont                          | MA    | | Boston                           | MA    | | Braintree                        | MA    | | Brockton                         | MA    | | Brookline                        | MA    | | Cambridge                        | MA    | | Chelsea                          | MA    | | Dedham                           | MA    | | Everett                          | MA    | | Fall River                       | MA    | | Fitchburg                        | MA    | | Haverhill                        | MA    | | Holyoke Chicopee                 | MA    | | Lawrence                         | MA    | | Lexington                        | MA    | | Lowell                           | MA    | | Lynn                             | MA    | | Malden                           | MA    | | Medford                          | MA    | | Melrose                          | MA    | | Milton                           | MA    | | Needham                          | MA    | | New Bedford                      | MA    | | Newton                           | MA    | | Pittsfield                       | MA    | | Quincy                           | MA    | | Revere                           | MA    | | Salem                            | MA    | | Saugus                           | MA    | | Somerville                       | MA    | | Springfield                      | MA    | | Waltham                          | MA    | | Watertown                        | MA    | | Winchester                       | MA    | | Winthrop                         | MA    | | Worcester                        | MA    | | Baltimore                        | MD    | | Augusta                          | ME    | | Boothbay                         | ME    | | Portland                         | ME    | | Sanford                          | ME    | | Waterville                       | ME    | | Battle Creek                     | MI    | | Bay City                         | MI    | | Detroit                          | MI    | | Flint                            | MI    | | Grand Rapids                     | MI    | | Jackson                          | MI    | | Kalamazoo                        | MI    | | Lansing                          | MI    | | Muskegon                         | MI    | | Pontiac                          | MI    | | Saginaw                          | MI    | | Austin                           | MN    | | Duluth                           | MN    | | Mankato                          | MN    | | Minneapolis                      | MN    | | Rochester                        | MN    | | St.\u00a0Cloud                        | MN    | | St.\u00a0Paul                         | MN    | | Staples                          | MN    | | Cape Girardeau                   | MO    | | Carthage                         | MO    | | Greater Kansas City              | MO    | | Joplin                           | MO    | | Springfield                      | MO    | | St.\u00a0Joseph                       | MO    | | St.\u00a0Louis                        | MO    | | Jackson                          | MS    | | Asheville                        | NC    | | Charlotte                        | NC    | | Durham                           | NC    | | Elizabeth City                   | NC    | | Fayetteville                     | NC    | | Goldsboro                        | NC    | | Greensboro                       | NC    | | Hendersonville                   | NC    | | High Point                       | NC    | | New Bern                         | NC    | | Rocky Mount                      | NC    | | Statesville                      | NC    | | Winston-Salem                    | NC    | | Fargo                            | ND    | | Grand Forks                      | ND    | | Minot                            | ND    | | Williston                        | ND    | | Lincoln                          | NE    | | Omaha                            | NE    | | Manchester                       | NH    | | Atlantic City                    | NJ    | | Bergen Co.                       | NJ    | | Camden                           | NJ    | | Essex Co.                        | NJ    | | Hudson Co.                       | NJ    | | Monmouth                         | NJ    | | Passaic County                   | NJ    | | Perth Amboy                      | NJ    | | Trenton                          | NJ    | | Union Co.                        | NJ    | | Albany                           | NY    | | Binghamton-Johnson City          | NY    | | Bronx                            | NY    | | Brooklyn                         | NY    | | Buffalo                          | NY    | | Elmira                           | NY    | | Jamestown                        | NY    | | Lower Westchester Co.            | NY    | | Manhattan                        | NY    | | Niagara Falls                    | NY    | | Poughkeepsie                     | NY    | | Queens                           | NY    | | Rochester                        | NY    | | Schenectady                      | NY    | | Staten Island                    | NY    | | Syracuse                         | NY    | | Troy                             | NY    | | Utica                            | NY    | | Akron                            | OH    | | Canton                           | OH    | | Cleveland                        | OH    | | Columbus                         | OH    | | Dayton                           | OH    | | Hamilton                         | OH    | | Lima                             | OH    | | Lorain                           | OH    | | Portsmouth                       | OH    | | Springfield                      | OH    | | Toledo                           | OH    | | Warren                           | OH    | | Youngstown                       | OH    | | Ada                              | OK    | | Alva                             | OK    | | Enid                             | OK    | | Miami Ottawa County              | OK    | | Muskogee                         | OK    | | Norman                           | OK    | | Oklahoma City                    | OK    | | South McAlester                  | OK    | | Tulsa                            | OK    | | Portland                         | OR    | | Allentown                        | PA    | | Altoona                          | PA    | | Bethlehem                        | PA    | | Chester                          | PA    | | Erie                             | PA    | | Harrisburg                       | PA    | | Johnstown                        | PA    | | Lancaster                        | PA    | | McKeesport                       | PA    | | New Castle                       | PA    | | Philadelphia                     | PA    | | Pittsburgh                       | PA    | | Wilkes-Barre                     | PA    | | York                             | PA    | | Pawtucket &amp; Central Falls        | RI    | | Providence                       | RI    | | Woonsocket                       | RI    | | Aiken                            | SC    | | Charleston                       | SC    | | Columbia                         | SC    | | Greater Anderson                 | SC    | | Greater Greenville               | SC    | | Orangeburg                       | SC    | | Rock Hill                        | SC    | | Spartanburg                      | SC    | | Sumter                           | SC    | | Aberdeen                         | SD    | | Huron                            | SD    | | Milbank                          | SD    | | Mitchell                         | SD    | | Rapid City                       | SD    | | Sioux Falls                      | SD    | | Vermillion                       | SD    | | Watertown                        | SD    | | Chattanooga                      | TN    | | Elizabethton                     | TN    | | Erwin                            | TN    | | Greenville                       | TN    | | Johnson City                     | TN    | | Knoxville                        | TN    | | Memphis                          | TN    | | Nashville                        | TN    | | Amarillo                         | TX    | | Austin                           | TX    | | Beaumont                         | TX    | | Dallas                           | TX    | | El Paso                          | TX    | | Fort Worth                       | TX    | | Galveston                        | TX    | | Houston                          | TX    | | Port Arthur                      | TX    | | San Antonio                      | TX    | | Waco                             | TX    | | Wichita Falls                    | TX    | | Ogden                            | UT    | | Salt Lake City                   | UT    | | Bristol                          | VA    | | Danville                         | VA    | | Harrisonburg                     | VA    | | Lynchburg                        | VA    | | Newport News                     | VA    | | Norfolk                          | VA    | | Petersburg                       | VA    | | Phoebus                          | VA    | | Richmond                         | VA    | | Roanoke                          | VA    | | Staunton                         | VA    | | Bennington                       | VT    | | Brattleboro                      | VT    | | Burlington                       | VT    | | Montpelier                       | VT    | | Newport City                     | VT    | | Poultney                         | VT    | | Rutland                          | VT    | | Springfield                      | VT    | | St.\u00a0Albans                       | VT    | | St.\u00a0Johnsbury                    | VT    | | Windsor                          | VT    | | Seattle                          | WA    | | Spokane                          | WA    | | Tacoma                           | WA    | | Kenosha                          | WI    | | Madison                          | WI    | | Milwaukee Co.                    | WI    | | Oshkosh                          | WI    | | Racine                           | WI    | | Charleston                       | WV    | | Huntington                       | WV    | | Wheeling                         | WV    |    FUNCTION: Stream HOLC data from a city  <pre><code># Function to load and filter redlining data by city\nload_city_redlining_data &lt;- function(city_name) {\n  # URL to the GeoJSON data\n  url &lt;- \"https://raw.githubusercontent.com/americanpanorama/mapping-inequality-census-crosswalk/main/MIv3Areas_2010TractCrosswalk.geojson\"\n\n  # Read the GeoJSON file into an sf object\n  redlining_data &lt;- read_sf(url)\n\n  # Filter the data for the specified city and non-empty grades\n\n  city_redline &lt;- redlining_data %&gt;%\n    filter(city == city_name )\n\n  # Return the filtered data\n  return(city_redline)\n}\n</code></pre>  Stream HOLC data for Denver, CO  <pre><code># Load redlining data for Denver\ndenver_redlining &lt;- load_city_redlining_data(\"Denver\")\nknitr::kable(head(denver_redlining), format = \"markdown\")\n</code></pre>  | area_id | city   | state | city_survey | cat  | grade | label | res  | com   | ind   | fill     | GEOID10     | GISJOIN        |    calc_area | pct_tract | geometry                     | |--------:|:-------|:------|:------------|:-----|:------|:------|:-----|:------|:------|:---------|:------------|:---------------|-------------:|----------:|:-----------------------------| |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004104 | G0800310004104 | 1.525535e+01 |   0.00001 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004201 | G0800310004201 | 3.987458e+05 |   0.20900 | MULTIPOLYGON (((-104.9246 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004304 | G0800310004304 | 1.554195e+05 |   0.05927 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004202 | G0800310004202 | 1.117770e+06 |   0.57245 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6529 | Denver | CO    | TRUE        | Best | A     | A2    | TRUE | FALSE | FALSE | \\#76a865 | 08031004302 | G0800310004302 | 3.133415e+05 |   0.28381 | MULTIPOLYGON (((-104.928 39\u2026 | |    6529 | Denver | CO    | TRUE        | Best | A     | A2    | TRUE | FALSE | FALSE | \\#76a865 | 08031004301 | G0800310004301 | 1.221218e+05 |   0.08622 | MULTIPOLYGON (((-104.9305 3\u2026 |    FUNCTION: Get Points-of-Interest from city of interest  <pre><code>get_places &lt;- function(polygon_layer, type = \"food\" ) {\n  # Check if the input is an sf object\n  if (!inherits(polygon_layer, \"sf\")) {\n    stop(\"The provided object is not an sf object.\")\n  }\n\n  # Create a bounding box from the input sf object\n  bbox_here &lt;- st_bbox(polygon_layer) |&gt;\n    st_as_sfc()\n\n  if(type == \"food\"){\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n                 shop IN ('supermarket', 'bodega', 'market', 'other_market', 'farm', 'garden_centre', 'doityourself', 'farm_supply', 'compost', 'mulch', 'fertilizer') OR\n                 amenity IN ('social_facility', 'market', 'restaurant', 'coffee') OR\n                 leisure = 'garden' OR\n                 landuse IN ('farm', 'farmland', 'row_crops', 'orchard_plantation', 'dairy_grazing') OR\n                 building IN ('brewery', 'winery', 'distillery') OR\n                 shop = 'greengrocer' OR\n                 amenity = 'marketplace'\n               )\"\n    title &lt;- \"food\"\n  }\n\n  if (type == \"processed_food\") {\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n                   amenity IN ('fast_food', 'cafe', 'pub') OR\n                   shop IN ('convenience', 'supermarket') OR\n                   shop = 'kiosk'\n                 )\"\n    title &lt;- \"Processed Food Locations\"\n}\n\n  if(type == \"natural_habitats\"){\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n             boundary = 'protected_area' OR\n             natural IN ('tree', 'wood') OR\n             landuse = 'forest' OR\n             leisure = 'park'\n           )\"\n    title &lt;- \"Natural habitats or City owned trees\"\n  }\n\n   if(type == \"roads\"){\n    my_layer &lt;- \"lines\"\n    my_query &lt;- \"SELECT * FROM lines WHERE (\n             highway IN ('motorway', 'trunk', 'primary', 'secondary', 'tertiary') )\"\n    title &lt;- \"Major roads\"\n   }\n\n  if(type == \"rivers\"){\n    my_layer &lt;- \"lines\"\n    my_query &lt;- \"SELECT * FROM lines WHERE (\n             waterway IN ('river'))\"\n    title &lt;- \"Major rivers\"\n  }\n\n  if(type == \"internet_access\") {\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n                 amenity IN ('library', 'cafe', 'community_centre', 'public_building') AND\n                 internet_access = 'yes' \n               )\"\n    title &lt;- \"Internet Access Locations\"\n}\n\n  if(type == \"water_bodies\") {\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n                 natural IN ('water', 'lake', 'pond') OR\n                 water IN ('lake', 'pond') OR\n                 landuse = 'reservoir'\n               )\"\n    title &lt;- \"Water Bodies\"\n}\n\n if(type == \"government_buildings\") {\n    my_layer &lt;- \"multipolygons\"\n    my_query &lt;- \"SELECT * FROM multipolygons WHERE (\n                 amenity IN ('townhall', 'courthouse', 'embassy', 'police', 'fire_station') OR\n                 building IN ('capitol', 'government')\n               )\"\n    title &lt;- \"Government Buildings\"\n}\n\n\n\n  # Use the bbox to get data with oe_get(), specifying the desired layer and a custom SQL query for fresh food places\n  tryCatch({\n    places &lt;- oe_get(\n      place = bbox_here,\n      layer = my_layer,  # Adjusted layer; change as per actual data availability\n      query = my_query,\n      quiet = TRUE\n    )\n\n  places &lt;- st_make_valid(places)\n\n    # Crop the data to the bounding box\n    cropped_places &lt;- st_crop(places, bbox_here)\n\n    # Plotting the cropped fresh food places\n    plot &lt;- ggplot(data = cropped_places) +\n      geom_sf(fill=\"cornflowerblue\", color=\"cornflowerblue\") +\n      ggtitle(title) +\n  theme_tufte()+\n  theme(legend.position = \"none\",  # Optionally hide the legend\n        axis.text = element_blank(),     # Remove axis text\n        axis.title = element_blank(),    # Remove axis titles\n        axis.ticks = element_blank(),    # Remove axis ticks\n         plot.background = element_rect(fill = \"white\", color = NA),  # Set the plot background to white\n        panel.background = element_rect(fill = \"white\", color = NA),  # Set the panel background to white\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),\n        ) \n\n    # Save the plot as a PNG file\n    png_filename &lt;- paste0(title,\"_\", Sys.Date(), \".png\")\n    ggsave(png_filename, plot, width = 10, height = 8, units = \"in\")\n\n    # Return the cropped dataset\n    return(cropped_places)\n  }, error = function(e) {\n    stop(\"Failed to retrieve or plot data: \", e$message)\n  })\n}\n</code></pre>  FUNCTION: Plot POI over HOLC grades  <pre><code>plot_city_redlining &lt;- function(redlining_data, filename = \"redlining_plot.png\") {\n  # Fetch additional geographic data based on redlining data\n  roads &lt;- get_places(redlining_data, type = \"roads\")\n  rivers &lt;- get_places(redlining_data, type = \"rivers\")\n\n  # Filter residential zones with valid grades and where city survey is TRUE\n  residential_zones &lt;- redlining_data %&gt;%\n    filter(city_survey == TRUE &amp; grade != \"\") \n\n  # Colors for the grades\n  colors &lt;- c(\"#76a865\", \"#7cb5bd\", \"#ffff00\", \"#d9838d\")\n\n  # Plot the data using ggplot2\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.5, lwd = 1.1) +\n    geom_sf(data = residential_zones, aes(fill = grade), alpha = 0.5) +\n    theme_tufte() +\n    scale_fill_manual(values = colors) +\n    labs(fill = 'HOLC Categories') +\n    theme(\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      legend.position = \"right\"\n    )\n\n  # Save the plot as a high-resolution PNG file\n  ggsave(filename, plot, width = 10, height = 8, units = \"in\", dpi = 600)\n\n  # Return the plot object if needed for further manipulation or checking\n  return(plot)\n}\n</code></pre>  Plot Denver Redlining  <pre><code>denver_plot &lt;- plot_city_redlining(denver_redlining)\n</code></pre>  Stream amenities by category  <pre><code>food &lt;- get_places(denver_redlining, type=\"food\")\n\nfood_processed &lt;- get_places(denver_redlining, type=\"processed_food\")\n\nnatural_habitats &lt;- get_places(denver_redlining, type=\"natural_habitats\")\n\nroads &lt;- get_places(denver_redlining, type=\"roads\")\n\nrivers &lt;- get_places(denver_redlining, type=\"rivers\")\n\n#water_bodies &lt;- get_places(denver_redlining, type=\"water_bodies\")\n\ngovernment_buildings &lt;- get_places(denver_redlining, type=\"government_buildings\")\n</code></pre>  FUNCTION: Plot the HOLC grades individually  <pre><code>split_plot &lt;- function(sf_data, roads, rivers) {\n  # Filter for grades A, B, C, and D\n  sf_data_filtered &lt;- sf_data %&gt;% \n    filter(grade %in% c('A', 'B', 'C', 'D'))\n\n  # Define a color for each grade\n  grade_colors &lt;- c(\"A\" = \"#76a865\", \"B\" = \"#7cb5bd\", \"C\" = \"#ffff00\", \"D\" = \"#d9838d\")\n\n  # Create the plot with panels for each grade\n  plot &lt;- ggplot(data = sf_data_filtered) +\n    geom_sf(data = roads, alpha = 0.1, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(aes(fill = grade)) +\n    facet_wrap(~ grade, nrow = 1) +  # Free scales for different zoom levels if needed\n    scale_fill_manual(values = grade_colors) +\n    theme_minimal() +\n    labs(fill = 'HOLC Grade') +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"none\",  # Optionally hide the legend\n          axis.text = element_blank(),     # Remove axis text\n          axis.title = element_blank(),    # Remove axis titles\n          axis.ticks = element_blank(),    # Remove axis ticks\n          panel.grid.major = element_blank(),  # Remove major grid lines\n          panel.grid.minor = element_blank())  \n\n  ggsave(plot, filename = \"HOLC_grades_individually.png\", width = 10, height = 4, units = \"in\", dpi = 1200)\n  return(plot)\n}\n</code></pre>  Plot 4 HOLC grades individually  <pre><code>plot_row &lt;- split_plot(denver_redlining, roads, rivers)\n</code></pre>  FUNCTION: Map an amenity over each grade individually  <pre><code>process_and_plot_sf_layers &lt;- function(layer1, layer2, output_file = \"output_plot.png\") {\n # Make geometries valid\nlayer1 &lt;- st_make_valid(layer1)\nlayer2 &lt;- st_make_valid(layer2)\n\n# Optionally, simplify geometries to remove duplicate vertices\nlayer1 &lt;- st_simplify(layer1, preserveTopology = TRUE) |&gt;\n  filter(grade != \"\")\n\n# Prepare a list to store results\nresults &lt;- list()\n\n# Loop through each grade and perform operations\nfor (grade in c(\"A\", \"B\", \"C\", \"D\")) {\n  # Filter layer1 for current grade\n  layer1_grade &lt;- layer1[layer1$grade == grade, ]\n\n  # Buffer the geometries of the current grade\n  buffered_layer1_grade &lt;- st_buffer(layer1_grade, dist = 500)\n\n  # Intersect with the second layer\n  intersections &lt;- st_intersects(layer2, buffered_layer1_grade, sparse = FALSE)\n  selected_polygons &lt;- layer2[rowSums(intersections) &gt; 0, ]\n\n  # Add a new column to store the grade information\n  selected_polygons$grade &lt;- grade\n\n  # Store the result\n  results[[grade]] &lt;- selected_polygons\n}\n\n# Combine all selected polygons from different grades into one sf object\nfinal_selected_polygons &lt;- do.call(rbind, results)\n\n  # Define colors for the grades\n  grade_colors &lt;- c(\"A\" = \"grey\", \"B\" = \"grey\", \"C\" = \"grey\", \"D\" = \"grey\")\n\n  # Create the plot\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(data = layer1, fill = \"grey\", color = \"grey\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    geom_sf(data = final_selected_polygons,fill = \"green\", color = \"green\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    #scale_fill_manual(values = grade_colors) +\n    #scale_color_manual(values = grade_colors) +\n    theme_minimal() +\n    labs(fill = 'HOLC Grade') +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"none\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot as a high-resolution PNG file\n  ggsave(output_file, plot, width = 10, height = 4, units = \"in\", dpi = 1200)\n\n  # Return the plot for optional further use\n  return(list(plot=plot, sf = final_selected_polygons))\n}\n</code></pre>  FUNCTION: Create word cloud per grade  <pre><code>create_wordclouds_by_grade &lt;- function(sf_object, output_file = \"food_word_cloud_per_grade.png\",title = \"Healthy food place names word cloud\", max_size =25, col_select = \"name\") {\n\n\n    # Extract relevant data and prepare text data\n    text_data &lt;- sf_object %&gt;%\n        select(grade, col_select) %&gt;%\n        filter(!is.na(col_select)) %&gt;%\n        unnest_tokens(output = \"word\", input = col_select, token = \"words\") %&gt;%\n        count(grade, word, sort = TRUE) %&gt;%\n        ungroup() %&gt;%\n        filter(n() &gt; 1)  # Filter to remove overly common or single-occurrence words\n\n    # Ensure there are no NA values in the 'word' column\n    text_data &lt;- text_data %&gt;% filter(!is.na(word))\n\n    # Handle cases where text_data might be empty\n    if (nrow(text_data) == 0) {\n        stop(\"No data available for creating word clouds.\")\n    }\n\n    # Create a word cloud using ggplot2 and ggwordcloud\n    p &lt;- ggplot( ) +\n        geom_text_wordcloud_area(data=text_data, aes(label = word, size = n),rm_outside = TRUE) +\n        scale_size_area(max_size = max_size) +\n        facet_wrap(~ grade, nrow = 1) +\n      scale_color_gradient(low = \"darkred\", high = \"red\") +\n        theme_minimal() +\n        theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          panel.spacing = unit(0.5, \"lines\"),\n              plot.title = element_text(size = 16, face = \"bold\"),\n              legend.position = \"none\") +\n        labs(title = title)\n\n    # Attempt to save the plot and handle any errors\n    tryCatch({\n        ggsave(output_file, p, width = 10, height = 4, units = \"in\", dpi = 600)\n    }, error = function(e) {\n        cat(\"Error in saving the plot: \", e$message, \"\\n\")\n    })\n\n    return(p)\n}\n</code></pre>  Map food over each grade individually  <pre><code> layer1 &lt;- denver_redlining\n layer2 &lt;- food\n food_match &lt;- process_and_plot_sf_layers(layer1, layer2, \"food_match.png\")\n</code></pre>  WORD CLOUD: Names of places with fresh food  <pre><code>food_word_cloud &lt;- create_wordclouds_by_grade(food_match$sf, output_file = \"food_word_cloud_per_grade.png\")\n</code></pre>      Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.     \u2139 Please use `all_of()` or `any_of()` instead.       # Was:       data %&gt;% select(col_select)        # Now:       data %&gt;% select(all_of(col_select))      See .      Warning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :     Some words could not fit on page. They have been removed.    Map processed food over each grade individually  <pre><code> layer1 &lt;- denver_redlining\n layer2 &lt;- food_processed\n processed_food_match &lt;- process_and_plot_sf_layers(layer1, layer2, \"processed_food_match.png\")\n</code></pre>  WORD CLOUD: Names of places with processed food  <pre><code>processed_food_cloud &lt;- create_wordclouds_by_grade(processed_food_match$sf, output_file = \"processed_food_word_cloud_per_grade.png\",title = \"Processed food place names where larger text is more frequent\", max_size =17)\n</code></pre>"},{"location":"worksheets/worksheet_redlining/#part-2-integrating-environmental-data","title":"Part 2: Integrating Environmental Data","text":""},{"location":"worksheets/worksheet_redlining/#data-processing","title":"Data Processing","text":"<ul> <li>Use satellite data from 2010 to analyze greenspace using NDVI, an     index that measures the quantity of vegetation in an area.</li> <li>Apply methods to adjust for potential confounders as described in     the study, ensuring that comparisons of greenspace across HOLC     grades are valid and not biased by historical or socio-demographic     factors.</li> </ul>  Map natural habitats over each grade individually  <pre><code> layer1 &lt;- denver_redlining\n layer2 &lt;- natural_habitats\n natural_habitats_match &lt;- process_and_plot_sf_layers(layer1, layer2, \"natural_habitats_match.png\")\n print(natural_habitats_match$plot)\n</code></pre>  ![](redlining_figures/figure-gfm/unnamed-chunk-18-1.png)    WORD CLOUD: Name of natural habitat area  <pre><code>natural_habitats_cloud &lt;- create_wordclouds_by_grade(natural_habitats_match$sf, output_file = \"natural_habitats_word_cloud_per_grade.png\",title = \"Natural habitats place names where larger text is more frequent\", max_size =35)\n</code></pre>  FUNCTION: Stream NDVI data  <pre><code>polygon_layer &lt;- denver_redlining\n# Function to process satellite data based on an SF polygon's extent\nprocess_satellite_data &lt;- function(polygon_layer, start_date, end_date, assets, fps = 1, output_file = \"anim.gif\") {\n  # Record start time\n  start_time &lt;- Sys.time()\n\n  # Calculate the bbox from the polygon layer\n  bbox &lt;- st_bbox(polygon_layer)\n\n  s = stac(\"https://earth-search.aws.element84.com/v0\")\n\n\n  # Use stacR to search for Sentinel-2 images within the bbox and date range\n  items = s |&gt; stac_search(\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox = c(bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]),\n    datetime = paste(start_date, end_date, sep = \"/\"),\n    limit = 500\n  ) %&gt;% \n  post_request()\n\n  # Define mask for Sentinel-2 image quality\n  #S2.mask &lt;- image_mask(\"SCL\", values = c(3, 8, 9))\n\n  # Create a collection of images filtering by cloud cover\n  col &lt;- stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 30})\n\n  # Define a view for processing the data\n  v &lt;- cube_view(srs = \"EPSG:4326\", \n                 extent = list(t0 = start_date, t1 = end_date,\n                               left = bbox[\"xmin\"], right = bbox[\"xmax\"], \n                               top = bbox[\"ymax\"], bottom = bbox[\"ymin\"]),\n                 dx = 0.001, dy = 0.001, dt = \"P1M\", \n                 aggregation = \"median\", resampling = \"bilinear\")\n\n  # Calculate NDVI and create an animation\n  ndvi_col &lt;- function(n) {\n    rev(sequential_hcl(n, \"Green-Yellow\"))\n  }\n\n  #raster_cube(col, v, mask = S2.mask) %&gt;%\n  raster_cube(col, v) %&gt;%\n    select_bands(c(\"B04\", \"B08\")) %&gt;%\n    apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") %&gt;%\n    gdalcubes::animate(col = ndvi_col, zlim = c(-0.2, 1), key.pos = 1, save_as = output_file, fps = fps)\n\n  # Calculate processing time\n  end_time &lt;- Sys.time()\n  processing_time &lt;- difftime(end_time, start_time)\n\n  # Return processing time\n  return(processing_time)\n}\n</code></pre>  Stream NDVI data: animation  <pre><code>processing_time &lt;- process_satellite_data(denver_redlining, \"2022-05-31\", \"2023-05-31\", c(\"B04\", \"B08\"))\n</code></pre>  FUNCTION: Stream year average NDVI  <pre><code>yearly_average_ndvi &lt;- function(polygon_layer, output_file = \"ndvi.png\", dx = 0.01, dy = 0.01) {\n  # Record start time\n  start_time &lt;- Sys.time()\n\n  # Calculate the bbox from the polygon layer\n  bbox &lt;- st_bbox(polygon_layer)\n\n  s = stac(\"https://earth-search.aws.element84.com/v0\")\n\n  # Search for Sentinel-2 images within the bbox for June\n  items &lt;- s |&gt; stac_search(\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox = c(bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]),\n    datetime = \"2023-01-01/2023-12-31\",\n    limit = 500\n  ) %&gt;% \n  post_request()\n\n  # Create a collection of images filtering by cloud cover\n  col &lt;- stac_image_collection(items$features, asset_names = c(\"B04\", \"B08\"), property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 80})\n\n  # Define a view for processing the data specifically for June\n  v &lt;- cube_view(srs = \"EPSG:4326\", \n                 extent = list(t0 = \"2023-01-01\", t1 = \"2023-12-31\",\n                               left = bbox[\"xmin\"], right = bbox[\"xmax\"], \n                               top = bbox[\"ymax\"], bottom = bbox[\"ymin\"]),\n                 dx = dx, dy = dy, dt = \"P1Y\", \n                 aggregation = \"median\", resampling = \"bilinear\")\n\n  # Process NDVI\n  ndvi_rast &lt;- raster_cube(col, v) %&gt;%\n    select_bands(c(\"B04\", \"B08\")) %&gt;%\n    apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") %&gt;%\n    write_tif() |&gt;\n    terra::rast()\n\n\n  # Convert terra Raster to ggplot using tidyterra\nndvi_plot &lt;-   ggplot() +\n    geom_spatraster(data = ndvi_rast, aes(fill = NDVI)) +\n    scale_fill_viridis_c(option = \"viridis\", direction = -1, name = \"NDVI\") +\n    labs(title = \"NDVI mean for 2023\") +\n    theme_minimal() +\n    coord_sf() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank()) \n\n  # Save the plot as a high-resolution PNG file\n  ggsave(output_file, ndvi_plot, width = 10, height = 8, dpi = 600)\n\n  # Calculate processing time\n  end_time &lt;- Sys.time()\n  processing_time &lt;- difftime(end_time, start_time)\n\n  # Return the plot and processing time\n  return(list(plot = ndvi_plot, processing_time = processing_time, raster = ndvi_rast))\n}\n</code></pre>  Stream NDVI: high resolution  <pre><code>ndvi_background &lt;- yearly_average_ndvi(denver_redlining,dx = 0.0001, dy = 0.0001)\n</code></pre>  FUNCTION: Map NDVI per HOLC grade individually  <pre><code>create_mask_and_plot &lt;- function(redlining_sf, background_raster = ndvi$raster, roads = NULL, rivers = NULL){\n  start_time &lt;- Sys.time()  # Start timing\n\n  # Validate and prepare the redlining data\n  redlining_sf &lt;- redlining_sf %&gt;%\n    filter(grade != \"\") %&gt;%\n    st_make_valid()\n\n\nbbox &lt;- st_bbox(redlining_sf)  # Get original bounding box\n\n\nexpanded_bbox &lt;- expand_bbox(bbox, 6000, 1000)  # \n\n\nexpanded_bbox_poly &lt;- st_as_sfc(expanded_bbox, crs = st_crs(redlining_sf)) %&gt;%\n    st_make_valid()\n\n  # Initialize an empty list to store masks\n  masks &lt;- list()\n\n  # Iterate over each grade to create masks\n  unique_grades &lt;- unique(redlining_sf$grade)\n  for (grade in unique_grades) {\n    # Filter polygons by grade\n    grade_polygons &lt;- redlining_sf[redlining_sf$grade == grade, ]\n\n    # Create an \"inverted\" mask by subtracting these polygons from the background\n    mask &lt;- st_difference(expanded_bbox_poly, st_union(grade_polygons))\n\n    # Store the mask in the list with the grade as the name\n    masks[[grade]] &lt;- st_sf(geometry = mask, grade = grade)\n  }\n\n  # Combine all masks into a single sf object\n  mask_sf &lt;- do.call(rbind, masks)\n\n  # Normalize the grades so that C.2 becomes C, but correctly handle other grades\n  mask_sf$grade &lt;- ifelse(mask_sf$grade == \"C.2\", \"C\", mask_sf$grade)\n\n  # Prepare the plot\n  plot &lt;- ggplot() +\n    geom_spatraster(data = background_raster, aes(fill = NDVI)) +\n  scale_fill_viridis_c(name = \"NDVI\", option = \"viridis\", direction = -1) +\n\n    geom_sf(data = mask_sf, aes(color = grade), fill = \"white\", size = 0.1, show.legend = FALSE) +\n    scale_color_manual(values = c(\"A\" = \"white\", \"B\" = \"white\", \"C\" = \"white\", \"D\" = \"white\"), name = \"Grade\") +\n    facet_wrap(~ grade, nrow = 1) +\n     geom_sf(data = roads, alpha = 1, lwd = 0.1, color=\"white\") +\n    geom_sf(data = rivers, color = \"white\", alpha = 0.5, lwd = 1.1) +\n    labs(title = \"NDVI: Normalized Difference Vegetation Index\") +\n    theme_minimal() +\n    coord_sf(xlim = c(bbox[\"xmin\"], bbox[\"xmax\"]), \n           ylim = c(bbox[\"ymin\"], bbox[\"ymax\"]), \n           expand = FALSE) + \n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"bottom\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot\n  ggsave(\"redlining_mask_ndvi.png\", plot, width = 10, height = 4, dpi = 600)\n\n  end_time &lt;- Sys.time()  # End timing\n  runtime &lt;- end_time - start_time\n\n  # Return the plot and runtime\n  return(list(plot = plot, runtime = runtime, mask_sf = mask_sf))\n}\n</code></pre>  Stream NDVI: low resolution  <pre><code>ndvi_background_low &lt;- yearly_average_ndvi(denver_redlining)\n</code></pre>  Map low resolution NDVI per HOLC grade  <pre><code>ndvi &lt;- create_mask_and_plot(denver_redlining, background_raster = ndvi_background_low$raster, roads = roads, rivers = rivers)\n</code></pre>  FUNCTION: Map Denver City provided data per HOLC grade  <pre><code>process_city_inventory_data &lt;- function(address, inner_file, polygon_layer, output_filename,variable_label= 'Tree Density') {\n  # Download and read the shapefile\n  full_path &lt;- glue(\"/vsizip/vsicurl/{address}/{inner_file}\")\n  shape_data &lt;- st_read(full_path, quiet = TRUE) |&gt; st_as_sf()\n\n  # Process the shape data with the provided polygon layer\n  processed_data &lt;- process_and_plot_sf_layers(polygon_layer, shape_data, paste0(output_filename, \".png\"))\n\n  # Extract trees from the processed data\n  trees &lt;- processed_data$sf\n  denver_redlining_residential &lt;- polygon_layer |&gt; filter(grade != \"\")\n\n  # Generate the density plot\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(data = denver_redlining_residential, fill = \"grey\", color = \"grey\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    stat_density_2d(data = trees, \n                    mapping = aes(x = map_dbl(geometry, ~.[1]),\n                                  y = map_dbl(geometry, ~.[2]),\n                                  fill = stat(density)),\n                    geom = 'tile',\n                    contour = FALSE,\n                    alpha = 0.9) +\n    scale_fill_gradientn(colors = c(\"transparent\", \"white\", \"limegreen\"),\n                         values = scales::rescale(c(0, 0.1, 1)),  # Adjust these based on your density range\n                         guide = \"colourbar\") +\n    theme_minimal() +\n    labs(fill = variable_label) +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"bottom\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot\n  ggsave(paste0(output_filename, \"_density_plot.png\"), plot, width = 10, height = 4, units = \"in\", dpi = 600)\n\n  # Return the plot and the tree layer\n  return(list(plot = plot, layer = trees))\n}\n</code></pre>  Map tree inventory per HOLC grade  <pre><code>result &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/tree_inventory/shape/tree_inventory.zip\",\n  \"tree_inventory.shp\",\n  denver_redlining,\n  \"Denver_tree_inventory_2023\"\n)\n</code></pre>      Warning: `stat(density)` was deprecated in ggplot2 3.4.0.     \u2139 Please use `after_stat(density)` instead.    Map traffic accidents per HOLC grade  <pre><code>result &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/traffic_accidents/shape/traffic_accidents.zip\",\n  \"traffic_accidents.shp\",\n  denver_redlining,\n  \"Denver_traffic_accidents\",\n  variable_label= 'Traffic accidents density'\n)\n</code></pre>  Map stream sampling effort per HOLC grade  <pre><code>instream_sampling_sites &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/instream_sampling_sites/shape/instream_sampling_sites.zip\",\n  \"instream_sampling_sites.shp\",\n  denver_redlining,\n  \"instream_sampling_sites\",\n  variable_label= 'Instream sampling sites density'\n)\n</code></pre>  Map soil sampling effort per HOLC grade  <pre><code>soil_samples &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/soil_samples/shape/soil_samples.zip\",\n  \"soil_samples.shp\",\n  denver_redlining,\n  \"Soil samples\",\n  variable_label= 'soil samples density'\n)\n</code></pre>  Map public art density per HOLC grade  <pre><code>public_art &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/public_art/shape/public_art.zip\",\n  \"public_art.shp\",\n  denver_redlining,\n  \"Public art \",\n  variable_label= 'Public art density'\n)\n</code></pre>  Map liquor licenses density per HOLC grade  <pre><code>liquor_licenses &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/liquor_licenses/shape/liquor_licenses.zip\",\n  \"liquor_licenses.shp\",\n  denver_redlining,\n  \"liquor licenses \",\n  variable_label= 'liquor licenses density'\n)\n</code></pre>  Map crime density per HOLC grade  <pre><code>Crime &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/crime/shape/crime.zip\",\n  \"crime.shp\",\n  denver_redlining,\n  \"crime\",\n  variable_label= 'Crime density'\n)\n</code></pre>  WORD CLOUD: Types of crimes  <pre><code>crime_cloud &lt;- create_wordclouds_by_grade(Crime$layer, output_file = \"Crime_word_cloud_per_grade.png\",title = \"Crime type where larger text is more frequent\", max_size =25, col_select = \"OFFENSE_TY\")\n</code></pre>      Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.     \u2139 Please use `all_of()` or `any_of()` instead.       # Was:       data %&gt;% select(col_select)        # Now:       data %&gt;% select(all_of(col_select))      See .    Map police shooting density per HOLC grade  <pre><code>Denver_police_shootings &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/denver_police_officer_involved_shootings/shape/denver_police_officer_involved_shootings.zip\",\n  \"denver_police_officer_involved_shootings.shp\",\n  denver_redlining,\n  \"Police shootings\",\n  variable_label= 'Police shootings density'\n)\n</code></pre> <p>Not enough data for density across all 4</p>  WORD CLOUD: Police involved shootings  <pre><code>Denver_police_shootings_cloud &lt;- create_wordclouds_by_grade(Denver_police_shootings$layer, output_file = \"police_shootings_word_cloud_per_grade.png\",title = \"police involved shooting per crime type where larger text is more frequent\", max_size =35, col_select = \"SHOOT_ACTI\")\n</code></pre> <p></p>"},{"location":"worksheets/worksheet_redlining/#part-3-comparative-analysis-and-visualization","title":"Part 3: Comparative Analysis and Visualization","text":""},{"location":"worksheets/worksheet_redlining/#statistical-analysis","title":"Statistical Analysis","text":"<ul> <li>Conduct a detailed statistical analysis to compare greenspace across     different HOLC grades, using techniques like Targeted Maximum     Likelihood Estimation (TMLE) to assess the association between     historical redlining and current greenspace levels.</li> <li>Visualize the disparities in greenspace distribution using GIS     tools, highlighting how redlining has shaped urban ecological     landscapes.</li> </ul>"},{"location":"worksheets/worksheet_redlining/#conclusion","title":"Conclusion","text":"<p>This tutorial provides tools and methodologies to explore the lingering effects of historic redlining on urban greenspace, offering insights into the intersection of urban planning, environmental justice, and public health.</p>"},{"location":"worksheets/worksheet_redlining/#references_1","title":"References","text":"<ul> <li>Nardone, A., Rudolph, K. E., Morello-Frosch, R., &amp; Casey, J. A.     (2021). Redlines and Greenspace: The Relationship between Historical     Redlining and 2010 Greenspace across the United States.     Environmental Health Perspectives, 129(1), 017006.     DOI:10.1289/EHP7495. Available     online</li> </ul>"},{"location":"worksheets/worksheet_redlining_student_edition/","title":"student edition","text":"R libraries we use in this analysis  <pre><code>if (!requireNamespace(\"tidytext\", quietly = TRUE)) {\n  install.packages(\"tidytext\")\n}\nlibrary(tidytext)\n## Warning: package 'tidytext' was built under R version 4.3.2\nlibrary(sf)\n## Warning: package 'sf' was built under R version 4.3.2\n## Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\nlibrary(ggplot2)\n## Warning: package 'ggplot2' was built under R version 4.3.2\nlibrary(ggthemes)\n## Warning: package 'ggthemes' was built under R version 4.3.2\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(rstac)\n## Warning: package 'rstac' was built under R version 4.3.2\nlibrary(gdalcubes)\n## Warning: package 'gdalcubes' was built under R version 4.3.2\nlibrary(gdalUtils)\n## Please note that rgdal will be retired during October 2023,\n## plan transition to sf/stars/terra functions using GDAL and PROJ\n## at your earliest convenience.\n## See https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\n## rgdal: version: 1.6-7, (SVN revision 1203)\n## Geospatial Data Abstraction Library extensions to R successfully loaded\n## Loaded GDAL runtime: GDAL 3.5.3, released 2022/10/21\n## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/rgdal/gdal\n##  GDAL does not use iconv for recoding strings.\n## GDAL binary built with GEOS: TRUE \n## Loaded PROJ runtime: Rel. 9.1.0, September 1st, 2022, [PJ_VERSION: 910]\n## Path to PROJ shared files: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/gdalcubes/proj\n## PROJ CDN enabled: FALSE\n## Linking to sp version:1.6-1\n## To mute warnings of possible GDAL/OSR exportToProj4() degradation,\n## use options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n## \n## Attaching package: 'gdalUtils'\n## The following object is masked from 'package:sf':\n## \n##     gdal_rasterize\nlibrary(gdalcubes)\nlibrary(colorspace)\nlibrary(terra)\n## Warning: package 'terra' was built under R version 4.3.2\n## terra 1.7.71\n## \n## Attaching package: 'terra'\n## The following object is masked from 'package:colorspace':\n## \n##     RGB\n## The following objects are masked from 'package:gdalcubes':\n## \n##     animate, crop, size\nlibrary(tidyterra)\n## \n## Attaching package: 'tidyterra'\n## The following object is masked from 'package:stats':\n## \n##     filter\nlibrary(basemapR)\nlibrary(tidytext)\nlibrary(ggwordcloud)\nlibrary(osmextract)\n## Data (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\n## Check the package website, https://docs.ropensci.org/osmextract/, for more details.\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(glue)\n## \n## Attaching package: 'glue'\n## The following object is masked from 'package:terra':\n## \n##     trim\n\nlibrary(purrr)\n</code></pre>  FUNCTION: Stream HOLC data from a city  <pre><code># Function to load and filter redlining data by city\nload_city_redlining_data &lt;- function(city_name) {\n  # URL to the GeoJSON data\n  url &lt;- \"https://raw.githubusercontent.com/americanpanorama/mapping-inequality-census-crosswalk/main/MIv3Areas_2010TractCrosswalk.geojson\"\n\n  # Read the GeoJSON file into an sf object\n  redlining_data &lt;- read_sf(url)\n\n  # Filter the data for the specified city and non-empty grades\n\n  city_redline &lt;- redlining_data %&gt;%\n    filter(city == city_name )\n\n  # Return the filtered data\n  return(city_redline)\n}\n</code></pre>  Stream HOLC data for Denver, CO  <pre><code># Load redlining data for Denver\ndenver_redlining &lt;- load_city_redlining_data(\"Denver\")\nknitr::kable(head(denver_redlining), format = \"markdown\")\n</code></pre>  | area_id | city   | state | city_survey | cat  | grade | label | res  | com   | ind   | fill     | GEOID10     | GISJOIN        |    calc_area | pct_tract | geometry                     | |--------:|:-------|:------|:------------|:-----|:------|:------|:-----|:------|:------|:---------|:------------|:---------------|-------------:|----------:|:-----------------------------| |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004104 | G0800310004104 | 1.525535e+01 |   0.00001 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004201 | G0800310004201 | 3.987458e+05 |   0.20900 | MULTIPOLYGON (((-104.9246 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004304 | G0800310004304 | 1.554195e+05 |   0.05927 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6525 | Denver | CO    | TRUE        | Best | A     | A1    | TRUE | FALSE | FALSE | \\#76a865 | 08031004202 | G0800310004202 | 1.117770e+06 |   0.57245 | MULTIPOLYGON (((-104.9125 3\u2026 | |    6529 | Denver | CO    | TRUE        | Best | A     | A2    | TRUE | FALSE | FALSE | \\#76a865 | 08031004302 | G0800310004302 | 3.133415e+05 |   0.28381 | MULTIPOLYGON (((-104.928 39\u2026 | |    6529 | Denver | CO    | TRUE        | Best | A     | A2    | TRUE | FALSE | FALSE | \\#76a865 | 08031004301 | G0800310004301 | 1.221218e+05 |   0.08622 | MULTIPOLYGON (((-104.9305 3\u2026 |    FUNCTION: Get Points-of-Interest from city of interest  <pre><code>get_places &lt;- function(polygon_layer, type = \"food\" ) {\n  # Check if the input is an sf object\n  if (!inherits(polygon_layer, \"sf\")) {\n    stop(\"The provided object is not an sf object.\")\n  }\n\n  # Create a bounding box from the input sf object\n  bbox_here &lt;- st_bbox(polygon_layer) |&gt;\n    st_as_sfc()\n\n   if(type == \"roads\"){\n    my_layer &lt;- \"lines\"\n    my_query &lt;- \"SELECT * FROM lines WHERE (\n             highway IN ('motorway', 'trunk', 'primary', 'secondary', 'tertiary') )\"\n    title &lt;- \"Major roads\"\n   }\n\n  if(type == \"rivers\"){\n    my_layer &lt;- \"lines\"\n    my_query &lt;- \"SELECT * FROM lines WHERE (\n             waterway IN ('river'))\"\n    title &lt;- \"Major rivers\"\n  }\n\n\n\n  # Use the bbox to get data with oe_get(), specifying the desired layer and a custom SQL query for fresh food places\n  tryCatch({\n    places &lt;- oe_get(\n      place = bbox_here,\n      layer = my_layer,  # Adjusted layer; change as per actual data availability\n      query = my_query,\n      quiet = TRUE\n    )\n\n  places &lt;- st_make_valid(places)\n\n    # Crop the data to the bounding box\n    cropped_places &lt;- st_crop(places, bbox_here)\n\n    # Plotting the cropped fresh food places\n    plot &lt;- ggplot(data = cropped_places) +\n      geom_sf(fill=\"cornflowerblue\", color=\"cornflowerblue\") +\n      ggtitle(title) +\n  theme_tufte()+\n  theme(legend.position = \"none\",  # Optionally hide the legend\n        axis.text = element_blank(),     # Remove axis text\n        axis.title = element_blank(),    # Remove axis titles\n        axis.ticks = element_blank(),    # Remove axis ticks\n         plot.background = element_rect(fill = \"white\", color = NA),  # Set the plot background to white\n        panel.background = element_rect(fill = \"white\", color = NA),  # Set the panel background to white\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),\n        ) \n\n    # Save the plot as a PNG file\n    png_filename &lt;- paste0(title,\"_\", Sys.Date(), \".png\")\n    ggsave(png_filename, plot, width = 10, height = 8, units = \"in\")\n\n    # Return the cropped dataset\n    return(cropped_places)\n  }, error = function(e) {\n    stop(\"Failed to retrieve or plot data: \", e$message)\n  })\n}\n</code></pre>  Stream amenities by category  <pre><code>roads &lt;- get_places(denver_redlining, type=\"roads\")\n\nrivers &lt;- get_places(denver_redlining, type=\"rivers\")\n</code></pre>  FUNCTION: Plot POI over HOLC grades  <pre><code>plot_city_redlining &lt;- function(redlining_data, filename = \"redlining_plot.png\") {\n  # Fetch additional geographic data based on redlining data\n  roads &lt;- get_places(redlining_data, type = \"roads\")\n  rivers &lt;- get_places(redlining_data, type = \"rivers\")\n\n  # Filter residential zones with valid grades and where city survey is TRUE\n  residential_zones &lt;- redlining_data %&gt;%\n    filter(city_survey == TRUE &amp; grade != \"\") \n\n  # Colors for the grades\n  colors &lt;- c(\"#76a865\", \"#7cb5bd\", \"#ffff00\", \"#d9838d\")\n\n  # Plot the data using ggplot2\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.5, lwd = 1.1) +\n    geom_sf(data = residential_zones, aes(fill = grade), alpha = 0.5) +\n    theme_tufte() +\n    scale_fill_manual(values = colors) +\n    labs(fill = 'HOLC Categories') +\n    theme(\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      legend.position = \"right\"\n    )\n\n  # Save the plot as a high-resolution PNG file\n  ggsave(filename, plot, width = 10, height = 8, units = \"in\", dpi = 600)\n\n  # Return the plot object if needed for further manipulation or checking\n  return(plot)\n}\n</code></pre>  FUNCTION: Plot the HOLC grades individually  <pre><code>split_plot &lt;- function(sf_data, roads, rivers) {\n  # Filter for grades A, B, C, and D\n  sf_data_filtered &lt;- sf_data %&gt;% \n    filter(grade %in% c('A', 'B', 'C', 'D'))\n\n  # Define a color for each grade\n  grade_colors &lt;- c(\"A\" = \"#76a865\", \"B\" = \"#7cb5bd\", \"C\" = \"#ffff00\", \"D\" = \"#d9838d\")\n\n  # Create the plot with panels for each grade\n  plot &lt;- ggplot(data = sf_data_filtered) +\n    geom_sf(data = roads, alpha = 0.1, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(aes(fill = grade)) +\n    facet_wrap(~ grade, nrow = 1) +  # Free scales for different zoom levels if needed\n    scale_fill_manual(values = grade_colors) +\n    theme_minimal() +\n    labs(fill = 'HOLC Grade') +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"none\",  # Optionally hide the legend\n          axis.text = element_blank(),     # Remove axis text\n          axis.title = element_blank(),    # Remove axis titles\n          axis.ticks = element_blank(),    # Remove axis ticks\n          panel.grid.major = element_blank(),  # Remove major grid lines\n          panel.grid.minor = element_blank())  \n\n  ggsave(plot, filename = \"HOLC_grades_individually.png\", width = 10, height = 4, units = \"in\", dpi = 1200)\n  return(plot)\n}\n</code></pre>  Plot Denver Redlining  <pre><code>denver_plot &lt;- plot_city_redlining(denver_redlining)\ndenver_plot\n</code></pre>  ![](redlining_student_figures/figure-gfm/unnamed-chunk-8-1.png)    Plot 4 HOLC grades individually  <pre><code>plot_row &lt;- split_plot(denver_redlining, roads, rivers)\nplot_row\n</code></pre>  ![](redlining_student_figures/figure-gfm/unnamed-chunk-9-1.png)    FUNCTION: Map an amenity over each grade individually  <pre><code>process_and_plot_sf_layers &lt;- function(layer1, layer2, output_file = \"output_plot.png\") {\n # Make geometries valid\nlayer1 &lt;- st_make_valid(layer1)\nlayer2 &lt;- st_make_valid(layer2)\n\n# Optionally, simplify geometries to remove duplicate vertices\nlayer1 &lt;- st_simplify(layer1, preserveTopology = TRUE) |&gt;\n  filter(grade != \"\")\n\n# Prepare a list to store results\nresults &lt;- list()\n\n# Loop through each grade and perform operations\nfor (grade in c(\"A\", \"B\", \"C\", \"D\")) {\n  # Filter layer1 for current grade\n  layer1_grade &lt;- layer1[layer1$grade == grade, ]\n\n  # Buffer the geometries of the current grade\n  buffered_layer1_grade &lt;- st_buffer(layer1_grade, dist = 500)\n\n  # Intersect with the second layer\n  intersections &lt;- st_intersects(layer2, buffered_layer1_grade, sparse = FALSE)\n  selected_polygons &lt;- layer2[rowSums(intersections) &gt; 0, ]\n\n  # Add a new column to store the grade information\n  selected_polygons$grade &lt;- grade\n\n  # Store the result\n  results[[grade]] &lt;- selected_polygons\n}\n\n# Combine all selected polygons from different grades into one sf object\nfinal_selected_polygons &lt;- do.call(rbind, results)\n\n  # Define colors for the grades\n  grade_colors &lt;- c(\"A\" = \"grey\", \"B\" = \"grey\", \"C\" = \"grey\", \"D\" = \"grey\")\n\n  # Create the plot\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(data = layer1, fill = \"grey\", color = \"grey\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    geom_sf(data = final_selected_polygons,fill = \"green\", color = \"green\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    #scale_fill_manual(values = grade_colors) +\n    #scale_color_manual(values = grade_colors) +\n    theme_minimal() +\n    labs(fill = 'HOLC Grade') +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"none\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot as a high-resolution PNG file\n  ggsave(output_file, plot, width = 10, height = 4, units = \"in\", dpi = 1200)\n\n  # Return the plot for optional further use\n  return(list(plot=plot, sf = final_selected_polygons))\n}\n</code></pre>"},{"location":"worksheets/worksheet_redlining_student_edition/#part-2-integrating-environmental-data","title":"Part 2: Integrating Environmental Data","text":""},{"location":"worksheets/worksheet_redlining_student_edition/#data-processing","title":"Data Processing","text":"<ul> <li>Use satellite data from 2010 to analyze greenspace using NDVI, an     index that measures the quantity of vegetation in an area.</li> <li>Apply methods to adjust for potential confounders as described in     the study, ensuring that comparisons of greenspace across HOLC     grades are valid and not biased by historical or socio-demographic     factors.</li> </ul>  FUNCTION: Stream NDVI data  <pre><code>polygon_layer &lt;- denver_redlining\n# Function to process satellite data based on an SF polygon's extent\nprocess_satellite_data &lt;- function(polygon_layer, start_date, end_date, assets, fps = 1, output_file = \"anim.gif\") {\n  # Record start time\n  start_time &lt;- Sys.time()\n\n  # Calculate the bbox from the polygon layer\n  bbox &lt;- st_bbox(polygon_layer)\n\n  s = stac(\"https://earth-search.aws.element84.com/v0\")\n\n\n  # Use stacR to search for Sentinel-2 images within the bbox and date range\n  items = s |&gt; stac_search(\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox = c(bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]),\n    datetime = paste(start_date, end_date, sep = \"/\"),\n    limit = 500\n  ) %&gt;% \n  post_request()\n\n  # Define mask for Sentinel-2 image quality\n  #S2.mask &lt;- image_mask(\"SCL\", values = c(3, 8, 9))\n\n  # Create a collection of images filtering by cloud cover\n  col &lt;- stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 30})\n\n  # Define a view for processing the data\n  v &lt;- cube_view(srs = \"EPSG:4326\", \n                 extent = list(t0 = start_date, t1 = end_date,\n                               left = bbox[\"xmin\"], right = bbox[\"xmax\"], \n                               top = bbox[\"ymax\"], bottom = bbox[\"ymin\"]),\n                 dx = 0.001, dy = 0.001, dt = \"P1M\", \n                 aggregation = \"median\", resampling = \"bilinear\")\n\n  # Calculate NDVI and create an animation\n  ndvi_col &lt;- function(n) {\n    rev(sequential_hcl(n, \"Green-Yellow\"))\n  }\n\n  #raster_cube(col, v, mask = S2.mask) %&gt;%\n  raster_cube(col, v) %&gt;%\n    select_bands(c(\"B04\", \"B08\")) %&gt;%\n    apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") %&gt;%\n    gdalcubes::animate(col = ndvi_col, zlim = c(-0.2, 1), key.pos = 1, save_as = output_file, fps = fps)\n\n  # Calculate processing time\n  end_time &lt;- Sys.time()\n  processing_time &lt;- difftime(end_time, start_time)\n\n  # Return processing time\n  return(processing_time)\n}\n</code></pre>  Stream NDVI data: animation  <pre><code>processing_time &lt;- process_satellite_data(denver_redlining, \"2022-05-31\", \"2023-05-31\", c(\"B04\", \"B08\"))\n</code></pre>  FUNCTION: Map NDVI per HOLC grade individually  <pre><code>create_mask_and_plot &lt;- function(redlining_sf, background_raster = ndvi$raster, roads = NULL, rivers = NULL){\n  start_time &lt;- Sys.time()  # Start timing\n\n  # Validate and prepare the redlining data\n  redlining_sf &lt;- redlining_sf %&gt;%\n    filter(grade != \"\") %&gt;%\n    st_make_valid()\n\n\nbbox &lt;- st_bbox(redlining_sf)  # Get original bounding box\n\n\nexpanded_bbox &lt;- expand_bbox(bbox, 6000, 1000)  # \n\n\nexpanded_bbox_poly &lt;- st_as_sfc(expanded_bbox, crs = st_crs(redlining_sf)) %&gt;%\n    st_make_valid()\n\n  # Initialize an empty list to store masks\n  masks &lt;- list()\n\n  # Iterate over each grade to create masks\n  unique_grades &lt;- unique(redlining_sf$grade)\n  for (grade in unique_grades) {\n    # Filter polygons by grade\n    grade_polygons &lt;- redlining_sf[redlining_sf$grade == grade, ]\n\n    # Create an \"inverted\" mask by subtracting these polygons from the background\n    mask &lt;- st_difference(expanded_bbox_poly, st_union(grade_polygons))\n\n    # Store the mask in the list with the grade as the name\n    masks[[grade]] &lt;- st_sf(geometry = mask, grade = grade)\n  }\n\n  # Combine all masks into a single sf object\n  mask_sf &lt;- do.call(rbind, masks)\n\n  # Normalize the grades so that C.2 becomes C, but correctly handle other grades\n  mask_sf$grade &lt;- ifelse(mask_sf$grade == \"C.2\", \"C\", mask_sf$grade)\n\n  # Prepare the plot\n  plot &lt;- ggplot() +\n    geom_spatraster(data = background_raster, aes(fill = NDVI)) +\n  scale_fill_viridis_c(name = \"NDVI\", option = \"viridis\", direction = -1) +\n\n    geom_sf(data = mask_sf, aes(color = grade), fill = \"white\", size = 0.1, show.legend = FALSE) +\n    scale_color_manual(values = c(\"A\" = \"white\", \"B\" = \"white\", \"C\" = \"white\", \"D\" = \"white\"), name = \"Grade\") +\n    facet_wrap(~ grade, nrow = 1) +\n     geom_sf(data = roads, alpha = 1, lwd = 0.1, color=\"white\") +\n    geom_sf(data = rivers, color = \"white\", alpha = 0.5, lwd = 1.1) +\n    labs(title = \"NDVI: Normalized Difference Vegetation Index\") +\n    theme_minimal() +\n    coord_sf(xlim = c(bbox[\"xmin\"], bbox[\"xmax\"]), \n           ylim = c(bbox[\"ymin\"], bbox[\"ymax\"]), \n           expand = FALSE) + \n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"bottom\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot\n  ggsave(\"redlining_mask_ndvi.png\", plot, width = 10, height = 4, dpi = 600)\n\n  end_time &lt;- Sys.time()  # End timing\n  runtime &lt;- end_time - start_time\n\n  # Return the plot and runtime\n  return(list(plot = plot, runtime = runtime, mask_sf = mask_sf))\n}\n</code></pre>  FUNCTION: Stream year average NDVI  <pre><code>yearly_average_ndvi &lt;- function(polygon_layer, output_file = \"ndvi.png\", dx = 0.01, dy = 0.01) {\n  # Record start time\n  start_time &lt;- Sys.time()\n\n  # Calculate the bbox from the polygon layer\n  bbox &lt;- st_bbox(polygon_layer)\n\n  s = stac(\"https://earth-search.aws.element84.com/v0\")\n\n  # Search for Sentinel-2 images within the bbox for June\n  items &lt;- s |&gt; stac_search(\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox = c(bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]),\n    datetime = \"2023-01-01/2023-12-31\",\n    limit = 500\n  ) %&gt;% \n  post_request()\n\n  # Create a collection of images filtering by cloud cover\n  col &lt;- stac_image_collection(items$features, asset_names = c(\"B04\", \"B08\"), property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 80})\n\n  # Define a view for processing the data specifically for June\n  v &lt;- cube_view(srs = \"EPSG:4326\", \n                 extent = list(t0 = \"2023-01-01\", t1 = \"2023-12-31\",\n                               left = bbox[\"xmin\"], right = bbox[\"xmax\"], \n                               top = bbox[\"ymax\"], bottom = bbox[\"ymin\"]),\n                 dx = dx, dy = dy, dt = \"P1Y\", \n                 aggregation = \"median\", resampling = \"bilinear\")\n\n  # Process NDVI\n  ndvi_rast &lt;- raster_cube(col, v) %&gt;%\n    select_bands(c(\"B04\", \"B08\")) %&gt;%\n    apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") %&gt;%\n    write_tif() |&gt;\n    terra::rast()\n\n\n  # Convert terra Raster to ggplot using tidyterra\nndvi_plot &lt;-   ggplot() +\n    geom_spatraster(data = ndvi_rast, aes(fill = NDVI)) +\n    scale_fill_viridis_c(option = \"viridis\", direction = -1, name = \"NDVI\") +\n    labs(title = \"NDVI mean for 2023\") +\n    theme_minimal() +\n    coord_sf() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank()) \n\n  # Save the plot as a high-resolution PNG file\n  ggsave(output_file, ndvi_plot, width = 10, height = 8, dpi = 600)\n\n  # Calculate processing time\n  end_time &lt;- Sys.time()\n  processing_time &lt;- difftime(end_time, start_time)\n\n  # Return the plot and processing time\n  return(list(plot = ndvi_plot, processing_time = processing_time, raster = ndvi_rast))\n}\n</code></pre>  Stream NDVI: low resolution  <pre><code>ndvi_background_low &lt;- yearly_average_ndvi(denver_redlining)\n</code></pre>  Map low resolution NDVI per HOLC grade  <pre><code>ndvi &lt;- create_mask_and_plot(denver_redlining, background_raster = ndvi_background_low$raster, roads = roads, rivers = rivers)\n</code></pre>  FUNCTION: Map Denver City provided data per HOLC grade  <pre><code>process_city_inventory_data &lt;- function(address, inner_file, polygon_layer, output_filename,variable_label= 'Tree Density') {\n  # Download and read the shapefile\n  full_path &lt;- glue(\"/vsizip/vsicurl/{address}/{inner_file}\")\n  shape_data &lt;- st_read(full_path, quiet = TRUE) |&gt; st_as_sf()\n\n  # Process the shape data with the provided polygon layer\n  processed_data &lt;- process_and_plot_sf_layers(polygon_layer, shape_data, paste0(output_filename, \".png\"))\n\n  # Extract trees from the processed data\n  trees &lt;- processed_data$sf\n  denver_redlining_residential &lt;- polygon_layer |&gt; filter(grade != \"\")\n\n  # Generate the density plot\n  plot &lt;- ggplot() +\n    geom_sf(data = roads, alpha = 0.05, lwd = 0.1) +\n    geom_sf(data = rivers, color = \"blue\", alpha = 0.1, lwd = 1.1) +\n    geom_sf(data = denver_redlining_residential, fill = \"grey\", color = \"grey\", size = 0.1) +\n    facet_wrap(~ grade, nrow = 1) +\n    stat_density_2d(data = trees, \n                    mapping = aes(x = map_dbl(geometry, ~.[1]),\n                                  y = map_dbl(geometry, ~.[2]),\n                                  fill = stat(density)),\n                    geom = 'tile',\n                    contour = FALSE,\n                    alpha = 0.9) +\n    scale_fill_gradientn(colors = c(\"transparent\", \"white\", \"limegreen\"),\n                         values = scales::rescale(c(0, 0.1, 1)),  # Adjust these based on your density range\n                         guide = \"colourbar\") +\n    theme_minimal() +\n    labs(fill = variable_label) +\n    theme_tufte() +\n    theme(plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA),\n          legend.position = \"bottom\",\n          axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank())\n\n  # Save the plot\n  ggsave(paste0(output_filename, \"_density_plot.png\"), plot, width = 10, height = 4, units = \"in\", dpi = 600)\n\n  # Return the plot and the tree layer\n  return(list(plot = plot, layer = trees))\n}\n</code></pre>  Map tree inventory per HOLC grade  <pre><code>result &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/tree_inventory/shape/tree_inventory.zip\",\n  \"tree_inventory.shp\",\n  denver_redlining,\n  \"Denver_tree_inventory_2023\"\n)\n</code></pre>      Warning: `stat(density)` was deprecated in ggplot2 3.4.0.     \u2139 Please use `after_stat(density)` instead.    Map traffic accidents per HOLC grade  <pre><code>result &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/traffic_accidents/shape/traffic_accidents.zip\",\n  \"traffic_accidents.shp\",\n  denver_redlining,\n  \"Denver_traffic_accidents\",\n  variable_label= 'Traffic accidents density'\n)\n</code></pre>  Map stream sampling effort per HOLC grade  <pre><code>instream_sampling_sites &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/instream_sampling_sites/shape/instream_sampling_sites.zip\",\n  \"instream_sampling_sites.shp\",\n  denver_redlining,\n  \"instream_sampling_sites\",\n  variable_label= 'Instream sampling sites density'\n)\n</code></pre>  Map soil sampling effort per HOLC grade  <pre><code>soil_samples &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/soil_samples/shape/soil_samples.zip\",\n  \"soil_samples.shp\",\n  denver_redlining,\n  \"Soil samples\",\n  variable_label= 'soil samples density'\n)\n</code></pre>  Map public art density per HOLC grade  <pre><code>public_art &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/public_art/shape/public_art.zip\",\n  \"public_art.shp\",\n  denver_redlining,\n  \"Public art \",\n  variable_label= 'Public art density'\n)\n</code></pre>  Map liquor licenses density per HOLC grade  <pre><code>liquor_licenses &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/liquor_licenses/shape/liquor_licenses.zip\",\n  \"liquor_licenses.shp\",\n  denver_redlining,\n  \"liquor licenses \",\n  variable_label= 'liquor licenses density'\n)\n</code></pre>  Map crime density per HOLC grade  <pre><code>Crime &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/crime/shape/crime.zip\",\n  \"crime.shp\",\n  denver_redlining,\n  \"crime\",\n  variable_label= 'Crime density'\n)\n</code></pre>  Map police shooting density per HOLC grade  <pre><code>Denver_police_shootings &lt;- process_city_inventory_data(\n  \"https://www.denvergov.org/media/gis/DataCatalog/denver_police_officer_involved_shootings/shape/denver_police_officer_involved_shootings.zip\",\n  \"denver_police_officer_involved_shootings.shp\",\n  denver_redlining,\n  \"Police shootings\",\n  variable_label= 'Police shootings density'\n)\n</code></pre>"},{"location":"worksheets/worksheet_redlining_student_edition/#part-3-comparative-analysis-and-visualization","title":"Part 3: Comparative Analysis and Visualization","text":""},{"location":"worksheets/worksheet_redlining_student_edition/#statistical-analysis","title":"Statistical Analysis","text":"<ul> <li>Conduct a detailed statistical analysis to compare greenspace across     different HOLC grades, using techniques like Targeted Maximum     Likelihood Estimation (TMLE) to assess the association between     historical redlining and current greenspace levels.</li> <li>Visualize the disparities in greenspace distribution using GIS     tools, highlighting how redlining has shaped urban ecological     landscapes.</li> </ul>"},{"location":"worksheets/worksheet_redlining_student_edition/#conclusion","title":"Conclusion","text":"<p>This tutorial provides tools and methodologies to explore the lingering effects of historic redlining on urban greenspace, offering insights into the intersection of urban planning, environmental justice, and public health.</p>"},{"location":"worksheets/worksheet_redlining_student_edition/#references","title":"References","text":"<ul> <li>Nardone, A., Rudolph, K. E., Morello-Frosch, R., &amp; Casey, J. A.     (2021). Redlines and Greenspace: The Relationship between Historical     Redlining and 2010 Greenspace across the United States.     Environmental Health Perspectives, 129(1), 017006.     DOI:10.1289/EHP7495. Available     online</li> </ul>"},{"location":"tags/","title":"Tags","text":"<ul> <li>analytics</li> <li>Bayesian</li> <li>biomass</li> <li>burn-severity</li> <li>climate</li> <li>cloud</li> <li>collaboration</li> <li>container</li> <li>contribution</li> <li>data library</li> <li>data-cubes</li> <li>data-management</li> <li>development</li> <li>disturbance</li> <li>docker</li> <li>drought</li> <li>ecoregions</li> <li>education</li> <li>epa</li> <li>example</li> <li>featured</li> <li>fia</li> <li>fire</li> <li>forest</li> <li>gdal</li> <li>gedi</li> <li>GLMM</li> <li>INLA</li> <li>inventory</li> <li>landcover</li> <li>landfire</li> <li>LGM</li> <li>lidar</li> <li>modis</li> <li>oasis</li> <li>quickstart</li> <li>R</li> <li>raster</li> <li>remote-sensing</li> <li>schedule</li> <li>sentinel-2</li> <li>spatial</li> <li>spatiotemporal</li> <li>stac</li> <li>streaming</li> <li>treemap</li> <li>tutorial</li> <li>usage</li> <li>usgs</li> <li>vegetation</li> <li>vsi</li> </ul>"},{"location":"tags/#crt-triangle","title":"CRT triangle","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> </ul>"},{"location":"tags/#gdal","title":"GDAL","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#stac","title":"STAC","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#vsi","title":"VSI","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> </ul>"},{"location":"tags/#cloud","title":"cloud","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#collaboration","title":"collaboration","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#container","title":"container","text":"<ul> <li>Example Container</li> </ul>"},{"location":"tags/#cyverse","title":"cyverse","text":"<ul> <li>Cloud Reproducibility Triangle (CRT)</li> </ul>"},{"location":"tags/#data-library","title":"data library","text":"<ul> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#development","title":"development","text":"<ul> <li>Development Requests</li> <li>Development Schedule</li> <li>Pending Development Tasks</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Starting with OASIS</li> </ul>"},{"location":"tags/#education","title":"education","text":"<ul> <li>Advanced Textbook</li> </ul>"},{"location":"tags/#example","title":"example","text":"<ul> <li>Example Container</li> </ul>"},{"location":"tags/#gdal_1","title":"gdal","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#oasis","title":"oasis","text":"<ul> <li>Starting with OASIS</li> </ul>"},{"location":"tags/#quickstart","title":"quickstart","text":"<ul> <li>Quick Start</li> <li>Cloud Reproducibility Triangle (CRT)</li> <li>Starting with OASIS</li> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#raster","title":"raster","text":"<ul> <li>Example Container</li> <li>Advanced Textbook</li> </ul>"},{"location":"tags/#requests","title":"requests","text":"<ul> <li>Development Requests</li> </ul>"},{"location":"tags/#schedule","title":"schedule","text":"<ul> <li>Development Schedule</li> </ul>"},{"location":"tags/#stac_1","title":"stac","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#streaming","title":"streaming","text":"<ul> <li>Data Exploration and Collaboration in the Cloud</li> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"},{"location":"tags/#usage","title":"usage","text":"<ul> <li>How to Use the Data Library</li> </ul>"},{"location":"tags/#vsi_1","title":"vsi","text":"<ul> <li>Streaming Data in the Cloud with GDAL, VSI, and STAC</li> </ul>"}]}